""" Code for weight ranking according to different methods """

""" Modules """
import os
import copy
from glob import glob

""" Numpy and pandas """
import numpy as np
import pandas as pd

""" Tensorflow """
import tensorflow as tf

#from tqdm.notebook import tqdm
from tqdm import tqdm

""" WSBMR modules """
import wsbmr

# Fkeras for Hessian ranking and all the dependencies
import fkeras
import time

####################################################################################################################
#
# GENERIC RANKER (PARENT OF ALL)
#
####################################################################################################################

# Generic Weight Ranker (parent of rest)
class WeightRanker:
    def __init__(self, quantization: 'QuantizationScheme',
                  ascending = False, times_weights = False, normalize_score = False, 
                  **kwargs):
        """."""
        super().__init__()

        """ Initialize df """
        self.df = None
        self.quantization = quantization
        self.complete_ranking = True
        self.ascending = ascending
        self.times_weights = times_weights
        self.normalize_score = normalize_score
    
    """ Extracting the table of weights and creating the pandas DF is the same 
        for all methods, so we can define it inside the generic weightRanker obj 
    """
    def extract_weight_table(self, model, quantization, verbose = False, **kwargs):
        
        # Initialize dataframe table with all properties for all layers 
        df = []
        num_bits = quantization.m

        # Get supported weights and pruned masks 
        results = wsbmr.models.get_supported_weights(model.model, numpy = False, pruned = True, verbose = False)
        supported_weights, supported_pruned_masks, supported_layers, weights_param_num = results
        if verbose:
            print(f'[INFO] - Found a total of {len(supported_weights)} supported weights: {", ".join(list(supported_weights.keys()))}')

        # Get layer index per layer in supported_layers
        supported_layers_idxs = {lname: model.model.layers.index(supported_layers[lname]) for lname in supported_layers}

        # Now build the table for pandas 
        for ikw, kw in enumerate(supported_weights):
            if '/bias:' in kw:
                continue
            # Get vals
            w = supported_weights[kw]
            msk = supported_pruned_masks[kw]
            ly = supported_layers[kw]
            ily = supported_layers_idxs[kw]
            param_num = weights_param_num[kw]

            # Layer name 
            layer_name = ly.name

            # Get coords 
            str_coords, coords = wsbmr.models.get_weight_coordinates(w.numpy())
            # Repeat num_bits times
            str_coords = np.tile(str_coords, num_bits)

            # build weight name (with coords)
            str_weight_name = np.array([f"{layer_name}[" + "][".join(item) + "]" for item in coords.astype(str)])
            # Repeat num_bits times
            str_weight_name = np.tile(str_weight_name, num_bits)

            # Build pruned value 
            if msk is not None:
                str_mask = np.array([msk[tuple(c)] for c in coords]).flatten()
            else:
                str_mask = np.zeros(len(coords))
            # Repeat num_bits times
            str_mask = np.tile(str_mask, num_bits)
            
            # Bits 
            bits = np.repeat(np.arange(num_bits), len(coords))

            # Get kernel gradient 
            suscept = np.array([np.nan]*len(coords)*num_bits).flatten()
            
            # Now let's build the table we want for all weights. This table should contain the following information:
            #
            #  | weight               | layer | coord        | value | rank | susceptibility | bit |
            #  +----------------------+-------+--------------+-------+------+----------------+-----+
            #  | conv2d_1[0][0][0][0] | 0     | [0][0][0][0] | 0.45  | ?    | ?              |  0  |
            #
            # We'll add the rank and susceptibility later
            subT = {'weight': str_weight_name, #
                    'layer': [ily]*len(coords)*num_bits, #
                    'coord': str_coords, #
                    'value': np.tile(w.numpy().flatten(), num_bits), #
                    'bit': bits, #
                    'param_num': np.tile(param_num.numpy().flatten(), num_bits), #
                    'pruned': str_mask,
                    'rank' : [0]*len(coords)*num_bits, #
                    'susceptibility': suscept}
            subT = pd.DataFrame(subT)

            # Append to df
            df.append(subT)

        # concat all dfs 
        df = pd.concat(df, axis = 0).reset_index()

        df = df.sort_values(by = 'rank')

        # [@manuelbv]: add binary repr, which might be handy later
        df['binary'] = df['value'].apply(lambda x: wsbmr.utils.float_to_binary(x, num_bits))

        self.df = df

        return df

    # Method to write table to csv file 
    def _save_to_csv(self, df, filepath = None):
        # Make sure parent dir exists 
        os.makedirs(os.path.dirname(filepath), exist_ok = True)
        # Write to file 
        with open(filepath, 'w') as fo:
            df.to_csv(fo)
        
        print(f'Rank dataframe written to file {filepath}')
    
    @property
    def alias(self):
        return 'generic'


####################################################################################################################
#
# RANDOM RANKER
#
####################################################################################################################

# Random weight Ranker (list all the weights in the structure and rank them randomly)
class RandomWeightRanker(WeightRanker):
    def __init__(self, quantization, *args, **kwargs):
        """."""
        super().__init__(quantization, *args, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization

    # Method to actually rank the weights
    def rank(self, model, *args, exclude_zero = True, **kwargs):
        # Call super method to obtain DF 
        df = self.extract_weight_table(model, self.quantization)

        # Susceptibility here is considered uniform (hence the randomness assigning TMR)
        df['susceptibility'] = [1/(len(df))]*len(df)
        df['rank'] = np.random.permutation(np.arange(len(df)))
        
        # Sort by rank
        df = df.sort_values(by='rank')

        # assign to self 
        self.df = df

        return df
    
    # save to csv
    def save_to_csv(self, *args, **kwargs):
        self._save_to_csv(self.df, *args, **kwargs)
    
    @property
    def alias(self):
        return 'random'


####################################################################################################################
#
# WEIGHT VALUE RANKERS (AbsoluteValue)
#
####################################################################################################################

""" Rank weights according to their absolute value (the larger, the most important) """
class AbsoluteValueWeightRanker(WeightRanker):
    def __init__(self, quantization: 'QuantizationScheme', *args, **kwargs):
        """."""
        super().__init__(quantization, *args, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization

    # Method to actually rank the weights
    def rank(self, model, *args, exclude_zero = True, ascending = False, **kwargs):
        # Call super method to obtain DF 
        df = self.extract_weight_table(model, self.quantization)

        # Susceptibility here is considered uniform (hence the randomness assigning TMR)
        df = df.sort_values(by='value', key=abs, ascending=ascending)
        df['rank'] = np.arange(len(df))
        df['susceptibility'] = -np.log10(np.abs(df['value'].values))/np.max(np.abs(np.log10(np.abs(df['value'].values))))
        
        # assign to self 
        self.df = df

        return df
    
    # save to csv
    def save_to_csv(self, *args, **kwargs):
        self._save_to_csv(self.df, *args, **kwargs)
    
    @property
    def alias(self):
        return 'weight_abs_value'


####################################################################################################################
#
# POSITIONAL RANKERS (Bitwise MSB_LSB, layerwise, etc)
#
####################################################################################################################

""" Rank weights by layer (top to bottom, bottom to top or custom order) """
class BitwiseWeightRanker(WeightRanker):
    def __init__(self, quantization:'QuantizationScheme', *args, **kwargs):
        """."""
        super().__init__(quantization, *args, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization

    # Method to actually rank the weights
    def rank(self, model, *args, exclude_zero = True, ascending = True, **kwargs):
        # Call super method to obtain DF 
        df = self.extract_weight_table(model, self.quantization)

        # Susceptibility here is considered uniform (hence the randomness assigning TMR)
        df = df.sort_values(by=['bit'], ascending = ascending)
        df['rank'] = np.arange(len(df))
        df['susceptibility'] = [1/(len(df))]*len(df)
        
        # assign to self 
        self.df = df
        self.ascending = ascending

        return df
    
    # save to csv
    def save_to_csv(self, *args, **kwargs):
        self._save_to_csv(self.df, *args, **kwargs)
    
    @property
    def alias(self):
        return f'bitwise_{"msb" if self.ascending else "lsb"}'


""" Rank weights by layer (top to bottom, bottom to top or custom order) """
class LayerWeightRanker(WeightRanker):
    def __init__(self, quantization: 'QuantizationScheme', *args, **kwargs):
        """."""
        super().__init__(quantization, *args, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization

    # Method to actually rank the weights
    def rank(self, model, *args, exclude_zero = True, ascending = True, **kwargs):
        # Call super method to obtain DF 
        df = self.extract_weight_table(model, self.quantization)

        # Susceptibility here is considered uniform (hence the randomness assigning TMR)
        df = df.sort_values(by=['layer','coord','bit'], ascending = ascending)
        df['rank'] = np.arange(len(df))
        df['susceptibility'] = [1/(len(df))]*len(df)
        
        # assign to self 
        self.df = df
        self.ascending = ascending

        return df
    
    # save to csv
    def save_to_csv(self, *args, **kwargs):
        self._save_to_csv(self.df, *args, **kwargs)
    
    @property
    def alias(self):
        return f'layerwise_{"first" if self.ascending else "last"}'



####################################################################################################################
#
# COMPENSATIONAL RANKERS (DiffBitsPerWeight, RecursiveUnevenRanker)
#
####################################################################################################################

""" Rank weights with how different bits per weight"""
class DiffBitsPerWeightRanker(WeightRanker):
    def __init__(self, quantization: 'QuantizationScheme', *args, **kwargs):
        """."""
        super().__init__(quantization, *args, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization

    def calculate_bit_differences(self, binary_str):
        # Initialize the sum of differences
        diff_sum = 0
        # Iterate through the binary string, excluding the sign bit
        for i in range(1, len(binary_str) - 1):
            # Calculate the difference between adjacent bits
            diff = int(binary_str[i + 1]) - int(binary_str[i])
            # Add the absolute value of the difference to the sum
            diff_sum += abs(diff)
        return diff_sum

    def rank(self, model, *args, exclude_zero = True, **kwargs):
        # Call super method to obtain DF 
        def process_value(value):
            b = wsbmr.utils.float_to_binary(value,self.quantization.m)
            diff_sum = self.calculate_bit_differences(b)
            return diff_sum

        df = self.extract_weight_table(model, self.quantization.m)

        differences = np.vectorize(process_value)(df['value'].values)
        df['susceptibility'] = differences
        df = df.sort_values(by='susceptibility', ascending=False)
        df['rank'] = np.arange(len(df))

        # assign to self 
        self.df = df

        return df

    # save to csv
    def save_to_csv(self, *args, **kwargs):
        self._save_to_csv(self.df, *args, **kwargs)
    
    @property
    def alias(self):
        return f'diff_bits_per_weight'


""" Rank weights by using proportion Recursively """
class RecursiveUnevenRanker(WeightRanker):
    def __init__(self, quantization: 'QuantizationScheme', *args, **kwargs):
        """."""
        super().__init__(quantization, *args, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization

    # Method to actually rank the weights
    def rank(self, model, *args, **kwargs):
        # Call super method to obtain DF 
        df = self.extract_weight_table(model, self.quantization, **kwargs)

        last_level = self.quantization.m - 1
        input_df = df[df['bit']==0]
        rank = self.rec(input_df, last_level)
        rank['rank'] = np.arange(len(rank))

        # assign to self 
        self.df = df

        return df

    def _rec(self, input_df, last_level, bit_level = 0, indexes_to_triplicate = []):

        # Calculate proportions 
        subdf = input_df['binary'].str[bit_level].astype(int)
        c0 = len(input_df[subdf == 0])
        c1 = len(input_df[subdf == 1])
        # check which one is greater (if c0 < c1 -> we want 1, otherwise get 0)
        next_bit = int(c0 < c1)
        # get the next subdff
        subdff = input_df[subdf == next_bit]

        # If this is not the last level, keep recursively
        if bit_level < last_level:
            indexes_to_triplicate = self._rec(subdff, bit_level + 1, last_level, indexes_to_triplicate = indexes_to_triplicate)
            #indexes_to_triplicate += list(indexes_to_triplicate2)
        else:
            # Now, we reached the last bit, which means we need to pick min(c0,c1) rows from 
            # whatever {c0,c1} has a greater proportion, and set them to triplicate.
            if min(c0,c1) > 0:
                indexes_to_triplicate = subdff.index[:min(c0,c1)]
            else:
                indexes_to_triplicate = subdff.index[:max(c0,c1)]
        return indexes_to_triplicate
            

    # Entry point function
    def rec(self, input_df, last_level, bit_level = 0, indexes_to_triplicate = []):
        rank = None
        total_weights = len(input_df)
        count = 0

        while (len(input_df) > 0):
            # Let's calculate all the proportions for all bits
            codes = np.stack(input_df['binary'].apply(lambda x: [int(i) for i in x]))
            ps = codes.mean(axis=0)
            
            w_left = len(input_df)
            msg = f'Weights left: {len(input_df)}  -->  '
            msg += '      '.join([f'({i}) {100*(1-p):3.2f}% {100*p:3.2f}%' for i,p in enumerate(ps)])
            print(msg)
            #pbar.set_postfix({'weights_left': len(input_df)})
            #count += len(indexes_to_triplicate)
            #pbar.update(count)
            if len(indexes_to_triplicate) > 0:
                # Remove from input_df
                sub = input_df.loc[indexes_to_triplicate]
                input_df = input_df.drop(indexes_to_triplicate)
                bits = np.repeat(np.arange(last_level+1)[:,None].T, len(sub), axis = 0).flatten()
                sub = sub.loc[sub.index.repeat(last_level+1)]
                sub['bit'] = bits
                # all_indexes = list(input_df.index)
                # indexes_not_to_triplicate = []
                # for i in indexes_to_triplicate:
                #     if indexes_to_triplicate in all_indexes:
                #         indexes_not_to_triplicate.append(i)
                # input_df = input_df.loc[indexes_not_to_triplicate]
                if rank is None:
                    rank = sub
                else:
                    # Append
                    rank = pd.concat([rank, sub], axis = 0)
                
                # Reset indexes_to_triplicate
                indexes_to_triplicate = []
                
            # Just call recursive method 
            indexes_to_triplicate = self._rec(input_df, bit_level, last_level, indexes_to_triplicate = indexes_to_triplicate)
        
        #rank = pd.concat([rank, sub], axis = 0)
        return rank
            
    # save to csv
    def save_to_csv(self, *args, **kwargs):
        self._save_to_csv(self.df, *args, **kwargs)
    
    @property
    def alias(self):
        return f'recursive_uneven'



####################################################################################################################
#
# FIRST ORDER GRADIENT RANKERS (GRADCAM++)
#
####################################################################################################################

""" Rank weights by using HiRes (gradcam++) (we don't really use this class directly, this is just a parent 
    for HiResCamRanker and HiResDeltaRanker) 
"""
class GradRanker(WeightRanker):
    def __init__(self, quantization: 'QuantizationScheme', *args, use_delta_as_weight = False, **kwargs):
        """."""
        super().__init__(quantization, *args, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization
        self.use_delta_as_weight = use_delta_as_weight

    # Method to actually rank the weights
    def rank(self, model, X, *args, **kwargs):
        # Call super method to obtain DF 
        df = self.extract_weight_table(model, X, self.quantization, **kwargs)

        # assign to self 
        self.df = df

        return df
    
    """ Extracting the table of weights and creating the pandas DF is the same 
        for all methods, so we can define it inside the generic weightRanker obj 
    """
    def extract_weight_table(self, model, X, quantization: 'QuantizationScheme', 
                                batch_size = 1000, verbose = True, 
                                bit_value = None, out_dir = ".", **kwargs):
        
        if 'normalize_score' in kwargs:
            normalize_score = kwargs.pop('normalize_score')
            self.normalize_score = normalize_score
        if 'times_weights' in kwargs:
            times_weights = kwargs.pop('times_weights')
            self.times_weights = times_weights
        if 'ascending' in kwargs:
            ascending = kwargs.pop('ascending')
            self.ascending = ascending

        # Initialize dataframe table with all properties for all layers 
        df = []
        num_bits = quantization.m
        use_delta_as_weight = self.use_delta_as_weight
        print(f'[INFO] - {"U" if use_delta_as_weight else "NOT u"}sing delta as weights.')

        # Get supported weights and pruned masks 
        results = wsbmr.models.get_supported_weights(model.model, numpy = False, pruned = True, verbose = False)
        supported_weights, supported_pruned_masks, supported_layers, weights_param_num = results
        
        if verbose:
            print(f'[INFO] - Found a total of {len(supported_weights)} supported weights: {", ".join(list(supported_weights.keys()))}')

        # Get layer index per layer in supported_layers
        supported_layers_idxs = {lname: model.model.layers.index(supported_layers[lname]) for lname in supported_layers}

        # Get deltas per weight
        deltas = {kw: wsbmr.models.get_deltas(kv, num_bits = num_bits) for kw, kv in supported_weights.items()}
        is_bit_one = {kw: deltas[kw][1] for kw in supported_weights}
        deltas = {kw: deltas[kw][0] for kw in supported_weights}

        # Store the old weights 
        old_weights = copy.deepcopy(supported_weights)

        # if bit value is a number, but it's outside of the range [0, num_bits-1], or if it's not an integer, just make it None
        if bit_value is not None:
            if not isinstance(bit_value, int):
                bit_value = None
                if verbose:
                    print(f'[WARN] - Bit value is not an integer. Setting to None.')
            else:
                if bit_value < 0 or bit_value >= num_bits:
                    bit_value = None
                    if verbose:
                        print(f'[WARN] - Bit value outside of range [0, {num_bits-1}]. Setting to None.')

        # if bit value is none, just range over all bits
        if bit_value is None:
            bit_range = np.arange(num_bits)
            try_to_load_previous_bits = False
        else:
            bit_range = [bit_value]
            try_to_load_previous_bits = True

        # Loop thru bits and place the deltas 
        for i in bit_range:
            
            # # Now build the grad model
            # grad_model = tf.keras.models.Model([model.model.layers[1].input],model_layer_outputs)
            # Use activation model as grad model
            grad_model = model.activation_model

            # Replace the weights with the deltas (if use_delta_as_weight)
            if use_delta_as_weight:
                for w in grad_model.weights:
                    kw = w.name
                    if kw in deltas:
                        w.assign(deltas[kw][...,i])
            
            # Create a model that outputs the desired layer's output and the model's output
            model_layer_outputs = []
            w_layers = []
            for ly in grad_model.layers:
                for w in ly.weights:
                    if np.any([w.name in kw for kw in deltas]):
                        model_layer_outputs.append(ly.output)
                        w_layers.append(ly)
            
            # Iterate over the dataset to get the gradients
            batch_size = np.minimum(batch_size, len(X))
            niters = np.maximum(np.floor(len(X)//batch_size).astype(int), 1)
            nb_classes = model.model.output.shape[1]
            grads = {}
            Xiter = iter(X)

            for Bi, B in tqdm(enumerate(range(niters)), total = niters, desc = f'Extracting gradients (per batch) - bit {i}'):

                target_classes = []

                # Compute the gradients w.r.t. the weights of each layer 
                with tf.GradientTape(persistent=True) as tape:

                    # Get trainable weights from within tape
                    w_trainable_vars = [ly.trainable_weights for ly in w_layers]

                    # Watch the weights of the target layer
                    tape.watch(w_trainable_vars)

                    # Get the layer output and predictions
                    if isinstance(X, tf.data.Dataset) or isinstance(X, tf.keras.preprocessing.image.DirectoryIterator):
                        Xnext, Ynext = next(Xiter)
                    else:
                        Xnext = X[B*batch_size:(B+1)*batch_size]
                    layer_outputs = grad_model(Xnext)

                    # Get prediction, popping the last one of tmp_outputs
                    predictions = layer_outputs.pop(-1)

                    # Compute the gradient of the target class prediction with respect to the weights
                    for n in range(nb_classes):
                        # Get the target class prediction
                        target_classes.append(predictions[:, n])

                # Reorganize grads and average per classes 
                for n in range(nb_classes):
                    tmplist = tape.gradient(target_classes[n], w_trainable_vars)
                    # Convert to dictionary 
                    for iw, w in enumerate(w_trainable_vars):
                        for iww, ww in enumerate(w):
                            # grads times weights 
                            if times_weights:
                                gtw = tf.math.abs(tmplist[iw][iww]*ww)
                            else:
                                gtw = tf.math.abs(tmplist[iw][iww])
                            if ww.name not in grads:
                                grads[ww.name] = gtw
                            else:
                                if np.all(gtw != grads[ww.name]):
                                    grads[ww.name] += gtw
                                else:
                                    continue
                            # if verbose:
                            #     print(f'Adding {ww.name} to grads {n} {iw} {iww} -> {np.shape(ww)} {np.shape(tmplist[iw][iww])}')


            # TODO: What if we normalize it by layer instead of globally?
            # TODO: What if we take the ABS value instead of just the value itself? A big negative grad is better than zero!!!
            gmax = -np.inf
            gmin = np.inf
            for kw, kv in grads.items():
                gmax = np.maximum(gmax, tf.math.reduce_max(kv))
                gmin = np.minimum(gmin, tf.math.reduce_min(kv))
            
            # normalize
            for kw, kv in grads.items():
                grads[kw] = (kv-gmin)/(gmax-gmin)            

            # Now build the table for pandas 
            for ikw, kw in enumerate(supported_weights):
                if '/bias:' in kw:
                    continue
                # Get vals
                w = old_weights[kw]
                msk = supported_pruned_masks[kw]
                ly = supported_layers[kw]
                # So it's saying "prune_low_..." is not in grads, thus kw has the prune_low whatever
                w_name = kw.replace('prune_low_magnitude_', '') 

                g = grads[w_name]
                ily = supported_layers_idxs[kw]
                param_num = weights_param_num[kw]

                # Layer name 
                layer_name = ly.name

                # Get coords 
                str_coords, coords = wsbmr.models.get_weight_coordinates(w.numpy())

                # build weight name (with coords)
                str_weight_name = np.array([f"{layer_name}[" + "][".join(item) + "]" for item in coords.astype(str)])

                # Build pruned value 
                if msk is not None:
                    try:
                        str_mask = np.array([msk[tuple(c)] for c in coords]).flatten()
                    except:
                        print(f'Error at {kw} {np.shape(msk)} {np.shape(coords)}')
                else:
                    str_mask = np.zeros(len(coords))
                
                bits = np.array([i]*len(coords)).flatten()
                suscept_factor = 2.0**(-i)

                # Get kernel gradient 
                suscept = g.numpy().flatten()
                if normalize_score:
                    suscept *= suscept_factor

                # Build all params before setting table 
                str_ily = [ily]*len(coords)
                str_value = w.numpy().flatten()
                str_param_num = param_num.numpy().flatten()
                
                if not use_delta_as_weight:
                    # We need to repeat everything num_bits times cause this is the last iteration (We'll break the loop after this)
                    str_weight_name = np.tile(str_weight_name, num_bits)
                    str_ily = np.tile(str_ily, num_bits)
                    str_coords = np.tile(str_coords, num_bits)
                    str_value = np.tile(str_value, num_bits)
                    str_param_num = np.tile(str_param_num, num_bits)
                    str_mask = np.tile(str_mask, num_bits)

                    # Redefine bits
                    bits = np.repeat(np.arange(num_bits), len(coords))

                    # Redefine susceptibility
                    suscept_factor = 2.0**(-bits)
                    suscept = np.tile(g.numpy().flatten(), num_bits)
                    if normalize_score:
                        suscept *= suscept_factor


                # Now let's build the table we want for all weights. This table should contain the following information:
                #
                #  | weight               | layer | coord        | value | rank | susceptibility | bit |
                #  +----------------------+-------+--------------+-------+------+----------------+-----+
                #  | conv2d_1[0][0][0][0] | 0     | [0][0][0][0] | 0.45  | ?    | ?              |  0  |
                #
                # We'll add the rank and susceptibility later
                subT = {'weight': str_weight_name, #
                        'layer': str_ily, #
                        'coord': str_coords, #
                        'value': str_value, #
                        'bit': bits, #
                        'param_num': str_param_num, #
                        'pruned': str_mask,
                        'rank' : [0]*len(str_mask), #
                        'susceptibility': suscept}
                subT = pd.DataFrame(subT)

                # Append to df
                df.append(subT)

            # break if we are using deltas as weights
            if not use_delta_as_weight:
                break

        # concat all dfs 
        df = pd.concat(df, axis = 0).reset_index()

        # Rank by susceptibility
        if not normalize_score:
            df = df.sort_values(by=['bit','susceptibility'], ascending = [True, ascending])
            df['rank'] = np.arange(len(df))
        else:
            df = df.sort_values(by='susceptibility', ascending = ascending)
        df['rank'] = np.arange(len(df))

        # Finally, restore the weights of the model
        if use_delta_as_weight:
            for w in model.model.weights:
                kw = w.name
                if kw in deltas:
                    w.assign(old_weights[kw])
            
        self.df = df

        # Now, if let's see if we can load the previous bits
        any_bit_missing = False
        if try_to_load_previous_bits:
            for i in range(num_bits):
                if i != bit_value:
                    # Get parent path from out_dir
                    parent_dir = os.path.dirname(out_dir)
                    # Find other possible rankings in other folders at this level
                    fs = glob(os.path.join(parent_dir, '*', f'ranking_bit_{i}.csv'))

                    # If there are files, pick the latest one
                    if len(fs) > 0:
                        fs = sorted(fs, key = lambda x: os.path.getmtime(x), reverse = True)
                        fs = fs[0]
                        if os.path.isfile(fs):
                            try:
                                df_bit = pd.read_csv(fs)
                                df = pd.concat([df, df_bit], axis = 0)
                                print(f'[INFO] - Loaded previous bit {i} from file {fs}')
                            except Exception as e:
                                print(f'[ERR] - Could not load previous bit {i} from file {fs}: {e}')
                                any_bit_missing = True
                        else:
                            print(f'[WARN] - Could not find file {fs} to load bit {i}')
                            any_bit_missing = True
                    else:
                        print(f'[WARN] - Could not find any ranking file for bit {i} on directory {parent_dir}')
                        any_bit_missing = True

        # completeness of this ranking only if no bits are missing
        self.complete_ranking = not any_bit_missing

        # Filename (this is only relevant for delta methods, cause we want to parallelize it)
        filename = 'ranking.csv'
        if bit_value is not None:
            filename = f'ranking_bit_{bit_value}.csv'

        return df, filename

    # save to csv
    def save_to_csv(self, *args, **kwargs):
        self._save_to_csv(self.df, *args, **kwargs)

""" Rank weights by using HiRes (gradcam++) as a weight to compute, on average, how important each weight is """
class HiResCamWeightRanker(GradRanker):
    def __init__(self, quantization: 'QuantizationScheme', *args, **kwargs):
        """."""
        super().__init__(quantization, *args, use_delta_as_weight = False, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization
    
    @property
    def alias(self):
        alias = 'hirescam'
        if self.times_weights:
            alias += '_times_weights'
        if self.normalize_score:
            alias += '_norm'
        return alias

""" Rank weights by using HiRes (gradcam++) but instead of using the weights of the model, we use the DELTAS as weights """
class HiResDeltaRanker(GradRanker):
    def __init__(self, quantization: 'QuantizationScheme', *args, **kwargs):
        """."""
        super().__init__(quantization, *args, use_delta_as_weight = True, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization
    
    @property
    def alias(self):
        alias = 'hiresdelta'
        if self.times_weights:
            alias += '_times_weights'
        if self.normalize_score:
            alias += '_norm'
        return alias


####################################################################################################################
#
# SECOND ORDER GRADIENT RANKERS (Hessian)
#
####################################################################################################################

""" This is the parent class for all Hessian-based rankers """
class HessianRanker(WeightRanker):
    def __init__(self, quantization: 'QuantizationScheme', *args, use_delta_as_weight = False, **kwargs):
        """."""
        super().__init__(quantization, *args, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization
        self.use_delta_as_weight = use_delta_as_weight

    # override extract_weight_table
    def extract_weight_table(self, model, X, Y, quantization: 'QuantizationScheme', 
                             inner_ranking_method = 'same', 
                             delta_ranking = False, verbose = True, **kwargs):
        # In the original code they also ignore bias!!
        ignore_bias = True

        if 'normalize_score' in kwargs:
            normalize_score = kwargs.pop('normalize_score')
            self.normalize_score = normalize_score
        if 'times_weights' in kwargs:
            times_weights = kwargs.pop('times_weights')
            self.times_weights = times_weights
        if 'ascending' in kwargs:
            ascending = kwargs.pop('ascending')
            self.ascending = ascending

        """ Assertions """
        assert inner_ranking_method in ['same', 'hierarchical', 'msb'], 'Invalid ranking method'
        
        # Initialize dataframe table with all properties for all layers 
        df = []
        num_bits = quantization.m
        use_delta_as_weight = self.use_delta_as_weight
        print(f'[INFO] - {"U" if use_delta_as_weight else "NOT u"}sing delta as weights.')

        # Get supported weights and pruned masks 
        results = wsbmr.models.get_supported_weights(model.model, numpy = False, pruned = True, verbose = False)
        supported_weights, supported_pruned_masks, supported_layers, weights_param_num = results
        
        if verbose:
            print(f'[INFO] - Found a total of {len(supported_weights)} supported weights: {", ".join(list(supported_weights.keys()))}')

        # Get layer index per layer in supported_layers
        supported_layers_idxs = {lname: model.model.layers.index(supported_layers[lname]) for lname in supported_layers}

        # Get deltas per weight
        deltas = {kw: wsbmr.models.get_deltas(kv, num_bits = num_bits) for kw, kv in supported_weights.items()}
        is_bit_one = {kw: deltas[kw][1] for kw in supported_weights}
        deltas = {kw: deltas[kw][0] for kw in supported_weights}

        # Store the old weights 
        old_weights = copy.deepcopy(supported_weights)

        # Pick the right loss
        loss = model.loss
        if isinstance(loss, str):
            if loss == 'categorical_crossentropy':
                loss = tf.keras.losses.CategoricalCrossentropy()
            elif loss == 'mse' or loss == 'mean_squared_error':
                loss = tf.keras.losses.MeanSquaredError()
            else:
                raise ValueError(f'Loss {model.loss} not supported. Only categorical_crossentropy and mean_squared_error are supported for now.')

        # Loop thru bits and place the deltas 
        for i in range(num_bits):
            
            # Replace the weights with the deltas (if use_delta_as_weight)
            if use_delta_as_weight:
                for w in model.model.weights:
                    kw = w.name
                    if kw in deltas:
                        w.assign(deltas[kw][...,i])
            
            # Perform actual hessian ranking on our model
            hess = fkeras.metrics.HessianMetrics(
                model.model, 
                loss, 
                X, 
                Y,
                batch_size=480
            )

            hess_start = time.time()
            top_k = 8
            BIT_WIDTH = 8
            strategy = "sum"
            # Hessian model-wide sensitivity ranking
            eigenvalues, eigenvectors = hess.top_k_eigenvalues(k=top_k, max_iter=500, rank_BN=False, prefix=f"Bit {i} - " if use_delta_as_weight else "")

            print(f'Hessian eigenvalue compute time: {time.time() - hess_start} seconds\n')
            # eigenvalues = None
            rank_start_time = time.time()

            param_ranking, param_scores = hess.hessian_ranking_general(
                eigenvectors, eigenvalues=eigenvalues, k=top_k, strategy=strategy, iter_by=1
            )

            # First let's get the list of parameters per layer
            num_params_per_layer = []
            cumsum = 0 

            for ily, ly in enumerate(model.model.layers):
                if hasattr(ly, 'layer'):
                    ly = ly.layer
                if ly.__class__.__name__ in fkeras.fmodel.SUPPORTED_LAYERS:
                    #print(ly.name)
                    ps = [tuple(w.shape) for w in ly.trainable_variables]
                    pst = [np.prod(w.shape) for w in ly.trainable_variables]
                    if ignore_bias:
                        ps = [ps[0]]
                        pst = [pst[0]]
                    total = np.sum(pst)
                    cumsum += total
                    # Tuple output (will make debugging easier)
                    t = (ly.name, ily, total, cumsum, pst, ps)
                    # Append to list
                    num_params_per_layer.append(t)

            # Get the cumulative sum of parameters
            cumsum = np.array([t[3] for t in num_params_per_layer])

            # First, we will find to which layer each parameter belongs 
            # Get layers indexes 
            layer_idxs = np.array([t[1] for t in num_params_per_layer])
            param_ly = np.argmax(param_ranking[:,None] < cumsum[None,:], axis = 1)
                
            # Now, within the layer find the actual index 
            for rank, (p, score, ply) in enumerate(zip(param_ranking, param_scores, param_ly)):
                ly_t = num_params_per_layer[ply]
                ly_name = ly_t[0]

                # Remember to subtract the global cumsum (now tht we are looking inside the layer )
                ly_cumsum = 0
                if ply > 0:
                    ly_cumsum = num_params_per_layer[ply-1][3]    
                p_internal = p - ly_cumsum

                # Now get the number of params
                ly_num_params = ly_t[-2]

                # Get the shapes of the layer weights
                ly_shapes = ly_t[-1]

                # Get the cumsum internal to the layer 
                ly_internal_cumsum = np.cumsum(ly_num_params)

                # Get the index of the weight this param belongs to
                wi = np.argmax(p_internal < ly_internal_cumsum)

                # Get the shape of this weight 
                wi_idx = np.unravel_index(p_internal, ly_shapes[wi])

                # Form table entry 
                t = (ly_name, p, score, ply, ly_cumsum, p_internal, wi, wi_idx, ly_shapes[wi])
                #ranking.append(t)


                # Now let's build the table we want for all weights. This table should contain the following information:
                #
                #  | weight               | layer | coord        | value | rank | susceptibility | bit |
                #  +----------------------+-------+--------------+-------+------+----------------+-----+
                #  | conv2d_1[0][0][0][0] | 0     | [0][0][0][0] | 0.45  | ?    | ?              |  0  |
                #
                # Let's build the coordinate string 
                str_coord = '[' + ']['.join(list(map(str,wi_idx))) + ']'
                str_weight_name = f'{ly_name}{str_coord}'

                # Get weight value 
                global_layer_idx = layer_idxs[ply]
                w = model.model.layers[global_layer_idx].get_weights()[wi][wi_idx]
                str_value = str(w)

                # bits 
                str_bits = i

                # Get weight name 
                w_name = model.model.layers[global_layer_idx].weights[wi].name

                # Now build the "pruned" param
                pruned = supported_pruned_masks[w_name][wi_idx]
                str_pruned = pruned.numpy() if pruned is not None else False

                # Param num
                str_param_num = p

                # susceptibility
                suscept = score
                suscept_factor = 2.0**(-i)

                str_rank = rank

                if not use_delta_as_weight:
                    # We need to repeat everything num_bits times cause this is the last iteration (We'll break the loop after this)
                    str_weight_name = np.tile(str_weight_name, num_bits)
                    str_ily = np.tile(global_layer_idx, num_bits)
                    str_coords = np.tile(str_coord, num_bits)
                    str_value = np.tile(str_value, num_bits)
                    str_param_num = np.tile(str_param_num, num_bits)

                    # Redefine bits
                    bits = np.arange(num_bits)
                    str_bits = bits

                    # Redefine susceptibility
                    suscept_factor = 2.0**(-bits)
                    suscept = [score]*num_bits

                    # Str rank
                    str_rank = [rank]*num_bits
                
                # Scores
                # [@manuelbv]: we have two options here, 
                # 1: we just copy the score for all bits 
                # 2: we multiply the score times the delta of each bit (1 for MSB, 0.5 for MSB-1, etc.)
                if inner_ranking_method == 'same':
                    suscept = suscept
                elif inner_ranking_method == 'hierarchical':
                    suscept = suscept*suscept_factor

                # Now let's build the table we want for all weights. This table should contain the following information:
                #
                #  | weight               | layer | coord        | value | rank | susceptibility | bit |
                #  +----------------------+-------+--------------+-------+------+----------------+-----+
                #  | conv2d_1[0][0][0][0] | 0     | [0][0][0][0] | 0.45  | ?    | ?              |  0  |
                #
                # We'll add the rank and susceptibility later
                subT = {'weight': str_weight_name, 'layer': global_layer_idx, 'coord': str_coord, 
                        'value': str_value, 'bit': str_bits, 'pruned' : str_pruned, 'rank' : str_rank, 
                        'param_num': str_param_num, 'susceptibility': suscept}
                if use_delta_as_weight:
                    # We need to pass an index
                    subT = pd.DataFrame(subT, index = [rank])
                else:
                    subT = pd.DataFrame(subT)

                # Append to dfs structure 
                df.append(subT)

            # break if we are using deltas as weights
            if not use_delta_as_weight:
                break

        # concat all dfs 
        df = pd.concat(df, axis = 0).reset_index()

        # Finally, restore the weights of the model
        if use_delta_as_weight:
            for w in model.model.weights:
                kw = w.name
                if kw in deltas:
                    w.assign(old_weights[kw])
            
        self.df = df

        return df


    # Method to actually rank the weights
    def rank(self, model, XY, *args, inner_ranking_method = 'msb', exclude_zero = True, ascending = False, **kwargs):
        
        max_samples = 1000
        # Get X and Y
        if isinstance(XY, tf.data.Dataset) or isinstance(XY, tf.keras.preprocessing.image.DirectoryIterator):
            # Convert to numpy
            X, Y = None, None
            exp_batch_size = len(next(iter(XY))[0])
            exp_num_iters = int(np.ceil(max_samples/exp_batch_size))
            exp_samples = 0
            for x, y in tqdm(XY, total = np.minimum(exp_num_iters,len(XY)), desc = 'Converting tf.data.Dataset to numpy'):
                # get exp batch size
                exp_samples += len(x)
                # Concatenate 
                if X is None or y is None:
                    X = x
                    Y = y
                else:
                    X = np.concatenate((X, x), axis = 0)
                    Y = np.concatenate((Y, y), axis = 0)
                
                if exp_samples >= max_samples:
                    break
            
            # Get only a part 
            X = X[:max_samples]
            Y = Y[:max_samples]
        elif isinstance(XY, tuple):
            X, Y = XY
        elif isinstance(XY, np.ndarray):
            X = XY
            if len(args) > 0:
                Y = args[0]
        else:
            raise ValueError(f'XY must be either a tuple (X,Y) or a tf.data.Dataset, but got {type(XY)}')     
        
        # Call super method to obtain DF 
        df = self.extract_weight_table(model, X, Y, self.quantization, inner_ranking_method = inner_ranking_method, **kwargs)

        # Sort by susceptibility now (only changes stuff if inner_ranking_method is hierarchical)
        if inner_ranking_method == 'msb':
            df = df.sort_values(by=['bit','susceptibility'], ascending = [True, ascending])
        else:
            df = df.sort_values(by='susceptibility', ascending = ascending)

        # Re rank (if inner_ranking_method == same this does nothing)
        df['rank'] = np.arange(len(df))
        
        # assign to self 
        self.df = df

        return df
    
    # save to csv
    def save_to_csv(self, *args, **kwargs):
        self._save_to_csv(self.df, *args, **kwargs)


# Hessian based weight ranker 
# This was taken from: https://github.com/oliviaweng/CIFAR10/blob/fkeras/hls4ml/hessian_analysis.py
class HessianWeightRanker(HessianRanker):
    def __init__(self, quantization: 'QuantizationScheme', *args, **kwargs):
        """."""
        super().__init__(quantization, *args, use_delta_as_weight = False, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization
    
    @property
    def alias(self):
        alias = 'hessian'
        if self.normalize_score:
            alias += '_norm'
        return alias

""" Same, but using deltas as weights """
class HessianDeltaWeightRanker(HessianRanker):
    def __init__(self, quantization: 'QuantizationScheme', *args, **kwargs):
        """."""
        super().__init__(quantization, *args, use_delta_as_weight = True, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization
    
    @property
    def alias(self):
        alias = 'hessiandelta'
        if self.normalize_score:
            alias += '_norm'
        return alias
    


####################################################################################################################
#
# ORACLE RANKING (WORK IN PROGRESS...)
#
####################################################################################################################

# Oracle (we'll basically inject noise to the weights one by one and bit by bit and see how much 
# either the output or the loss changes)
class OracleWeightRanker(WeightRanker):
    def __init__(self, quantization: 'QuantizationScheme', *args, **kwargs):
        """."""
        super().__init__(quantization, *args, **kwargs)

        # Init df 
        self.df = None
        self.quantization = quantization
    
    # Method to actually rank the weights
    def rank(self, model, X, Y, *args, exclude_zero = True, **kwargs):
        df = self.extract_weight_table(model, X, Y, self.quantization, exclude_zero = exclude_zero, **kwargs)

        # Susceptibility here is considered uniform (hence the randomness assigning TMR)
        #df = df.sort_values(by=['susceptibility_loss', 'susceptibility_activation'], key=abs, ascending=False)
        df = df.sort_values(by = ['susceptibility_acc'], key = abs, ascending = False)

        df['rank'] = np.arange(len(df))
        #df['susceptibility'] = -np.log10(np.abs(df['value'].values))/np.max(np.abs(np.log10(np.abs(df['value'].values))))
        
        # assign to self 
        self.df = df

        return df
    
    # save to csv
    def save_to_csv(self, *args, **kwargs):
        self._save_to_csv(self.df, *args, **kwargs)
    
    """ Extracting the table of weights and creating the pandas DF is the same 
        for all methods, so we can define it inside the generic weightRanker obj 
    """
    def extract_weight_table(self, model, X, Y, quantization: 'QuantizationScheme', batch_size = 1000, verbose = False, **kwargs):
        
        # Initialize dataframe table with all properties for all layers 
        df = []
        num_bits = quantization.m

        # Get the loss and activations for all the batches (we'll compare this to the loss and activations of the corrupted weights)
        niters = np.floor(len(X)//batch_size).astype(int)

        # Make sure to compile activation model 
        #act_model = model.activation_model
        #act_model.compile(model.optimizer, loss= model.loss, metrics = model.metrics)

        # Baseline accuracy
        #layer_outputs = act_model.predict(X, batch_size = batch_size)
        losses = model.model.evaluate(X, Y, batch_size = batch_size, return_dict = True)
        baseline_acc = losses['accuracy']

        # Get pruned masks 
        pruned_masks = model.pruned_masks

        if pruned_masks is None:
            # Create array of empty masks 
            pruned_masks = [None]*len(model.model.layers)
        
        # Assert they have the same length as the model's layers
        assert( len(pruned_masks) == len(model.model.layers) ), 'Number of pruned masks and layers do not match'

        """
            2) COMPUTATION OF THE DELTA TO BE ADDED TO EACH WEIGHT, ACCORDING TO THEIR INDIVIDUAL BIT VALUE (0 or 1)
        """
        # Initialize the delta_bits per layer 
        deltas = []

        # Loop thru layers to find those that are weight layers 
        for ily, ly in enumerate(model.model.layers):
            # Get this layer's weights
            _ws = ly.get_weights()

            bits_delta = []

            # Make sure this a weight layer (otherwise just pass)
            skip = True
            if len(_ws) > 0:
                if verbose:
                    print(ily, ly)
                skip = False
                if len(_ws) == 1:
                    w = _ws[0]
                elif len(_ws) == 2:
                    # Get weight and bias
                    w, b = _ws
                else:
                    skip = True

                if not skip:
                    # Get layer name
                    layer_name = ly.name

                    """ bits_delta_for 0 --> 1 """
                    bits_delta = 2.0**(-np.arange(0,num_bits))
                    bits_delta = np.reshape(bits_delta, tuple([1]*w.ndim) + (num_bits,))
                    bits_delta = np.tile(bits_delta, w.shape + (1,))

                    """ Now we need to check each weight and see whether their bits are 0 or 1 to apply the correct delta """
                    delta_bit_flip_shifted = ((2**(num_bits-1))*bits_delta).astype(int)

                    is_neg_weight = (w < 0)
                    weights_shifted = ((2**(num_bits-1))*np.abs(w)).astype(int)
                    is_neg_weight_shifted = (((2**(num_bits-1))*(1+w)).astype(int))
                    weights_shifted[is_neg_weight] = is_neg_weight_shifted[is_neg_weight]

                    # Add extra dimension for nbits
                    weights_shifted = np.tile(weights_shifted[...,None], num_bits)

                    # Perform the comparison
                    is_bit_one = ((delta_bit_flip_shifted & weights_shifted) == delta_bit_flip_shifted)

                    # Remember that MSB is for sign, so let's fix that in the "is_bit_one" matrix
                    is_bit_one[...,0] = (w<0)

                    # Now that we know which bits are one and which not, we can calculate the deltas for each bit
                    # Remember that 0 --> 1 means ADDING value except for the bit before the period, in which case 
                    # it means, subtracting 1.
                    bits_delta = 1.0*bits_delta
                    bits_delta[...,0] *= -1

                    bits_delta = bits_delta*(1-is_bit_one) + (-1*bits_delta)*is_bit_one

                    # How to check this code? 
                    # i = 5
                    # print(f'This number: {w[0,0,0,i]} should be (almost) equal to this number: ', end = '')
                    # p = np.sum(is_bit_one[0,0,0,i,1:]*(2.0**(-np.arange(1,num_bits))))
                    # if is_bit_one[0,0,0,i,0]:
                    #     print(p-1)
                    # else:
                    #     print(p)

            # Append bits_delta to list
            deltas.append(bits_delta)


        # Loop thru layers to find those that are weight layers 
        for ily, (ly, msk) in enumerate(zip(model.model.layers, pruned_masks)):
            # Get this layer's weights
            _ws = ly.get_weights()

            # If no weights, skip 
            if len(_ws) == 0:
                continue

            if len(_ws) != 2 and len(_ws) != 1:
                print(f'Unexpected number of weights {len(_ws)} for layer {ly}. Skipping.')
                continue
            
            # Make sure this a weight layer (otherwise just pass)
            b = None
            use_bias = False
            if len(_ws) == 1:
                w = _ws[0]
            elif len(_ws) == 2:
                # Only weight layers
                w, b = _ws
                use_bias = True

            # Get layer name
            layer_name = ly.name

            # Get the delta mask for this layer
            delta = deltas[ily]

            # Let's build a tensor with the same size as these weights, where we will hold how much 
            # each bit affects the loss or the activations
            w_suscept = np.zeros(delta.shape + (2,)) # 2 is for (loss divergence, activation divergence)
            b_suscept = np.zeros(b.shape + (2,)) if b is not None else None
            
            # Loop thru each weight bit and apply delta 
            # With that intent, let's create a multiindex array so we can dynamically 
            # access any weight with any dimension. This is how we'll do that:
            global_idxs = np.arange(np.prod(w.shape)) # I know I could add the bit dimension here, but it'll be more comprehensive and readable if I don't
            idxs = np.array(np.unravel_index(global_idxs, w.shape)).T
            
            # Copy weight 
            new_weights = w.copy()

            # Now loop thru each entry of this idxs table (each weight element)
            with tqdm(total = len(idxs), desc = f'Processing layer {ily}') as pbar:
                for idx in idxs:
                    #if verbose:
                        #print(f'\tProcessing weight {idx}')

                    # Convert idx to tuple 
                    idx = tuple(idx)
                    
                    # Get the weight value
                    w_val = w[idx]

                    pbar.set_postfix({'idx': idx})

                    # Loop thru bits 
                    for ibit in range(num_bits):
                        #tf.print(f'\t\tProcessing bit {ibit}')
                        #pbar.set_postfix({'idx': idx, 'bit': ibit})

                        # Apply delta 
                        w_corrupted = w_val + delta[idx][ibit]

                        # Change weight index
                        new_weights[idx] = w_corrupted

                        # Set the new weight value
                        if use_bias:
                            #act_model.layers[ily].set_weights([new_weights, b])
                            model.model.layers[ily].set_weights([new_weights, b])
                        else:
                            #act_model.layers[ily].set_weights([new_weights])
                            model.model.layers[ily].set_weights([new_weights])

                        # Get the new loss and activations
                        #new_layer_outputs = act_model.predict(X, batch_size = batch_size, verbose = False)
                        #new_losses = model.model.evaluate(X, Y, batch_size = batch_size, return_dict = True, verbose = False)
                        new_output = model.model.predict(X, batch_size = batch_size, verbose = False)

                        # Compute accuracy divergence between new_output and Y 
                        acc = tf.reduce_mean(tf.cast(tf.argmax(new_output, axis = 1) == tf.argmax(Y, axis = 1), tf.float32))
                        acc_div = tf.abs(acc - baseline_acc)

                        # Compute the div between the new and the old loss
                        #loss_div = (new_losses['loss'] - losses['loss'])

                        # Compute the div between the new and the old activations
                        # act_div = 0.0
                        # for i in range(len(layer_outputs[ily:])):
                        #     act_div += tf.reduce_mean(tf.abs(new_layer_outputs[ily+i] - layer_outputs[ily+i]))

                        # Compute the susceptibility
                        #w_suscept[idx][ibit] = [loss_div, act_div]
                        w_suscept[idx][ibit] = [acc_div]

                        # Reset weights
                        if use_bias:
                            #act_model.layers[ily].set_weights([w, b])
                            model.model.layers[ily].set_weights([w, b])
                        else:
                            #act_model.layers[ily].set_weights([w])
                            model.model.layers[ily].set_weights([w])

                    pbar.update(1)


               
            # Now let's get the coordinates of this layer weights'
            xs = [np.arange(0,s) for s in w.shape]
            if w.ndim == 2:
                xs = [xs[1], xs[0]]
            grid = np.meshgrid(*xs)
            ws = w.flatten()
            coords = np.stack([g.flatten() for g in grid], axis = 1)

            # REMEMBER TO SWAP COORDS[0] and COORDS[1] because np.meshgrid is silly and swaps X and Y ...
            if w.ndim >= 2: 
                reorder = [1,0]
                reorder += list(np.arange(2,w.ndim))
                coords = coords[:, reorder]

            str_coord = []
            str_weight_name = []
            str_bits = []
            str_values = []
            str_pruned = []
            #str_suscept_loss = []
            #str_suscept_act = []
            str_suscept_acc = []
            for i in range(len(ws)):
                for b in range(num_bits):
                    _W = w
                    _coord = ''
                    #print(coords[i])
                    for cc in coords[i]:
                        #print(cc)
                        _W = _W[cc]
                        _coord += f'[{cc}]'
                    #print(cc,_coord)
                    #_W = w[coords[i][0]][coords[i][1]][coords[i][2]][coords[i][3]]
                    err = _W - ws[i]

                    #_coord = f'[{coords[i][1]}][{coords[i][0]}][{coords[i][2]}][{coords[i][3]}]'
                    str_coord += [_coord]
                    str_weight_name += [f'{layer_name}{_coord}']
                    if np.abs(err) > 0:
                        print(f'w{_coord}: {_W} {ws[i]} --> {err}')
                    str_bits += [b]
                    str_values += [ws[i]]
                    #str_suscept_loss += [w_suscept[tuple(coords[i])][b][0]]
                    #str_suscept_act += [w_suscept[tuple(coords[i])][b][1]]

                    str_suscept_acc += [w_suscept[tuple(coords[i])][b][0]]
                str_pruned += [msk[coords[i]] if msk is not None else False]*len(num_bits)

            # Now let's build the table we want for all weights. This table should contain the following information:
            #
            #  | weight               | layer | coord        | value | rank | susceptibility | bit |
            #  +----------------------+-------+--------------+-------+------+----------------+-----+
            #  | conv2d_1[0][0][0][0] | 0     | [0][0][0][0] | 0.45  | ?    | ?              |  0  |
            #
            # We'll add the rank and susceptibility later
            subT = {'weight': str_weight_name, 'layer': [ily]*len(ws)*num_bits, 'coord': str_coord, 'value': str_values, 'bit': str_bits,
                    'susceptibility_acc': str_suscept_acc, 'pruned': str_pruned,
                    #'susceptibility_loss': str_suscept_loss, 'susceptibility_activation': str_suscept_act, 
                    'rank' : [0]*len(ws)*num_bits, 'susceptibility': [np.nan]*len(ws)*num_bits}
            subT = pd.DataFrame(subT)

            # Append to dfs structure 
            df.append(subT)

        # concat all dfs 
        df = pd.concat(df, axis = 0).reset_index()

        df = df.sort_values(by = 'rank')

        # [@manuelbv]: add binary repr, which might be handy later
        df['binary'] = df['value'].apply(lambda x: wsbmr.utils.float_to_binary(x, num_bits))

        self.df = df

        return df
    
    @property
    def alias(self):
        return 'oracle'



####################################################################################################################
#
# Access point to get rankers according to the method alias
#
####################################################################################################################

# Function to create a weight ranker given a couple of flags 
def build_weight_ranker(method: str, *args, **kwargs):
    options = {'random': RandomWeightRanker, 
                'weight_abs_value': AbsoluteValueWeightRanker,
                'layerwise': LayerWeightRanker,
                'bitwise': BitwiseWeightRanker,
                'hirescam': HiResCamWeightRanker,
                'hiresdelta': HiResDeltaRanker,
                'recursive_uneven': RecursiveUnevenRanker,
                'diffbitperweight': DiffBitsPerWeightRanker,
                # 'gradcrossentropy': GradCrossEntropyWeightRanker,
                'oracle': OracleWeightRanker,
                'hessian': HessianWeightRanker,
                'hessiandelta': HessianDeltaWeightRanker}
                
    ranker = options.get(method.lower(), WeightRanker)(*args, **kwargs)

    return ranker

