{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca78130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding /mnt/raid0/asic/projects/NU/netsurf/manuelbv to sys.path\n",
      "[INFO] - Added qkeras to sys.path from /mnt/raid0/asic/projects/NU/netsurf/manuelbv/qkeras\n",
      "[INFO] - Loaded theme: default\n",
      "\u001b[48;2;141;47;102m\u001b[48;2;141;47;102m \u001b[48;2;117;39;85m \u001b[48;2;94;31;68m \u001b[48;2;70;23;51m \u001b[48;2;46;15;33m \u001b[48;2;23;7;17m \u001b[48;2;0;0;0m \u001b[48;2;23;7;17m \u001b[48;2;46;15;33m \u001b[48;2;70;23;51m \u001b[48;2;94;31;68m \u001b[48;2;117;39;85m \u001b[48;2;141;47;102m      \u001b[48;2;249;230;207m â–—â–™â– â––â–—  â–œâ–› â–œ  â–œ \u001b[0m\n",
      "\u001b[48;2;141;47;102m\u001b[48;2;141;47;102m \u001b[48;2;117;39;85m \u001b[48;2;94;31;68m \u001b[48;2;70;23;51mğŒ\u001b[48;2;46;15;33m \u001b[48;2;23;7;17mâ† \u001b[48;2;0;0;0mâŒ¾\u001b[48;2;23;7;17mâ†\u001b[48;2;46;15;33m \u001b[48;2;70;23;51mğŒƒ\u001b[48;2;94;31;68m \u001b[48;2;117;39;85mğŒ–\u001b[48;2;141;47;102m ğŒ”    \u001b[48;2;250;198;122m â––â–˜â–™ â–â–œâ–˜â–—â–›   â–â–™  \u001b[0m\n",
      "\u001b[48;2;141;47;102m\u001b[48;2;141;47;102m \u001b[48;2;117;39;85m \u001b[48;2;94;31;68m \u001b[48;2;70;23;51m \u001b[48;2;46;15;33m \u001b[48;2;23;7;17m \u001b[48;2;0;0;0m \u001b[48;2;23;7;17m \u001b[48;2;46;15;33m \u001b[48;2;70;23;51m \u001b[48;2;94;31;68m \u001b[48;2;117;39;85m \u001b[48;2;141;47;102m      \u001b[48;2;250;198;122m â–—â––   â–œ â–œâ–›    â–™â–œ \u001b[0m\n",
      "\n",
      "Logging to file: /home/manuelbv/.nodus/nodus_20250416_103949.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-- Date: 16/Apr/2025\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
      "â•° INFO â”€â”¤ 10:39:49.94 â”‚ - Nodus initialized\n",
      "        â”‚ 10:39:49.94 â”‚ - Nodus version: 0.1.0\n",
      "        â”‚ 10:39:49.94 â”‚ - Nodus imported\n",
      "        â”‚ 10:39:49.94 â”‚ - Jobs imported\n",
      "        â”‚ 10:39:49.94 â”‚ - JobManager imported\n",
      "        â”‚ 10:39:49.94 â”‚ - Nodus ready to use\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found config file: /home/manuelbv/.netsurf/config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        â”‚ 10:39:50.16 â”‚ - Created jobs table in NodusDB instance 'netsurf_db'\n",
      "        â”‚ 10:39:50.16 â”‚ - Created job_dependencies table in NodusDB instance 'netsurf_db'\n",
      "        â”‚ 10:39:50.16 â”‚ - Added NodusDB instance 'netsurf_db' linked to database 'netsurf_db'\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "hls4ml_catapult_path = '/home/manuelbv/hls4ml_catapult'\n",
    "sys.path = [hls4ml_catapult_path] + sys.path\n",
    "\n",
    "# Import datetime to get today's date\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\" Let's add our custom netsurf code \"\"\"\n",
    "import netsurf\n",
    "\n",
    "\"\"\" Get netsurf path \"\"\"\n",
    "import os \n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(netsurf.__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca54892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§® <QuantizationScheme(q<6,2,1>)> obj @ (0x7f5c10d2caf0):\n",
      "    Total number of bits (m): 6\n",
      "    Integer bits         (n): 2\n",
      "    Float bits           (f): 3\n",
      "    Signed\n",
      "    Range: (-4, 3.875)\n",
      "    Min step: 0.125\n",
      "    Format: Sxx.xxx\n",
      "\n",
      "7_HLS_q6_2_1_bmark_keyword_spotting_prune0_125.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° BMK â”€â”€â”¤ 10:39:51.21 â”‚ - Initializing benchmark object keyword_spotting\n",
      "        â”‚ 10:39:51.21 â”‚ - Loss sparse_categorical_crossentropy found in tf.keras.losses with definition <function sparse_categorical_crossentropy at 0x7f5930833130>.\n",
      "        â”‚ 10:39:51.21 â”‚ - Adding custom metric sparse_categorical_accuracy with definition SparseCategoricalAccuracy(name=sparse_categorical_accuracy,dtype=float32).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /ml/conda_envs/manuelbv/envs/netsurf/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        â”‚ 10:39:56.56 â”‚ - Benchmark object keyword_spotting initialized\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° PLOT â”€â”¤ 10:40:18.46 â”‚ - Variable qq_conv2d_4/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.47 â”‚ - Variable qq_conv2d_3/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.48 â”‚ - Variable qq_conv2d_1/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.49 â”‚ - Variable qq_conv2d_2/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.51 â”‚ - Variable qq_dense/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.52 â”‚ - Variable qq_conv2d/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.52 â”‚ - Variable qq_depthwise_conv2d_2/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.53 â”‚ - Variable qq_depthwise_conv2d_3/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.54 â”‚ - Variable qq_depthwise_conv2d/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.54 â”‚ - Variable qq_depthwise_conv2d_1/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.55 â”‚ - Variable qq_batch_normalization_6/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.56 â”‚ - Variable qq_conv2d_4/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.57 â”‚ - Variable qq_apply_alpha_7/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.57 â”‚ - Variable qq_depthwise_conv2d_3/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.58 â”‚ - Variable qq_batch_normalization_6/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.59 â”‚ - Variable qq_batch_normalization_7/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.60 â”‚ - Variable qq_batch_normalization_5/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.60 â”‚ - Variable qq_apply_alpha_6/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.61 â”‚ - Variable qq_apply_alpha_7/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.62 â”‚ - Variable qq_batch_normalization_7/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.62 â”‚ - Variable qq_apply_alpha_8/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.63 â”‚ - Variable qq_apply_alpha_8/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.64 â”‚ - Variable qq_batch_normalization_8/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.65 â”‚ - Variable qq_batch_normalization_8/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.66 â”‚ - Variable qq_batch_normalization_2/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.67 â”‚ - Variable qq_apply_alpha_5/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.68 â”‚ - Variable qq_batch_normalization_5/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.69 â”‚ - Variable qq_apply_alpha_5/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.70 â”‚ - Variable qq_conv2d_3/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.70 â”‚ - Variable qq_apply_alpha_6/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.71 â”‚ - Variable qq_batch_normalization_4/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.72 â”‚ - Variable qq_batch_normalization_4/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.73 â”‚ - Variable qq_conv2d_2/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.73 â”‚ - Variable qq_batch_normalization_3/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.74 â”‚ - Variable qq_apply_alpha_3/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.75 â”‚ - Variable qq_batch_normalization_3/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.76 â”‚ - Variable qq_apply_alpha_3/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.76 â”‚ - Variable qq_apply_alpha_4/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.77 â”‚ - Variable qq_apply_alpha_4/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.77 â”‚ - Variable qq_depthwise_conv2d_2/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.78 â”‚ - Variable qq_apply_alpha_1/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.79 â”‚ - Variable qq_batch_normalization_2/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.79 â”‚ - Variable qq_depthwise_conv2d_1/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.80 â”‚ - Variable qq_apply_alpha_2/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.81 â”‚ - Variable qq_batch_normalization_1/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.82 â”‚ - Variable qq_batch_normalization_1/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.83 â”‚ - Variable qq_apply_alpha_2/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.83 â”‚ - Variable qq_conv2d_1/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.84 â”‚ - Variable qq_batch_normalization/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.85 â”‚ - Variable qq_apply_alpha_1/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.86 â”‚ - Variable qq_batch_normalization/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.86 â”‚ - Variable qq_depthwise_conv2d/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.87 â”‚ - Variable qq_apply_alpha/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:18.88 â”‚ - Variable qq_apply_alpha/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.89 â”‚ - Variable qq_conv2d/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:18.90 â”‚ - Variable qq_dense/bias:0 has sparsity 100.00%\n",
      "/mnt/raid0/asic/projects/NU/netsurf/manuelbv/tutorials/../netsurf/utils/plot.py:1019: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n",
      "        â”‚ 10:40:19.83 â”‚ - Saved sparsity plot to /tmp/netsurf_sparsitycnha300u.png\n",
      "        â”‚ 10:40:19.83 â”‚ - Plot range is -2.904186725616455 to 2.7973709106445312\n",
      "        â”‚ 10:40:20.14 â”‚ - Variable qq_conv2d/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.15 â”‚ - Variable qq_conv2d/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.16 â”‚ - Variable qq_apply_alpha/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.17 â”‚ - Variable qq_apply_alpha/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.18 â”‚ - Variable qq_batch_normalization/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.19 â”‚ - Variable qq_batch_normalization/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.20 â”‚ - Variable qq_depthwise_conv2d/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.21 â”‚ - Variable qq_depthwise_conv2d/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.21 â”‚ - Variable qq_apply_alpha_1/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.22 â”‚ - Variable qq_apply_alpha_1/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.23 â”‚ - Variable qq_batch_normalization_1/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.24 â”‚ - Variable qq_batch_normalization_1/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.25 â”‚ - Variable qq_conv2d_1/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.26 â”‚ - Variable qq_conv2d_1/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.27 â”‚ - Variable qq_apply_alpha_2/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.28 â”‚ - Variable qq_apply_alpha_2/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.29 â”‚ - Variable qq_batch_normalization_2/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.30 â”‚ - Variable qq_batch_normalization_2/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.31 â”‚ - Variable qq_depthwise_conv2d_1/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.31 â”‚ - Variable qq_depthwise_conv2d_1/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.32 â”‚ - Variable qq_apply_alpha_3/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.33 â”‚ - Variable qq_apply_alpha_3/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.35 â”‚ - Variable qq_batch_normalization_3/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.36 â”‚ - Variable qq_batch_normalization_3/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.37 â”‚ - Variable qq_conv2d_2/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.38 â”‚ - Variable qq_conv2d_2/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.38 â”‚ - Variable qq_apply_alpha_4/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.39 â”‚ - Variable qq_apply_alpha_4/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.41 â”‚ - Variable qq_batch_normalization_4/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.41 â”‚ - Variable qq_batch_normalization_4/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.42 â”‚ - Variable qq_depthwise_conv2d_2/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.43 â”‚ - Variable qq_depthwise_conv2d_2/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.44 â”‚ - Variable qq_apply_alpha_5/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.45 â”‚ - Variable qq_apply_alpha_5/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.46 â”‚ - Variable qq_batch_normalization_5/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.47 â”‚ - Variable qq_batch_normalization_5/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.48 â”‚ - Variable qq_conv2d_3/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.49 â”‚ - Variable qq_conv2d_3/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.50 â”‚ - Variable qq_apply_alpha_6/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.50 â”‚ - Variable qq_apply_alpha_6/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.52 â”‚ - Variable qq_batch_normalization_6/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.52 â”‚ - Variable qq_batch_normalization_6/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.53 â”‚ - Variable qq_depthwise_conv2d_3/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.54 â”‚ - Variable qq_depthwise_conv2d_3/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.55 â”‚ - Variable qq_apply_alpha_7/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.56 â”‚ - Variable qq_apply_alpha_7/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.57 â”‚ - Variable qq_batch_normalization_7/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.58 â”‚ - Variable qq_batch_normalization_7/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.59 â”‚ - Variable qq_conv2d_4/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.60 â”‚ - Variable qq_conv2d_4/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.61 â”‚ - Variable qq_apply_alpha_8/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.62 â”‚ - Variable qq_apply_alpha_8/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.63 â”‚ - Variable qq_batch_normalization_8/gamma:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.64 â”‚ - Variable qq_batch_normalization_8/beta:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:20.65 â”‚ - Variable qq_dense/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:40:20.66 â”‚ - Variable qq_dense/bias:0 has sparsity 100.00%\n",
      "        â”‚ 10:40:30.57 â”‚ - Saved sparsity plot to /tmp/netsurf_sparsity_separatedu2pjvqb2.png\n",
      "        â”‚ 10:40:30.57 â”‚ - Plot range is -2.904186725616455 to 2.7973709106445312\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° DATA â”€â”¤ 10:40:33.63 â”‚ - Normalizing dataset (input) using quantizer range (-4, 3.875)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] - HTML not implemented for keyword_spotting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° PLT â”€â”€â”¤ 10:40:35.47 â”‚ - Plotting histogram for activation qq_conv2d\n",
      "        â”‚ 10:40:35.56 â”‚ - Plotting histogram for activation qq_apply_alpha\n",
      "        â”‚ 10:40:35.65 â”‚ - Plotting histogram for activation qq_batch_normalization\n",
      "        â”‚ 10:40:35.74 â”‚ - Plotting histogram for activation qq_activation\n",
      "        â”‚ 10:40:35.84 â”‚ - Plotting histogram for activation qq_dropout\n",
      "        â”‚ 10:40:35.93 â”‚ - Plotting histogram for activation qq_depthwise_conv2d\n",
      "        â”‚ 10:40:36.02 â”‚ - Plotting histogram for activation qq_apply_alpha_1\n",
      "        â”‚ 10:40:36.11 â”‚ - Plotting histogram for activation qq_batch_normalization_1\n",
      "        â”‚ 10:40:36.20 â”‚ - Plotting histogram for activation qq_activation_1\n",
      "        â”‚ 10:40:36.30 â”‚ - Plotting histogram for activation qq_conv2d_1\n",
      "        â”‚ 10:40:36.39 â”‚ - Plotting histogram for activation qq_apply_alpha_2\n",
      "        â”‚ 10:40:36.48 â”‚ - Plotting histogram for activation qq_batch_normalization_2\n",
      "        â”‚ 10:40:36.57 â”‚ - Plotting histogram for activation qq_activation_2\n",
      "        â”‚ 10:40:36.67 â”‚ - Plotting histogram for activation qq_depthwise_conv2d_1\n",
      "        â”‚ 10:40:36.77 â”‚ - Plotting histogram for activation qq_apply_alpha_3\n",
      "        â”‚ 10:40:36.87 â”‚ - Plotting histogram for activation qq_batch_normalization_3\n",
      "        â”‚ 10:40:36.97 â”‚ - Plotting histogram for activation qq_activation_3\n",
      "        â”‚ 10:40:37.07 â”‚ - Plotting histogram for activation qq_conv2d_2\n",
      "        â”‚ 10:40:37.17 â”‚ - Plotting histogram for activation qq_apply_alpha_4\n",
      "        â”‚ 10:40:37.25 â”‚ - Plotting histogram for activation qq_batch_normalization_4\n",
      "        â”‚ 10:40:37.35 â”‚ - Plotting histogram for activation qq_activation_4\n",
      "        â”‚ 10:40:37.45 â”‚ - Plotting histogram for activation qq_depthwise_conv2d_2\n",
      "        â”‚ 10:40:37.54 â”‚ - Plotting histogram for activation qq_apply_alpha_5\n",
      "        â”‚ 10:40:37.63 â”‚ - Plotting histogram for activation qq_batch_normalization_5\n",
      "        â”‚ 10:40:37.73 â”‚ - Plotting histogram for activation qq_activation_5\n",
      "        â”‚ 10:40:37.83 â”‚ - Plotting histogram for activation qq_conv2d_3\n",
      "        â”‚ 10:40:37.92 â”‚ - Plotting histogram for activation qq_apply_alpha_6\n",
      "        â”‚ 10:40:38.00 â”‚ - Plotting histogram for activation qq_batch_normalization_6\n",
      "        â”‚ 10:40:38.09 â”‚ - Plotting histogram for activation qq_activation_6\n",
      "        â”‚ 10:40:38.19 â”‚ - Plotting histogram for activation qq_depthwise_conv2d_3\n",
      "        â”‚ 10:40:38.29 â”‚ - Plotting histogram for activation qq_apply_alpha_7\n",
      "        â”‚ 10:40:38.38 â”‚ - Plotting histogram for activation qq_batch_normalization_7\n",
      "        â”‚ 10:40:38.48 â”‚ - Plotting histogram for activation qq_activation_7\n",
      "        â”‚ 10:40:38.58 â”‚ - Plotting histogram for activation qq_conv2d_4\n",
      "        â”‚ 10:40:38.67 â”‚ - Plotting histogram for activation qq_apply_alpha_8\n",
      "        â”‚ 10:40:38.76 â”‚ - Plotting histogram for activation qq_batch_normalization_8\n",
      "        â”‚ 10:40:38.86 â”‚ - Plotting histogram for activation qq_activation_8\n",
      "        â”‚ 10:40:40.04 â”‚ - Plotting histogram for activation qq_dropout_1\n",
      "        â”‚ 10:40:40.14 â”‚ - Plotting histogram for activation average_pooling2d\n",
      "        â”‚ 10:40:40.23 â”‚ - Plotting histogram for activation qq_flatten\n",
      "        â”‚ 10:40:40.33 â”‚ - Plotting histogram for activation qq_dense\n",
      "        â”‚ 10:40:40.44 â”‚ - Plotting histogram for activation qq_softmax\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° BMK â”€â”€â”¤ 10:40:56.29 â”‚ - No path provided, looking for latest h5 file at default model path: /mnt/raid0/asic/projects/NU/netsurf/manuelbv/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/models\n",
      "        â”‚ 10:40:56.29 â”‚ - Looking for file /mnt/raid0/asic/projects/NU/netsurf/manuelbv/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/models/pruned_0.125_DSConv.keras.latest\n",
      "        â”‚ 10:40:56.42 â”‚ - Weights successfully loaded from file /mnt/raid0/asic/projects/NU/netsurf/manuelbv/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/models/pruned_0.125_DSConv.keras.latest\n",
      "        â”‚ 10:40:56.42 â”‚ - Loading session from file /mnt/raid0/asic/projects/NU/netsurf/manuelbv/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/sessions/training_session.20250416_000619/training_session.20250416_000619.pkl\n",
      "        â”‚ 10:40:58.63 â”‚ - Loss sparse_categorical_crossentropy found in tf.keras.losses with definition <function sparse_categorical_crossentropy at 0x7f5930833130>.\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° MDL â”€â”€â”¤ 10:40:59.02 â”‚ - Loading session with batch_size = 100, epochs = 36, opt_params = {'learning_rate': 0.001}, pruning_params = {'final_sparsity': 0.125, 'step': 2, 'end_epoch': 10}\n",
      "        â”‚ 10:40:59.02 â”‚ - Compiling model with parameters learning_rate=0.001\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° PLOT â”€â”¤ 10:41:00.76 â”‚ - Saved training history plot to /mnt/raid0/asic/projects/NU/netsurf/manuelbv/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/sessions/training_session.20250416_000619/training_session.20250416_000619_training_history.png\n",
      "/mnt/raid0/asic/projects/NU/netsurf/manuelbv/tutorials/../netsurf/utils/plot.py:604: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n",
      "  pts = pd.DataFrame(shapes).T.plot.pie(ax = ax, subplots = True)\n",
      "/mnt/raid0/asic/projects/NU/netsurf/manuelbv/tutorials/../netsurf/utils/plot.py:614: UserWarning: Tight layout not applied. tight_layout cannot make Axes width small enough to accommodate all Axes decorations\n",
      "  plt.tight_layout()\n",
      "        â”‚ 10:41:02.55 â”‚ - Saved figure to /mnt/raid0/asic/projects/NU/netsurf/manuelbv/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/models/weights_pie.png\n",
      "        â”‚ 10:41:02.64 â”‚ - Variable qq_conv2d_4/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.65 â”‚ - Variable qq_conv2d_3/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.66 â”‚ - Variable qq_conv2d_1/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.67 â”‚ - Variable qq_conv2d_2/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.68 â”‚ - Variable qq_dense/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.70 â”‚ - Variable qq_conv2d/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.70 â”‚ - Variable qq_depthwise_conv2d_2/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.71 â”‚ - Variable qq_depthwise_conv2d_3/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.71 â”‚ - Variable qq_depthwise_conv2d/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.72 â”‚ - Variable qq_depthwise_conv2d_1/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.73 â”‚ - Variable qq_batch_normalization_6/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.74 â”‚ - Variable qq_conv2d_4/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.75 â”‚ - Variable qq_apply_alpha_7/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.75 â”‚ - Variable qq_depthwise_conv2d_3/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.76 â”‚ - Variable qq_batch_normalization_6/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.77 â”‚ - Variable qq_batch_normalization_7/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.78 â”‚ - Variable qq_batch_normalization_5/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.78 â”‚ - Variable qq_apply_alpha_6/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.79 â”‚ - Variable qq_apply_alpha_7/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.80 â”‚ - Variable qq_batch_normalization_7/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.80 â”‚ - Variable qq_apply_alpha_8/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.81 â”‚ - Variable qq_apply_alpha_8/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.82 â”‚ - Variable qq_batch_normalization_8/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.83 â”‚ - Variable qq_batch_normalization_8/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.84 â”‚ - Variable qq_batch_normalization_2/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.84 â”‚ - Variable qq_apply_alpha_5/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.85 â”‚ - Variable qq_batch_normalization_5/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.86 â”‚ - Variable qq_apply_alpha_5/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.87 â”‚ - Variable qq_conv2d_3/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.87 â”‚ - Variable qq_apply_alpha_6/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.89 â”‚ - Variable qq_batch_normalization_4/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.89 â”‚ - Variable qq_batch_normalization_4/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.90 â”‚ - Variable qq_conv2d_2/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.90 â”‚ - Variable qq_batch_normalization_3/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.91 â”‚ - Variable qq_apply_alpha_3/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.92 â”‚ - Variable qq_batch_normalization_3/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:02.93 â”‚ - Variable qq_apply_alpha_3/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.93 â”‚ - Variable qq_apply_alpha_4/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.94 â”‚ - Variable qq_apply_alpha_4/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.95 â”‚ - Variable qq_depthwise_conv2d_2/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.95 â”‚ - Variable qq_apply_alpha_1/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.96 â”‚ - Variable qq_batch_normalization_2/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.97 â”‚ - Variable qq_depthwise_conv2d_1/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.98 â”‚ - Variable qq_apply_alpha_2/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.98 â”‚ - Variable qq_batch_normalization_1/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:02.99 â”‚ - Variable qq_batch_normalization_1/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:03.00 â”‚ - Variable qq_apply_alpha_2/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:03.01 â”‚ - Variable qq_conv2d_1/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:03.02 â”‚ - Variable qq_batch_normalization/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:03.03 â”‚ - Variable qq_apply_alpha_1/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:03.04 â”‚ - Variable qq_batch_normalization/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:03.04 â”‚ - Variable qq_depthwise_conv2d/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:03.05 â”‚ - Variable qq_apply_alpha/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:03.06 â”‚ - Variable qq_apply_alpha/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:03.06 â”‚ - Variable qq_conv2d/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:03.07 â”‚ - Variable qq_dense/bias:0 has sparsity 0.00%\n",
      "/mnt/raid0/asic/projects/NU/netsurf/manuelbv/tutorials/../netsurf/utils/plot.py:1019: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n",
      "        â”‚ 10:41:04.04 â”‚ - Saved sparsity plot to /mnt/raid0/asic/projects/NU/netsurf/manuelbv/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/sessions/training_session.20250416_000619/sparsity.png\n",
      "        â”‚ 10:41:04.04 â”‚ - Plot range is -3.624330997467041 to 3.4351866245269775\n",
      "        â”‚ 10:41:04.34 â”‚ - Variable qq_conv2d/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.35 â”‚ - Variable qq_conv2d/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.36 â”‚ - Variable qq_apply_alpha/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.36 â”‚ - Variable qq_apply_alpha/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.38 â”‚ - Variable qq_batch_normalization/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.38 â”‚ - Variable qq_batch_normalization/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.39 â”‚ - Variable qq_depthwise_conv2d/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.40 â”‚ - Variable qq_depthwise_conv2d/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.41 â”‚ - Variable qq_apply_alpha_1/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.42 â”‚ - Variable qq_apply_alpha_1/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.43 â”‚ - Variable qq_batch_normalization_1/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.44 â”‚ - Variable qq_batch_normalization_1/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.45 â”‚ - Variable qq_conv2d_1/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.46 â”‚ - Variable qq_conv2d_1/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.46 â”‚ - Variable qq_apply_alpha_2/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.47 â”‚ - Variable qq_apply_alpha_2/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.48 â”‚ - Variable qq_batch_normalization_2/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.49 â”‚ - Variable qq_batch_normalization_2/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.50 â”‚ - Variable qq_depthwise_conv2d_1/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.51 â”‚ - Variable qq_depthwise_conv2d_1/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.52 â”‚ - Variable qq_apply_alpha_3/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.52 â”‚ - Variable qq_apply_alpha_3/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.54 â”‚ - Variable qq_batch_normalization_3/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.55 â”‚ - Variable qq_batch_normalization_3/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.56 â”‚ - Variable qq_conv2d_2/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.57 â”‚ - Variable qq_conv2d_2/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.58 â”‚ - Variable qq_apply_alpha_4/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.58 â”‚ - Variable qq_apply_alpha_4/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.60 â”‚ - Variable qq_batch_normalization_4/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.60 â”‚ - Variable qq_batch_normalization_4/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.61 â”‚ - Variable qq_depthwise_conv2d_2/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.62 â”‚ - Variable qq_depthwise_conv2d_2/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.63 â”‚ - Variable qq_apply_alpha_5/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.63 â”‚ - Variable qq_apply_alpha_5/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.65 â”‚ - Variable qq_batch_normalization_5/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.66 â”‚ - Variable qq_batch_normalization_5/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.67 â”‚ - Variable qq_conv2d_3/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.68 â”‚ - Variable qq_conv2d_3/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.68 â”‚ - Variable qq_apply_alpha_6/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.69 â”‚ - Variable qq_apply_alpha_6/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.70 â”‚ - Variable qq_batch_normalization_6/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.71 â”‚ - Variable qq_batch_normalization_6/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.72 â”‚ - Variable qq_depthwise_conv2d_3/depthwise_kernel:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.73 â”‚ - Variable qq_depthwise_conv2d_3/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.74 â”‚ - Variable qq_apply_alpha_7/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.74 â”‚ - Variable qq_apply_alpha_7/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.76 â”‚ - Variable qq_batch_normalization_7/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.77 â”‚ - Variable qq_batch_normalization_7/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.78 â”‚ - Variable qq_conv2d_4/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.79 â”‚ - Variable qq_conv2d_4/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.79 â”‚ - Variable qq_apply_alpha_8/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.80 â”‚ - Variable qq_apply_alpha_8/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.81 â”‚ - Variable qq_batch_normalization_8/gamma:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.82 â”‚ - Variable qq_batch_normalization_8/beta:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:04.83 â”‚ - Variable qq_dense/kernel:0 has sparsity 12.50%\n",
      "        â”‚ 10:41:04.84 â”‚ - Variable qq_dense/bias:0 has sparsity 0.00%\n",
      "        â”‚ 10:41:14.42 â”‚ - Saved sparsity plot to /mnt/raid0/asic/projects/NU/netsurf/manuelbv/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/sessions/training_session.20250416_000619/sparsity_separated.png\n",
      "        â”‚ 10:41:14.42 â”‚ - Plot range is -3.624330997467041 to 3.4351866245269775\n",
      "        â”‚ 10:41:17.73 â”‚ - Baseline validation accuracy = 85.00%\n",
      "/ml/conda_envs/manuelbv/envs/netsurf/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1183: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels are not the same size as the confusion matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        â”‚ 10:41:18.15 â”‚ - Saved ROC plot to /mnt/raid0/asic/projects/NU/netsurf/manuelbv/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/sessions/training_session.20250416_000619/ROC.png\n",
      "        â”‚ 10:41:18.15 â”‚ - Plotting confusion matrix for labels ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
      "        â”‚ 10:41:18.49 â”‚ - Saved confusion matrix to /mnt/raid0/asic/projects/NU/netsurf/manuelbv/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/sessions/training_session.20250416_000619/confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "import pergamos as pg\n",
    "\n",
    "benchmark_name = 'keyword_spotting'\n",
    "# Set variables\n",
    "qscheme = \"q<6,2,1>\"\n",
    "pruning = 0.125\n",
    "prerank = True\n",
    "\n",
    "\n",
    "\"\"\" First of all, let's define a quantization Scheme \"\"\"\n",
    "Q = netsurf.QuantizationScheme(qscheme)\n",
    "print(Q)\n",
    "\n",
    "# Set filename\n",
    "benchmarks_dir = os.path.join(parent_dir, 'benchmarks')\n",
    "datasets_dir = os.path.join(parent_dir, 'datasets')\n",
    "\n",
    "filename = f\"7_HLS_{Q._scheme_str.no_special_chars()}_bmark_{benchmark_name}_prune{str(pruning).replace('.','_')}.html\"\n",
    "print(filename)\n",
    "doc = pg.Document(filename, theme=\"default\")\n",
    "doc.required_scripts.add('mathjax')\n",
    "\n",
    "\n",
    "\"\"\" Add a title to the document \"\"\"\n",
    "doc.append(pg.Markdown(f\"\"\"# Benchmarks Quantization Assertion\n",
    "> Author: Manuel B Valentin\n",
    "\n",
    "> Creation date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "> Project: netsurf\n",
    "\n",
    "> Used packages: netsurf, tensorflow, numpy, matplotlib, pergamos\n",
    "        \n",
    "\"\"\"))\n",
    "\n",
    "# Add a tab container\n",
    "tabs = pg.TabbedContainer({'Motivation': [], \n",
    "                        'Pre-training analysis': [],\n",
    "                        'Training': [],\n",
    "                            'Post-Training': [],\n",
    "                        'BER Injection': [],\n",
    "                        'Conclusions': []})\n",
    "\n",
    "# Get individual tabs\n",
    "tabmotivation = tabs['Motivation']\n",
    "tabpretraining = tabs['Pre-training analysis']\n",
    "tabtraining = tabs['Training']\n",
    "tabposttraining = tabs['Post-Training']\n",
    "tabber = tabs['BER Injection']\n",
    "tabconclusions = tabs['Conclusions']\n",
    "\n",
    "# Add to documnt\n",
    "doc.append(tabs)\n",
    "\n",
    "# Add a markdown description of what we want to achieve with this report in the first tab\n",
    "md = r\"\"\"\n",
    "## 1. Loss Taylor expansion\n",
    "\n",
    "Given a loss function $\\mathcal{L}(w)$, where $w$ is the vector of all weights in the network, the Taylor expansion around some point $w_0$ (say, the trained weights) for a small perturbation $\\Delta w$ is:\n",
    "\n",
    "$$\\mathcal{L}(w_0 + \\Delta w) \\approx \\mathcal{L}(w_0) + \\nabla \\mathcal{L}(w_0)^T \\Delta w + \\frac{1}{2} \\Delta w^T H \\Delta w$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\nabla \\mathcal{L}(w_0)$ is the gradient vector of the loss at w_0\n",
    "* $H$ is the Hessian matrix, i.e. $H = \\nabla^2 \\mathcal{L}(w_0)$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. If the model is trained??\n",
    "\n",
    "If the model has been well trained, then:\n",
    "\n",
    "$\\nabla \\mathcal{L}(w_0) \\approx 0$\n",
    "\n",
    "Because you're sitting near a (local) minimum.\n",
    "\n",
    "This removes the linear term:\n",
    "$\\mathcal{L}(w_0 + \\Delta w) - \\mathcal{L}(w_0) \\approx \\frac{1}{2} \\Delta w^T H \\Delta w$\n",
    "\n",
    "So the change in loss caused by a perturbation $\\Delta w$ is approximately:\n",
    "$\\Delta \\mathcal{L} \\approx \\frac{1}{2} \\Delta w^T H \\Delta w$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Interpretation for bit flips\n",
    "\n",
    "A bit flip in the quantized weights causes a small but structured change in the weights:\n",
    "\n",
    "* Say, flipping the 3rd bit in weight $w_i$ causes it to change by $\\delta_i$, so:\n",
    "\n",
    "$\\Delta w = \\begin{bmatrix}\n",
    "0 \\\\ \\cdots \\\\ \\delta_i \\\\ \\cdots \\\\ 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Then the loss increase is (approximately):\n",
    "$\\Delta \\mathcal{L} \\approx \\frac{1}{2} \\delta_i^2 H_{ii}$\n",
    "\n",
    "If multiple bits are flipped across weights, you sum their pairwise interactions via H, including off-diagonal terms (if not ignored).\n",
    "\n",
    "--- \n",
    "\n",
    "## 4. Implications for ranking\n",
    "\n",
    "This approximation motivates ranking bit positions (or weights) by:\n",
    "\n",
    "* $\\delta^2 \\cdot H_{ii}$: bit-flip magnitude times curvature\n",
    "* This is the FKeras method: estimates $H_{ii}$ and ranks accordingly\n",
    "* You could generalize it to your method:\n",
    "$\\text{Impact} \\cdot H$, not just gradients\n",
    "\n",
    "---\n",
    "\n",
    "## 5. When does this approximation hold?\n",
    "\n",
    "?? Works well when:\n",
    "\n",
    "* Bit-flip magnitude is small (i.e., local region)\n",
    "* Model is near a minimum\n",
    "* Hessian is stable (not exploding)\n",
    "\n",
    "?? Fails when:\n",
    "\n",
    "* Model isn??t trained well (gradient is large)\n",
    "* Loss surface is highly non-quadratic\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "The formula:\n",
    "$\\Delta \\mathcal{L} \\approx \\frac{1}{2} \\Delta \\mathbf{w}^T H \\Delta \\mathbf{w}$\n",
    "\n",
    "tells us how bit-flips propagate into loss increases, and explains why the Hessian is so powerful for ranking robustness. \n",
    "It encodes:\n",
    "\n",
    "* How impactful a perturbation is (via $\\delta$)\n",
    "* How sensitive the loss is locally (via $H$)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create markdown\n",
    "md = pg.Markdown(md)\n",
    "\n",
    "# Add to the first tab\n",
    "tabmotivation.append(md)\n",
    "\n",
    "\"\"\" Add quantization container to doc report \"\"\"\n",
    "tabpretraining.extend(Q.html())\n",
    "\n",
    "\"\"\" Save doc to file (we save after adding each element) \"\"\"\n",
    "doc.save(filename)\n",
    "\n",
    "# Let's create a container for all benchmarks \n",
    "benchmark_ct = pg.CollapsibleContainer(\"ğŸ§º Benchmarks\", layout='vertical')\n",
    "\n",
    "# And another one for tabtraining\n",
    "benchmark_sessions_ct = pg.CollapsibleContainer(\"ğŸ§º Benchmarks\", layout='vertical')\n",
    "\n",
    "# And another one for tabposttraining\n",
    "benchmark_posttraining_ct = pg.CollapsibleContainer(\"ğŸ§º Benchmarks\", layout='vertical')\n",
    "\n",
    "# And yet another for \"BER Injection\"\n",
    "benchmark_ber_ct = pg.CollapsibleContainer(\"ğŸ§ª Experiments\", layout='vertical')\n",
    "\n",
    "\"\"\" Add to documnt \"\"\"\n",
    "tabpretraining.append(benchmark_ct)\n",
    "\n",
    "\"\"\" Add \"\"\"\n",
    "tabtraining.append(benchmark_sessions_ct)\n",
    "\n",
    "\"\"\" Add \"\"\"\n",
    "tabposttraining.append(benchmark_posttraining_ct)\n",
    "\n",
    "\"\"\" Add \"\"\"\n",
    "tabber.append(benchmark_ber_ct)\n",
    "\n",
    "# Define benchmarks to analyze\n",
    "#benchmarks = ['dummy', 'mnist_hls4ml', 'autompg', 'smartpixel_small', 'smartpixel_large',\n",
    "#              'cifar10', 'mnist_lenet5', 'ECONT_AE'\n",
    "# 'cifar100', 'svhn', 'fashion_mnist', 'imdb', 'reuters', 'boston_housing']\n",
    "# TODO: Fix visualization/contrast for cifar10\n",
    "# TODO: mnist_lenet5 seems to be working (good accuracy), but I'm not too happy about the alphas/betas. Some layers still have a big portion outside of the valid interval\n",
    "\n",
    "config_per_methods = netsurf.config.config_per_method\n",
    "protection_range = (0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4) #netsurf.config.DEFAULT_PROTECTION\n",
    "ber_range = (0.001, 0.005, 0.01, 0.05, 0.1, 0.15) #netsurf.config.DEFAULT_BER\n",
    "\n",
    "#methods = ['qpolar', 'qpolargrad', 'bitwise_msb', 'random', 'hirescam_norm', \n",
    "#           'hiresdelta', 'hessian', 'hessiandelta', 'weight_abs_value']\n",
    "methods = ['laplace', 'hessian', 'fisher', 'bitwise_msb', 'random', 'grad_norm', 'graddelta', 'weight_abs_value'] # 'qpolar', 'qpolargrad', \n",
    "methods = methods\n",
    "\n",
    "# Create benchmark object \n",
    "bmk = netsurf.get_benchmark(benchmark_name, Q,  benchmarks_dir = benchmarks_dir,\n",
    "                            datasets_dir = datasets_dir, pruning = pruning,\n",
    "                            load_weights = False)\n",
    "\n",
    "# Add benchmark html to container (this includes model + dataset htmls)\n",
    "# (run before training the model...)\n",
    "benchmark_ct.append(bmk.html())\n",
    "\n",
    "# Now let's prepare the data\n",
    "nsample_mod = 48 if 'ECON' in bmk.name else -1\n",
    "XYTrain = netsurf.utils.prepare_data(bmk, subset = 'train', nsample_mod = nsample_mod)\n",
    "\n",
    "# # Initialize the uncertainty profiler (pre-training)\n",
    "# pre_robustness_sgn_path = os.path.join(bmk.model_dir, 'uncertainty_signatures', f'{benchmark_name}.pretraining.netsurf.sgn')\n",
    "# try:\n",
    "#     pre_robustness_sgn = netsurf.UncertaintyProfiler.profile(bmk.model, XYTrain, bmk.model.loss, \n",
    "#                                                             batch_size = 2000, filepath = pre_robustness_sgn_path,\n",
    "#                                                             verbose = True)\n",
    "#     # Save \n",
    "#     pre_robustness_sgn.save_to_file(pre_robustness_sgn_path)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Add profile to tabpretraining\n",
    "# benchmark_ct.append(pre_robustness_sgn.html())\n",
    "\n",
    "# Now we can reload the weights (After the profiling)\n",
    "bmk.load_weights(verbose = True)\n",
    "\n",
    "# TRAINING - SESSION\n",
    "# Try to get a session (if not, train)\n",
    "sess = netsurf.get_training_session(bmk, show_plots = False, plot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb14804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ds_conv\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 49, 10, 1)]       0         \n",
      "                                                                 \n",
      " qq_conv2d (QQConv2D)        (None, 25, 5, 64)         52417     \n",
      "                                                                 \n",
      " qq_apply_alpha (QQApplyAlph  (None, 25, 5, 64)        2434      \n",
      " a)                                                              \n",
      "                                                                 \n",
      " qq_batch_normalization (QQB  (None, 25, 5, 64)        2625      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " qq_activation (QQActivation  (None, 25, 5, 64)        1         \n",
      " )                                                               \n",
      "                                                                 \n",
      " qq_dropout (QQDropout)      (None, 25, 5, 64)         1         \n",
      "                                                                 \n",
      " qq_depthwise_conv2d (QQDept  (None, 25, 5, 64)        1793      \n",
      " hwiseConv2D)                                                    \n",
      "                                                                 \n",
      " qq_apply_alpha_1 (QQApplyAl  (None, 25, 5, 64)        2434      \n",
      " pha)                                                            \n",
      "                                                                 \n",
      " qq_batch_normalization_1 (Q  (None, 25, 5, 64)        2625      \n",
      " QBatchNormalization)                                            \n",
      "                                                                 \n",
      " qq_activation_1 (QQActivati  (None, 25, 5, 64)        1         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " qq_conv2d_1 (QQConv2D)      (None, 25, 5, 64)         83137     \n",
      "                                                                 \n",
      " qq_apply_alpha_2 (QQApplyAl  (None, 25, 5, 64)        2434      \n",
      " pha)                                                            \n",
      "                                                                 \n",
      " qq_batch_normalization_2 (Q  (None, 25, 5, 64)        2625      \n",
      " QBatchNormalization)                                            \n",
      "                                                                 \n",
      " qq_activation_2 (QQActivati  (None, 25, 5, 64)        1         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " qq_depthwise_conv2d_1 (QQDe  (None, 25, 5, 64)        1793      \n",
      " pthwiseConv2D)                                                  \n",
      "                                                                 \n",
      " qq_apply_alpha_3 (QQApplyAl  (None, 25, 5, 64)        2434      \n",
      " pha)                                                            \n",
      "                                                                 \n",
      " qq_batch_normalization_3 (Q  (None, 25, 5, 64)        2625      \n",
      " QBatchNormalization)                                            \n",
      "                                                                 \n",
      " qq_activation_3 (QQActivati  (None, 25, 5, 64)        1         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " qq_conv2d_2 (QQConv2D)      (None, 25, 5, 64)         83137     \n",
      "                                                                 \n",
      " qq_apply_alpha_4 (QQApplyAl  (None, 25, 5, 64)        2434      \n",
      " pha)                                                            \n",
      "                                                                 \n",
      " qq_batch_normalization_4 (Q  (None, 25, 5, 64)        2625      \n",
      " QBatchNormalization)                                            \n",
      "                                                                 \n",
      " qq_activation_4 (QQActivati  (None, 25, 5, 64)        1         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " qq_depthwise_conv2d_2 (QQDe  (None, 25, 5, 64)        1793      \n",
      " pthwiseConv2D)                                                  \n",
      "                                                                 \n",
      " qq_apply_alpha_5 (QQApplyAl  (None, 25, 5, 64)        2434      \n",
      " pha)                                                            \n",
      "                                                                 \n",
      " qq_batch_normalization_5 (Q  (None, 25, 5, 64)        2625      \n",
      " QBatchNormalization)                                            \n",
      "                                                                 \n",
      " qq_activation_5 (QQActivati  (None, 25, 5, 64)        1         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " qq_conv2d_3 (QQConv2D)      (None, 25, 5, 64)         83137     \n",
      "                                                                 \n",
      " qq_apply_alpha_6 (QQApplyAl  (None, 25, 5, 64)        2434      \n",
      " pha)                                                            \n",
      "                                                                 \n",
      " qq_batch_normalization_6 (Q  (None, 25, 5, 64)        2625      \n",
      " QBatchNormalization)                                            \n",
      "                                                                 \n",
      " qq_activation_6 (QQActivati  (None, 25, 5, 64)        1         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " qq_depthwise_conv2d_3 (QQDe  (None, 25, 5, 64)        1793      \n",
      " pthwiseConv2D)                                                  \n",
      "                                                                 \n",
      " qq_apply_alpha_7 (QQApplyAl  (None, 25, 5, 64)        2434      \n",
      " pha)                                                            \n",
      "                                                                 \n",
      " qq_batch_normalization_7 (Q  (None, 25, 5, 64)        2625      \n",
      " QBatchNormalization)                                            \n",
      "                                                                 \n",
      " qq_activation_7 (QQActivati  (None, 25, 5, 64)        1         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " qq_conv2d_4 (QQConv2D)      (None, 25, 5, 64)         83137     \n",
      "                                                                 \n",
      " qq_apply_alpha_8 (QQApplyAl  (None, 25, 5, 64)        2434      \n",
      " pha)                                                            \n",
      "                                                                 \n",
      " qq_batch_normalization_8 (Q  (None, 25, 5, 64)        2625      \n",
      " QBatchNormalization)                                            \n",
      "                                                                 \n",
      " qq_activation_8 (QQActivati  (None, 25, 5, 64)        1         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " qq_dropout_1 (QQDropout)    (None, 25, 5, 64)         1         \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 1, 1, 64)         0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " qq_flatten (QQFlatten)      (None, 64)                1         \n",
      "                                                                 \n",
      " qq_dense (QQDense)          (None, 49)                63652     \n",
      "                                                                 \n",
      " qq_softmax (QQSoftmax)      (None, 49)                1         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 501,335\n",
      "Trainable params: 27,313\n",
      "Non-trainable params: 474,022\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bmk.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a container for this bmk in tabtraining\n",
    "bmk_sess_ct = pg.CollapsibleContainer(benchmark_name, layout='vertical')\n",
    "\n",
    "# Add session to tabtraining\n",
    "bmk_sess_ct.append(sess.html())\n",
    "\n",
    "# And add to benchmarks in tabtraining\n",
    "benchmark_sessions_ct.append(bmk_sess_ct)\n",
    "\n",
    "# Add benchmark again to post-training to check how the weights changed\n",
    "#benchmark_posttraining_ct.append(bmk.html())\n",
    "\n",
    "# Initialize the uncertainty profiler (post-training)\n",
    "# try:\n",
    "#     post_robustness_sgn_path = os.path.join(bmk.model_dir, 'uncertainty_signatures', f'{benchmark_name}.posttraining.netsurf.sgn')\n",
    "#     post_robustness_sgn = netsurf.UncertaintyProfiler.profile(bmk.model, XYTrain, bmk.model.loss, \n",
    "#                                     batch_size = 2000, verbose = True, filepath = post_robustness_sgn_path)\n",
    "\n",
    "#     # Save \n",
    "#     post_robustness_sgn.save_to_file(post_robustness_sgn_path)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Add profile to tabposttraining\n",
    "# benchmark_posttraining_ct.append(post_robustness_sgn.html())\n",
    "\n",
    "# Let's compare the divergence between the profiles\n",
    "# div_profile = netsurf.ProfileDivergence.from_signatures(pre_robustness_sgn, post_robustness_sgn)\n",
    "\n",
    "#div_profile.plot_divergence_summary()\n",
    "#div_profile.plot_advanced_divergence_summary()\n",
    "\n",
    "\"\"\" Save doc to file (we save after adding each element) \"\"\"\n",
    "doc.save(filename)\n",
    "\n",
    "# Create a container for this benchmark in \"BER Injection\"\n",
    "bmk_ber_ct = pg.CollapsibleContainer(benchmark_name, layout='vertical')\n",
    "\n",
    "# Add to tabber\n",
    "benchmark_ber_ct.append(bmk_ber_ct)\n",
    "\n",
    "# Create another container to store the individual results of the experiments\n",
    "exp_individual_ct = pg.CollapsibleContainer(\"ğŸ Individual results\", layout='vertical')\n",
    "bmk_ber_ct.append(exp_individual_ct)\n",
    "\n",
    "# Create our ranker profiler (empty)\n",
    "rankers_bucket = netsurf.RankingComparator()\n",
    "\n",
    "# Loop thru methods\n",
    "exps = {}\n",
    "for method_alias in methods:\n",
    "    #########################################################################\n",
    "    # 0. Create the directory for the experiment (ranking, results, etc.)\n",
    "    #########################################################################\n",
    "    method = config_per_methods[method_alias]['method']\n",
    "    method_dir = os.path.join(bmk.experiments_dir, method)\n",
    "\n",
    "    #########################################################################\n",
    "    # 1. Create the ranker\n",
    "    #########################################################################\n",
    "    rkr = rankers_bucket.build_ranker(method, Q, config = config_per_methods[method], \n",
    "                                    path = method_dir,\n",
    "                                    is_baseline = (method == 'qpolargrad'))\n",
    "\n",
    "    # Get the exp dir from the ranker (with the hash)\n",
    "    exp_dir = rkr.path\n",
    "\n",
    "    #################################################################\n",
    "    # 1. Perform ranking according to method\n",
    "    #################################################################\n",
    "    # Rank weights \n",
    "    ranking = rkr.rank(bmk.model, *XYTrain, verbose = True)\n",
    "    # Save rank to csv file \n",
    "    ranking.save(overwrite = False)\n",
    "\n",
    "    #################################################################\n",
    "    # 2. Create experiment object\n",
    "    #################################################################\n",
    "    exp = netsurf.Experiment(bmk, ranking, num_reps = 100, \n",
    "                            ber_range = ber_range, \n",
    "                            protection_range = protection_range, \n",
    "                            path = exp_dir,\n",
    "                            verbose = True)\n",
    "\n",
    "    # Print experiment info \n",
    "    print(exp)\n",
    "\n",
    "    #################################################################\n",
    "    # 3. Run experiment with given ranking and for whatever \n",
    "    #       range of protection and rad \n",
    "    #################################################################\n",
    "    #batch_size = 1000,\n",
    "    exp.run_experiment(bmk, XYTrain,\n",
    "                    batch_size = None,\n",
    "                    ber_range = ber_range, \n",
    "                    protection_range = protection_range, \n",
    "                    rerun = False)\n",
    "    \n",
    "    # Add experiment to container\n",
    "    exp_individual_ct.append(exp.html())\n",
    "\n",
    "    # Save experiment object\n",
    "    exp.save()\n",
    "\n",
    "    # Add to dict\n",
    "    exps[method] = exp\n",
    "\n",
    "    \"\"\" Save doc to file (we save after adding each element) \"\"\"\n",
    "    doc.save(filename)\n",
    "\n",
    "\"\"\" Now perform the comparison of rankers \"\"\"\n",
    "#df = rankers_bucket.compare_rankers(granularity = 0.01, bootstrap = False)\n",
    "\n",
    "# Add comparison to container\n",
    "#bmk_ber_ct.append(rankers_bucket.html())\n",
    "\n",
    "# Save doc \n",
    "doc.save(filename)\n",
    "\n",
    "# # Save the rankers_bucket\n",
    "# rankers_comparison_filepath = os.path.join(bmk.experiments_dir, 'ranking_comparison.csv')\n",
    "# rankers_bucket.save_to_csv(rankers_comparison_filepath)\n",
    "\n",
    "# \"\"\" Create Experiment Comparator \"\"\"\n",
    "# exp_comp = netsurf.ExperimentComparator(list(exps.values()))\n",
    "#df = ExperimentComparator.compute_ranking_distribution(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqmodel = bmk.model\n",
    "qqmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42514b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now translate the model into a simple qkeras model for compilation\n",
    "import qkeras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Get the quantization scheme \n",
    "Q = qqmodel.quantizer\n",
    "print(Q)\n",
    "q_scheme = f'quantized_bits({Q.m},{Q.n},{bool(Q.s)}, alpha = 1, use_variables = True)'\n",
    "q_relu = f'quantized_relu({Q.m},{Q.n},{bool(Q.s)}, use_variables = True)'\n",
    "q_precision = f'ac_fixed<{Q.m},{Q.n+Q.s},{bool(Q.s)}>'\n",
    "n2 = Q.n*2\n",
    "f2 = Q.f*2\n",
    "m2 = n2 + f2 + int(Q.s)\n",
    "double_q_precision = f'ac_fixed<{m2},{n2+Q.s},{bool(Q.s)}>'\n",
    "\n",
    "print(q_scheme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb21b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = None \n",
    "for ilayer, layer in enumerate(qqmodel.layers):\n",
    "    print(layer.name)\n",
    "    name = layer.name.replace('qq_', '')\n",
    "    if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "        x = tf.keras.layers.Input(shape = layer.input_shape[0][1:], name = 'in0')\n",
    "        if inputs is None:\n",
    "            inputs = [x]\n",
    "        elif isinstance(inputs, list):\n",
    "            inputs.append(x)\n",
    "\n",
    "    elif isinstance(layer, tf.keras.layers.Flatten):\n",
    "        x = tf.keras.layers.Flatten(name = name)(x)\n",
    "    \n",
    "    elif isinstance(layer, tf.keras.layers.Dense):\n",
    "        ly = qkeras.QDense(layer.units, kernel_quantizer = q_scheme, name = name)\n",
    "        \n",
    "        # Pass the tensor so we can build the layer \n",
    "        x = ly(x)\n",
    "\n",
    "        # The next one MUST be a qqapplyalpha\n",
    "        next_layer = qqmodel.layers[ilayer + 1]\n",
    "        if isinstance(next_layer, netsurf.dnn.layers.QQApplyAlpha):\n",
    "            \n",
    "            # Transfer the weights over\n",
    "            # Remember we will have to apply the QAlpha to the dense layer before it\n",
    "            ws_fc0 = {v.name.split('/')[1].split(':')[0]: v for v in ly.trainable_variables}\n",
    "            ws_alpha0 = {v.name.split('/')[1].split(':')[0]: v for v in next_layer.trainable_variables}\n",
    "\n",
    "            # merge fc0 and alpha0\n",
    "            ws_fc0_alpha0 = {'kernel': ws_fc0['kernel']*ws_alpha0['alpha'], 'bias': ws_fc0['bias']*ws_alpha0['alpha'] + ws_alpha0['beta']}\n",
    "            ly.kernel.assign(ws_fc0_alpha0['kernel'])\n",
    "            ly.bias.assign(ws_fc0_alpha0['bias'])\n",
    "        elif isinstance(next_layer, tf.keras.layers.Softmax) or isinstance(next_layer, tf.keras.layers.Activation) or isinstance(next_layer, netsurf.dnn.layers.QQSoftmax) or isinstance(next_layer, netsurf.dnn.layers.QQActivation):\n",
    "            # It's okay\n",
    "            continue\n",
    "        else:\n",
    "            raise ValueError(f\"Layer {layer.name} should be followed by a QQApplyAlpha layer, but got {next_layer.name} instead\")\n",
    "\n",
    "    \n",
    "    elif isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        ly = qkeras.QConv2D(layer.filters, kernel_size = layer.kernel_size, strides = layer.strides,\n",
    "                        padding = layer.padding, kernel_quantizer = q_scheme, name = name)\n",
    "        \n",
    "        # Pass the tensor so we can build the layer \n",
    "        x = ly(x)\n",
    "\n",
    "        # The next one MUST be a qqapplyalpha\n",
    "        next_layer = qqmodel.layers[ilayer + 1]\n",
    "        if isinstance(next_layer, netsurf.dnn.layers.QQApplyAlpha):\n",
    "            \n",
    "            # Transfer the weights over\n",
    "            # Remember we will have to apply the QAlpha to the dense layer before it\n",
    "            ws_fc0 = {v.name.split('/')[1].split(':')[0]: v for v in ly.trainable_variables}\n",
    "            ws_alpha0 = {v.name.split('/')[1].split(':')[0]: v for v in next_layer.trainable_variables}\n",
    "\n",
    "            # merge fc0 and alpha0\n",
    "            ws_fc0_alpha0 = {'kernel': ws_fc0['kernel']*ws_alpha0['alpha'], 'bias': ws_fc0['bias']*ws_alpha0['alpha'] + ws_alpha0['beta']}\n",
    "            ly.kernel.assign(ws_fc0_alpha0['kernel'])\n",
    "            ly.bias.assign(ws_fc0_alpha0['bias'])\n",
    "        \n",
    "        elif isinstance(next_layer, tf.keras.layers.Softmax) or isinstance(next_layer, tf.keras.layers.Activation) or isinstance(next_layer, netsurf.dnn.layers.QQSoftmax) or isinstance(next_layer, netsurf.dnn.layers.QQActivation):\n",
    "            # It's okay\n",
    "            continue\n",
    "        else:\n",
    "            raise ValueError(f\"Layer {layer.name} should be followed by a QQApplyAlpha layer, but got {next_layer.name} instead\")\n",
    "\n",
    "    \n",
    "    elif isinstance(layer, tf.keras.layers.MaxPooling2D):\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size = layer.pool_size, strides = layer.strides,\n",
    "                                        padding = layer.padding, name = name)(x)\n",
    "    \n",
    "    elif isinstance(layer, tf.keras.layers.AveragePooling2D):\n",
    "        x = tf.keras.layers.AveragePooling2D(pool_size = layer.pool_size, strides = layer.strides,\n",
    "                                        padding = layer.padding, name = name)(x)\n",
    "    \n",
    "    elif isinstance(layer, tf.keras.layers.Activation) or isinstance(layer, netsurf.dnn.layers.QQActivation):\n",
    "        x = qkeras.QActivation(layer.activation, name = name)(x)\n",
    "    \n",
    "    elif isinstance(layer, tf.keras.layers.Dropout):\n",
    "        x = tf.keras.layers.Dropout(layer.rate, name = name)(x)\n",
    "    \n",
    "    elif isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        ly = tf.keras.layers.BatchNormalization(name = name)\n",
    "        # Pass \n",
    "        x = ly(x)\n",
    "        # We need to pass over the weights\n",
    "        # Transfer the weights over\n",
    "        # Remember we will have to apply the QAlpha to the dense layer before it\n",
    "        ws_fc0 = {v.name.split('/')[1].split(':')[0].replace('qq_',''): v for v in layer.trainable_variables}\n",
    "        # Add moving_variance and moving_average\n",
    "        ws_fc0['moving_mean'] = layer.moving_mean\n",
    "        ws_fc0['moving_variance'] = layer.moving_variance\n",
    "\n",
    "        # merge fc0 and alpha0\n",
    "        for key in ws_fc0.keys():\n",
    "            if 'gamma' in key:\n",
    "                print(f'Assigning layer.gamma to {key}')\n",
    "                ly.gamma.assign(ws_fc0[key])\n",
    "            elif 'beta' in key:\n",
    "                print(f'Assigning layer.beta to {key}')\n",
    "                ly.beta.assign(ws_fc0[key])\n",
    "            elif 'moving_mean' in key:\n",
    "                print(f'Assigning layer.moving_mean to {key}')\n",
    "                ly.moving_mean.assign(ws_fc0[key])\n",
    "            elif 'moving_variance' in key:\n",
    "                print(f'Assigning layer.moving_variance to {key}')\n",
    "                ly.moving_variance.assign(ws_fc0[key])\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown key {key} in BatchNormalization layer {layer.name}\")\n",
    "\n",
    "    \n",
    "    elif isinstance(layer, tf.keras.layers.Softmax):\n",
    "        x = tf.keras.layers.Softmax(name = name)(x)\n",
    "    else:\n",
    "        if not isinstance(layer, netsurf.dnn.layers.QQApplyAlpha):\n",
    "            print(f\"Layer {name} not supported, skipping...\")\n",
    "        continue\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop thru weights to check their values \n",
    "import matplotlib.pyplot as plt \n",
    "for v in model.trainable_variables:\n",
    "    plt.hist(v.numpy().flatten(), bins=100)\n",
    "    plt.title(v.name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e5323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks_dir = os.path.join(parent_dir, 'benchmarks')\n",
    "datasets_dir = os.path.join(parent_dir, 'datasets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d34e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then the QKeras model\n",
    "import hls4ml\n",
    "\n",
    "outpath = os.path.join(bmk.models_dir,'syn')\n",
    "os.makedirs(outpath, exist_ok = True)\n",
    "\n",
    "# Convert model to hls4ml\n",
    "hls_config_q = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "hls_config_q['Model']['ReuseFactor'] = 2000\n",
    "hls_config_q['Model']['Precision'] = q_precision\n",
    "hls_config_q[\"Model\"][\"Strategy\"] = \"Resource\"\n",
    "hls_config_q[\"Model\"][\"BramFactor\"] = 0\n",
    "\n",
    "\"\"\"\n",
    "    Input\n",
    "\"\"\"\n",
    "hls_config_q['LayerName']['in0']['Precision']['result'] = 'ac_fixed<8,1>'\n",
    "\n",
    "\n",
    "# Loop thru layers and assign precision\n",
    "for ly in model.layers:\n",
    "    # Get name \n",
    "    name = ly.name \n",
    "    if isinstance(ly, tf.keras.layers.Dense) or isinstance(ly, qkeras.QDense) or isinstance(ly, tf.keras.layers.Conv2D) or isinstance(ly, qkeras.QConv2D):\n",
    "        hls_config_q['LayerName'][name]['Precision']['weight'] = q_precision\n",
    "        hls_config_q['LayerName'][name]['Precision']['bias'] = q_precision\n",
    "        hls_config_q['LayerName'][name]['Precision']['result'] = double_q_precision\n",
    "        hls_config_q['LayerName'][name]['ReuseFactor'] = 2000\n",
    "    \n",
    "    elif isinstance(ly, tf.keras.layers.BatchNormalization):\n",
    "        hls_config_q['LayerName'][name]['Precision']['scale'] = q_precision\n",
    "        hls_config_q['LayerName'][name]['Precision']['bias'] = q_precision\n",
    "        hls_config_q['LayerName'][name]['Precision']['result'] = double_q_precision\n",
    "        hls_config_q['LayerName'][name]['Precision']['mean'] = q_precision\n",
    "        hls_config_q['LayerName'][name]['Precision']['variance'] = q_precision\n",
    "        hls_config_q['LayerName'][name]['Precision']['moving_variance'] = q_precision\n",
    "        hls_config_q['LayerName'][name]['Precision']['moving_mean'] = q_precision\n",
    "    \n",
    "    elif isinstance(ly, tf.keras.layers.Softmax):\n",
    "        hls_config_q['LayerName'][name]['Strategy'] = 'Stable'\n",
    "        hls_config_q['LayerName']['softmax']['Precision'] = f'ac_fixed<{Q.m},0,false>'\n",
    "        hls_config_q['LayerName']['softmax']['exp_table_t'] = 'ac_fixed<4,0>'\n",
    "        hls_config_q['LayerName']['softmax']['inv_table_t'] = 'ac_fixed<4,0>'\n",
    "    \n",
    "    else:\n",
    "        # Just set the precision to be q_precision for the result \n",
    "        hls_config_q['LayerName'][name]['Precision']['result'] = q_precision\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "#     fc0\n",
    "# \"\"\"\n",
    "# # Weights & bias \n",
    "# hls_config_q['LayerName']['fc0']['Precision']['weight'] = 'ac_fixed<8,0,true>'\n",
    "# # Set bias precision\n",
    "# hls_config_q['LayerName']['fc0']['Precision']['bias'] = q_precision\n",
    "# # Set activation precision\n",
    "# hls_config_q['LayerName']['fc0']['Precision']['result'] = double_q_precision\n",
    "# hls_config_q['LayerName']['fc0']['FlattenWeights'] = True\n",
    "# hls_config_q['LayerName']['fc0']['ReuseFactor'] = 2000\n",
    "\n",
    "# \"\"\"\n",
    "#     fcout\n",
    "# \"\"\"\n",
    "# # Weights & bias \n",
    "# hls_config_q['LayerName']['fc_out']['Precision']['weight'] = q_precision\n",
    "# # Set bias precision\n",
    "# hls_config_q['LayerName']['fc_out']['Precision']['bias'] = q_precision\n",
    "# # Set activation precision\n",
    "# hls_config_q['LayerName']['fc_out']['Precision']['result'] = double_q_precision\n",
    "# hls_config_q['LayerName']['fc_out']['ReuseFactor'] = 640\n",
    "\n",
    "\n",
    "# # fcout\n",
    "# hls_config_q['LayerName']['fc_out']['Precision']['result'] = \"ac_fixed<8,1,false>\"\n",
    "\n",
    "# \"\"\" \n",
    "#     softmax\n",
    "# \"\"\"\n",
    "# hls_config_q['LayerName']['softmax']['Strategy'] = 'Stable'\n",
    "# hls_config_q['LayerName']['softmax']['Precision'] = f'ac_fixed<{Q.m},0,false>'\n",
    "# hls_config_q['LayerName']['softmax']['exp_table_t'] = 'ac_fixed<4,0>'\n",
    "# hls_config_q['LayerName']['softmax']['inv_table_t'] = 'ac_fixed<4,0>'\n",
    "\n",
    "import json \n",
    "print(json.dumps(hls_config_q, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Create catapult config \"\"\"\n",
    "cfg_q = hls4ml.converters.create_config(backend='catapult')\n",
    "cfg_q['IOType'] = 'io_stream'  # Must set this if using CNNs!\n",
    "cfg_q['HLSConfig'] = hls_config_q\n",
    "cfg_q['KerasModel'] = model\n",
    "cfg_q['OutputDir'] = outpath\n",
    "#cfg_q['Technology'] = 'fpga' # inportant, should be in lower case\n",
    "cfg_q['Technology'] = 'asic' # inportant, should be in lower case\n",
    "cfg_q['ClockPeriod'] = 10\n",
    "cfg_q['ASICLibs'] = 'nangate-45nm_beh'\n",
    "cfg_q['FIFO'] = 'hls4ml_lib.mgc_pipe_mem'\n",
    "cfg_q['ASICFIFO']='hls4ml_lib.mgc_pipe_mem'\n",
    "#cfg_q['XilinxPart'] = 'xczu7cg-fbvb900-1-e'\n",
    "#cfg_q['Part'] = 'xczu7cg-fbvb900-1-e'\n",
    "#cfg_q.pop['XilinxPart']\n",
    "\n",
    "\n",
    "# cfg_q['BuildOptions'] = {\n",
    "#             'csim': True,\n",
    "#             'SCVerify': True,\n",
    "#             'Synth': True,\n",
    "#             'vhdl': False,\n",
    "#             'verilog': True,\n",
    "#             'RTLSynth': False,\n",
    "#             'RandomTBFrames': 2,\n",
    "#             'PowerEst': False,\n",
    "#             'PowerOpt': False,\n",
    "#             'BuildBUP': False,\n",
    "#             'BUPWorkers': False,\n",
    "#             'LaunchDA': False,\n",
    "#         }\n",
    "\n",
    "print(cfg_q)\n",
    "\n",
    "hls_model_q = hls4ml.converters.keras_to_hls(cfg_q)\n",
    "hls_model_q.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netsurf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
