{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca78130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding /Users/mbvalentin/scripts/netsurf to sys.path\n",
      "[INFO] - Added qkeras to sys.path from /Users/mbvalentin/scripts/netsurf/qkeras\n",
      "[INFO] - Added pergamos to sys.path from /Users/mbvalentin/scripts/netsurf/pergamos\n",
      "[INFO] - Loaded theme: default\n",
      "\u001b[48;2;141;47;102m\u001b[48;2;141;47;102m \u001b[48;2;117;39;85m \u001b[48;2;94;31;68m \u001b[48;2;70;23;51m \u001b[48;2;46;15;33m \u001b[48;2;23;7;17m \u001b[48;2;0;0;0m \u001b[48;2;23;7;17m \u001b[48;2;46;15;33m \u001b[48;2;70;23;51m \u001b[48;2;94;31;68m \u001b[48;2;117;39;85m \u001b[48;2;141;47;102m      \u001b[48;2;249;230;207m   â–  â– â–šâ–›â– â–›â–˜â–œâ–— \u001b[0m\n",
      "\u001b[48;2;141;47;102m\u001b[48;2;141;47;102m \u001b[48;2;117;39;85m \u001b[48;2;94;31;68m \u001b[48;2;70;23;51mğŒ\u001b[48;2;46;15;33m \u001b[48;2;23;7;17mâ† \u001b[48;2;0;0;0mâŒ¾\u001b[48;2;23;7;17mâ†\u001b[48;2;46;15;33m \u001b[48;2;70;23;51mğŒƒ\u001b[48;2;94;31;68m \u001b[48;2;117;39;85mğŒ–\u001b[48;2;141;47;102m ğŒ”    \u001b[48;2;250;198;122m     â– â––  â–â–š â–– â–— \u001b[0m\n",
      "\u001b[48;2;141;47;102m\u001b[48;2;141;47;102m \u001b[48;2;117;39;85m \u001b[48;2;94;31;68m \u001b[48;2;70;23;51m \u001b[48;2;46;15;33m \u001b[48;2;23;7;17m \u001b[48;2;0;0;0m \u001b[48;2;23;7;17m \u001b[48;2;46;15;33m \u001b[48;2;70;23;51m \u001b[48;2;94;31;68m \u001b[48;2;117;39;85m \u001b[48;2;141;47;102m      \u001b[48;2;250;198;122m â–  â–œ      â–—  â–š  \u001b[0m\n",
      "\n",
      "Logging to file: /Users/mbvalentin/.nodus/nodus_20250414_143358.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-- Date: 14/Apr/2025\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
      "â•° INFO â”€â”¤ 14:33:58.22 â”‚ - Nodus initialized\n",
      "        â”‚ 14:33:58.22 â”‚ - Nodus version: 0.1.0\n",
      "        â”‚ 14:33:58.23 â”‚ - Nodus imported\n",
      "        â”‚ 14:33:58.23 â”‚ - Jobs imported\n",
      "        â”‚ 14:33:58.23 â”‚ - JobManager imported\n",
      "        â”‚ 14:33:58.23 â”‚ - Nodus ready to use\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found config file: /Users/mbvalentin/.netsurf/config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        â”‚ 14:33:58.77 â”‚ - Created jobs table in NodusDB instance 'netsurf_db'\n",
      "        â”‚ 14:33:58.77 â”‚ - Created job_dependencies table in NodusDB instance 'netsurf_db'\n",
      "        â”‚ 14:33:58.77 â”‚ - Added NodusDB instance 'netsurf_db' linked to database 'netsurf_db'\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "#hls4ml_catapult_path = '/home/manuelbv/hls4ml_catapult'\n",
    "#sys.path = [hls4ml_catapult_path] + sys.path\n",
    "\n",
    "# Import datetime to get today's date\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\" Let's add our custom netsurf code \"\"\"\n",
    "import netsurf\n",
    "\n",
    "\"\"\" Get netsurf path \"\"\"\n",
    "import os \n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(netsurf.__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca54892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§® <QuantizationScheme(q<6,2,1>)> obj @ (0x31b25c1c0):\n",
      "    Total number of bits (m): 6\n",
      "    Integer bits         (n): 2\n",
      "    Float bits           (f): 3\n",
      "    Signed\n",
      "    Range: (-4, 3.875)\n",
      "    Min step: 0.125\n",
      "    Format: Sxx.xxx\n",
      "\n",
      "7_HLS_q6_2_1_bmark_mnist_fnn_lite_prune0_125.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° BMK â”€â”€â”¤ 14:34:02.74 â”‚ - Initializing benchmark object mnist_fnn_lite\n",
      "        â”‚ 14:34:02.74 â”‚ - Loss categorical_crossentropy found in tf.keras.losses with definition <function categorical_crossentropy at 0x1763a6560>.\n",
      "        â”‚ 14:34:02.74 â”‚ - Adding custom metric categorical_accuracy with definition CategoricalAccuracy(name=categorical_accuracy,dtype=float32).\n",
      "        â”‚ 14:34:05.38 â”‚ - Saving benchmark metadata to file /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/.metadata.netsurf\n",
      "        â”‚ 14:34:05.39 â”‚ - Saving quantization metadata to file /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/.metadata.netsurf\n",
      "        â”‚ 14:34:05.39 â”‚ - Saving model metadata to file /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/.metadata.netsurf\n",
      "        â”‚ 14:34:05.39 â”‚ - Benchmark object mnist_fnn_lite initialized\n",
      "/Users/mbvalentin/scripts/netsurf/tutorials/../netsurf/utils/plot.py:604: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n",
      "  pts = pd.DataFrame(shapes).T.plot.pie(ax = ax, subplots = True)\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° PLOT â”€â”¤ 14:34:11.45 â”‚ - Saved figure to /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/models/weights_pie.png\n",
      "        â”‚ 14:34:11.59 â”‚ - Variable fc0/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 14:34:11.61 â”‚ - Variable fc_out/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 14:34:11.62 â”‚ - Variable fc_out/bias:0 has sparsity 100.00%\n",
      "        â”‚ 14:34:11.64 â”‚ - Variable alpha0/beta:0 has sparsity 100.00%\n",
      "        â”‚ 14:34:11.65 â”‚ - Variable alpha0/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 14:34:11.67 â”‚ - Variable fc0/bias:0 has sparsity 100.00%\n",
      "        â”‚ 14:34:12.26 â”‚ - Saved sparsity plot to /var/folders/6b/p_271rqj4psdknf2z079285c0000gn/T/netsurf_sparsity_7egug9e.png\n",
      "        â”‚ 14:34:12.27 â”‚ - Plot range is -4.8259501457214355 to 4.531740188598633\n",
      "        â”‚ 14:34:12.38 â”‚ - Variable fc0/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 14:34:12.40 â”‚ - Variable fc0/bias:0 has sparsity 100.00%\n",
      "        â”‚ 14:34:12.41 â”‚ - Variable alpha0/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 14:34:12.42 â”‚ - Variable alpha0/beta:0 has sparsity 100.00%\n",
      "        â”‚ 14:34:12.44 â”‚ - Variable fc_out/kernel:0 has sparsity 0.00%\n",
      "        â”‚ 14:34:12.46 â”‚ - Variable fc_out/bias:0 has sparsity 100.00%\n",
      "        â”‚ 14:34:14.59 â”‚ - Saved sparsity plot to /var/folders/6b/p_271rqj4psdknf2z079285c0000gn/T/netsurf_sparsity_separatedegp9oxxa.png\n",
      "        â”‚ 14:34:14.59 â”‚ - Plot range is -4.8259501457214355 to 4.531740188598633\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° DATA â”€â”¤ 14:34:15.24 â”‚ - Normalizing dataset (input) using quantizer range (-4, 3.875)\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° PLT â”€â”€â”¤ 14:34:24.06 â”‚ - Plotting histogram for activation flatten\n",
      "        â”‚ 14:34:24.26 â”‚ - Plotting histogram for activation fc0\n",
      "        â”‚ 14:34:24.43 â”‚ - Plotting histogram for activation alpha0\n",
      "        â”‚ 14:34:24.57 â”‚ - Plotting histogram for activation act0\n",
      "        â”‚ 14:34:24.74 â”‚ - Plotting histogram for activation fc_out\n",
      "        â”‚ 14:34:24.87 â”‚ - Plotting histogram for activation softmax\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° BMK â”€â”€â”¤ 14:34:30.65 â”‚ - No path provided, looking for latest h5 file at default model path: /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/models\n",
      "        â”‚ 14:34:30.65 â”‚ - Looking for file /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/models/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax.keras.latest\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° WARN â”€â”¤ 14:34:30.65 â”‚ - Weights file /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/models/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax.keras.latest not found\n",
      "        â”‚ 14:34:30.65 â”‚ - No session files found at /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/sessions\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° MDL â”€â”€â”¤ 14:34:30.65 â”‚ - Running session with batch_size = 64, epochs = 20, opt_params = {'learning_rate': 0.001}, pruning_params = {'final_sparsity': 0.125, 'step': 2, 'end_epoch': 120}\n",
      "        â”‚ 14:34:30.65 â”‚ - Compiling model with parameters learning_rate=0.001\n",
      "        â”‚ 14:34:30.66 â”‚ - Fitting model with 20 epochs and batch_size 64\n",
      "        â”‚ 14:34:40.60 â”‚ - Epoch 0 - {'loss': 90.3592300415039, 'categorical_accuracy': 0.10859999805688858, 'val_loss': 31.24545669555664, 'val_categorical_accuracy': 0.14069999754428864, 'avg_alpha': 0.22730443, 'avg_beta': -0.06885612, 'loss_alpha_reg': 15.264018, 'fisher_trace': 6.694957746180675, 'fisher_entropy': 0.6490426848705549}\n",
      "        â”‚ 14:34:44.52 â”‚ - Epoch 1 - {'loss': 9.664284706115723, 'categorical_accuracy': 0.1420000046491623, 'val_loss': 2.2921407222747803, 'val_categorical_accuracy': 0.15449999272823334, 'avg_alpha': 0.0021441937, 'avg_beta': -0.10616972, 'loss_alpha_reg': 0.0, 'fisher_trace': 545.5847531206678, 'fisher_entropy': -9.506015299555651}\n",
      "        â”‚ 14:34:44.52 â”‚ - Applying pruning at 0.00% sparsity\n",
      "        â”‚ 14:34:47.97 â”‚ - Epoch 2 - {'loss': 2.254319190979004, 'categorical_accuracy': 0.17994999885559082, 'val_loss': 2.204538583755493, 'val_categorical_accuracy': 0.21549999713897705, 'avg_alpha': 0.0009470066, 'avg_beta': -0.08144423, 'loss_alpha_reg': 0.0, 'fisher_trace': 384.16265700251176, 'fisher_entropy': -11.352502309708369}\n",
      "        â”‚ 14:34:52.68 â”‚ - Epoch 3 - {'loss': 2.1778714656829834, 'categorical_accuracy': 0.2243500053882599, 'val_loss': 2.1214048862457275, 'val_categorical_accuracy': 0.2612999975681305, 'avg_alpha': 0.0010099467, 'avg_beta': -0.058056854, 'loss_alpha_reg': 0.0, 'fisher_trace': 416.39305392672685, 'fisher_entropy': -12.135261339412297}\n",
      "        â”‚ 14:34:52.69 â”‚ - Applying pruning at 0.21% sparsity\n",
      "        â”‚ 14:34:57.66 â”‚ - Epoch 4 - {'loss': 2.040580987930298, 'categorical_accuracy': 0.2857666611671448, 'val_loss': 1.9329637289047241, 'val_categorical_accuracy': 0.3273000121116638, 'avg_alpha': 0.0013705024, 'avg_beta': -0.0336817, 'loss_alpha_reg': 0.0, 'fisher_trace': 368.5943065393292, 'fisher_entropy': -11.791657487073339}\n",
      "        â”‚ 14:35:03.14 â”‚ - Epoch 5 - {'loss': 1.8133063316345215, 'categorical_accuracy': 0.3700166642665863, 'val_loss': 1.686163067817688, 'val_categorical_accuracy': 0.41019999980926514, 'avg_alpha': 0.0015321573, 'avg_beta': -0.010986445, 'loss_alpha_reg': 0.0, 'fisher_trace': 512.5246010018399, 'fisher_entropy': -11.90450506993105}\n",
      "        â”‚ 14:35:03.15 â”‚ - Applying pruning at 0.42% sparsity\n",
      "        â”‚ 14:35:06.85 â”‚ - Epoch 6 - {'loss': 1.5643961429595947, 'categorical_accuracy': 0.45401665568351746, 'val_loss': 1.4273573160171509, 'val_categorical_accuracy': 0.5073000192642212, 'avg_alpha': 0.0022649406, 'avg_beta': 0.015724728, 'loss_alpha_reg': 0.0, 'fisher_trace': 343.8195279780311, 'fisher_entropy': -11.534337481400721}\n",
      "        â”‚ 14:35:10.62 â”‚ - Epoch 7 - {'loss': 1.337308645248413, 'categorical_accuracy': 0.5328666567802429, 'val_loss': 1.217068076133728, 'val_categorical_accuracy': 0.5821999907493591, 'avg_alpha': 0.0024322625, 'avg_beta': 0.039537914, 'loss_alpha_reg': 0.0, 'fisher_trace': 357.4153551219756, 'fisher_entropy': -11.393759135435099}\n",
      "        â”‚ 14:35:10.62 â”‚ - Applying pruning at 0.64% sparsity\n",
      "        â”‚ 14:35:14.70 â”‚ - Epoch 8 - {'loss': 1.1390897035598755, 'categorical_accuracy': 0.6066833138465881, 'val_loss': 1.0306106805801392, 'val_categorical_accuracy': 0.6557000279426575, 'avg_alpha': 0.0022673283, 'avg_beta': 0.07011871, 'loss_alpha_reg': 0.0, 'fisher_trace': 357.0947809726705, 'fisher_entropy': -11.662044679550897}\n",
      "        â”‚ 14:35:18.24 â”‚ - Epoch 9 - {'loss': 0.9806852340698242, 'categorical_accuracy': 0.6666666865348816, 'val_loss': 0.9018335938453674, 'val_categorical_accuracy': 0.7016000151634216, 'avg_alpha': 0.0026697589, 'avg_beta': 0.094764255, 'loss_alpha_reg': 0.0, 'fisher_trace': 361.0731115615418, 'fisher_entropy': -11.530772263185124}\n",
      "        â”‚ 14:35:18.25 â”‚ - Applying pruning at 0.85% sparsity\n",
      "        â”‚ 14:35:22.13 â”‚ - Epoch 10 - {'loss': 0.8782861232757568, 'categorical_accuracy': 0.7085166573524475, 'val_loss': 0.8408060669898987, 'val_categorical_accuracy': 0.7278000116348267, 'avg_alpha': 0.0025426147, 'avg_beta': 0.11687522, 'loss_alpha_reg': 0.0, 'fisher_trace': 394.98057800949556, 'fisher_entropy': -11.579216818687014}\n",
      "        â”‚ 14:35:25.27 â”‚ - Epoch 11 - {'loss': 0.8082960247993469, 'categorical_accuracy': 0.7360333204269409, 'val_loss': 0.7596465945243835, 'val_categorical_accuracy': 0.7594000101089478, 'avg_alpha': 0.0023231213, 'avg_beta': 0.13572493, 'loss_alpha_reg': 0.0, 'fisher_trace': 334.2689706004062, 'fisher_entropy': -11.701564966616475}\n",
      "        â”‚ 14:35:25.27 â”‚ - Applying pruning at 1.06% sparsity\n",
      "        â”‚ 14:35:28.73 â”‚ - Epoch 12 - {'loss': 0.7564661502838135, 'categorical_accuracy': 0.7564499974250793, 'val_loss': 0.7439184784889221, 'val_categorical_accuracy': 0.7657999992370605, 'avg_alpha': 0.0028037855, 'avg_beta': 0.15433362, 'loss_alpha_reg': 0.006109488, 'fisher_trace': 305.1914573631749, 'fisher_entropy': -11.61084385596421}\n",
      "        â”‚ 14:35:31.78 â”‚ - Epoch 13 - {'loss': 0.7149333357810974, 'categorical_accuracy': 0.7728000283241272, 'val_loss': 0.6898941397666931, 'val_categorical_accuracy': 0.7849000096321106, 'avg_alpha': 0.0027199409, 'avg_beta': 0.17289202, 'loss_alpha_reg': 8.151382e-05, 'fisher_trace': 351.9832275618994, 'fisher_entropy': -11.574884958359831}\n",
      "        â”‚ 14:35:31.78 â”‚ - Applying pruning at 1.27% sparsity\n",
      "        â”‚ 14:35:34.95 â”‚ - Epoch 14 - {'loss': 0.6765819787979126, 'categorical_accuracy': 0.7881500124931335, 'val_loss': 0.6449800133705139, 'val_categorical_accuracy': 0.8011999726295471, 'avg_alpha': 0.0027382781, 'avg_beta': 0.18724354, 'loss_alpha_reg': 0.0002503708, 'fisher_trace': 270.753811587937, 'fisher_entropy': -11.768500377241532}\n",
      "        â”‚ 14:35:37.35 â”‚ - Epoch 15 - {'loss': 0.644960343837738, 'categorical_accuracy': 0.8004500269889832, 'val_loss': 0.6105732917785645, 'val_categorical_accuracy': 0.8149999976158142, 'avg_alpha': 0.002566739, 'avg_beta': 0.20338348, 'loss_alpha_reg': 0.0, 'fisher_trace': 269.02571993659217, 'fisher_entropy': -11.854335621245822}\n",
      "        â”‚ 14:35:37.36 â”‚ - Applying pruning at 1.48% sparsity\n",
      "        â”‚ 14:35:39.72 â”‚ - Epoch 16 - {'loss': 0.6160464286804199, 'categorical_accuracy': 0.8117666840553284, 'val_loss': 0.5948775410652161, 'val_categorical_accuracy': 0.8205999732017517, 'avg_alpha': 0.0027669838, 'avg_beta': 0.21742733, 'loss_alpha_reg': 0.0017324388, 'fisher_trace': 245.39946130154885, 'fisher_entropy': -11.750322879406443}\n",
      "        â”‚ 14:35:42.86 â”‚ - Epoch 17 - {'loss': 0.5913247466087341, 'categorical_accuracy': 0.8199666738510132, 'val_loss': 0.572365403175354, 'val_categorical_accuracy': 0.8306000232696533, 'avg_alpha': 0.0027085259, 'avg_beta': 0.23050475, 'loss_alpha_reg': 0.003334403, 'fisher_trace': 284.2426261821282, 'fisher_entropy': -11.742485470759839}\n",
      "        â”‚ 14:35:42.86 â”‚ - Applying pruning at 1.69% sparsity\n",
      "        â”‚ 14:35:45.14 â”‚ - Epoch 18 - {'loss': 0.5748757123947144, 'categorical_accuracy': 0.8260833621025085, 'val_loss': 0.5538872480392456, 'val_categorical_accuracy': 0.8328999876976013, 'avg_alpha': 0.0026477303, 'avg_beta': 0.24377215, 'loss_alpha_reg': 0.0011720002, 'fisher_trace': 299.4574142959619, 'fisher_entropy': -11.813337745582848}\n",
      "        â”‚ 14:35:47.43 â”‚ - Epoch 19 - {'loss': 0.5574164986610413, 'categorical_accuracy': 0.8323833346366882, 'val_loss': 0.5451886057853699, 'val_categorical_accuracy': 0.840399980545044, 'avg_alpha': 0.0029296568, 'avg_beta': 0.25596568, 'loss_alpha_reg': 0.010584036, 'fisher_trace': 241.56125272663303, 'fisher_entropy': -11.78287809056447}\n",
      "        â”‚ 14:35:47.43 â”‚ - Model fitted in 76.77 seconds after 20 epochs\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° BMK â”€â”€â”¤ 14:35:47.56 â”‚ - Session saved to file /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/sessions/training_session.20250414_143430/training_session.20250414_143430.pkl\n",
      "        â”‚ 14:35:47.56 â”‚ - Config saved to file /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/sessions/training_session.20250414_143430/config_training_session.20250414_143430.json\n",
      "        â”‚ 14:35:47.62 â”‚ - Weights saved to h5 file /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/models/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax.20250414_143547.weights.h5\n",
      "        â”‚ 14:35:47.68 â”‚ - Model saved to keras file /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/models/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax.20250414_143547.keras\n",
      "        â”‚ 14:35:47.69 â”‚ - Created a symlink to /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/models/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax.weights.h5.latest\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° PLOT â”€â”¤ 14:35:50.23 â”‚ - Saved training history plot to /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/sessions/training_session.20250414_143430/training_session.20250414_143430_training_history.png\n",
      "/Users/mbvalentin/scripts/netsurf/tutorials/../netsurf/utils/plot.py:604: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n",
      "  pts = pd.DataFrame(shapes).T.plot.pie(ax = ax, subplots = True)\n",
      "        â”‚ 14:35:50.49 â”‚ - Saved figure to /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/models/weights_pie.png\n",
      "        â”‚ 14:35:50.52 â”‚ - Variable fc0/kernel:0 has sparsity 1.70%\n",
      "        â”‚ 14:35:50.54 â”‚ - Variable fc_out/kernel:0 has sparsity 2.00%\n",
      "        â”‚ 14:35:50.55 â”‚ - Variable fc_out/bias:0 has sparsity 0.00%\n",
      "        â”‚ 14:35:50.56 â”‚ - Variable alpha0/beta:0 has sparsity 0.00%\n",
      "        â”‚ 14:35:50.57 â”‚ - Variable alpha0/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 14:35:50.58 â”‚ - Variable fc0/bias:0 has sparsity 0.00%\n",
      "        â”‚ 14:35:51.00 â”‚ - Saved sparsity plot to /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/sessions/training_session.20250414_143430/sparsity.png\n",
      "        â”‚ 14:35:51.01 â”‚ - Plot range is -4.463656425476074 to 4.461879730224609\n",
      "        â”‚ 14:35:51.07 â”‚ - Variable fc0/kernel:0 has sparsity 1.70%\n",
      "        â”‚ 14:35:51.08 â”‚ - Variable fc0/bias:0 has sparsity 0.00%\n",
      "        â”‚ 14:35:51.09 â”‚ - Variable alpha0/alpha:0 has sparsity 0.00%\n",
      "        â”‚ 14:35:51.10 â”‚ - Variable alpha0/beta:0 has sparsity 0.00%\n",
      "        â”‚ 14:35:51.11 â”‚ - Variable fc_out/kernel:0 has sparsity 2.00%\n",
      "        â”‚ 14:35:51.12 â”‚ - Variable fc_out/bias:0 has sparsity 0.00%\n",
      "        â”‚ 14:35:52.45 â”‚ - Saved sparsity plot to /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/sessions/training_session.20250414_143430/sparsity_separated.png\n",
      "        â”‚ 14:35:52.45 â”‚ - Plot range is -4.463656425476074 to 4.461879730224609\n",
      "        â”‚ 14:35:53.64 â”‚ - Baseline validation accuracy = 84.04%\n",
      "        â”‚ 14:35:54.34 â”‚ - Saved ROC plot to /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/sessions/training_session.20250414_143430/ROC.png\n",
      "        â”‚ 14:35:54.34 â”‚ - Plotting confusion matrix for labels ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "        â”‚ 14:35:54.81 â”‚ - Saved confusion matrix to /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/sessions/training_session.20250414_143430/confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "import pergamos as pg\n",
    "\n",
    "benchmark_name = 'mnist_fnn_lite'\n",
    "# Set variables\n",
    "qscheme = \"q<6,2,1>\"\n",
    "pruning = 0.125\n",
    "prerank = True\n",
    "\n",
    "\n",
    "\"\"\" First of all, let's define a quantization Scheme \"\"\"\n",
    "Q = netsurf.QuantizationScheme(qscheme)\n",
    "print(Q)\n",
    "\n",
    "# Set filename\n",
    "benchmarks_dir = os.path.join(parent_dir, 'benchmarks')\n",
    "datasets_dir = os.path.join(parent_dir, 'datasets')\n",
    "\n",
    "filename = f\"7_HLS_{Q._scheme_str.no_special_chars()}_bmark_{benchmark_name}_prune{str(pruning).replace('.','_')}.html\"\n",
    "print(filename)\n",
    "doc = pg.Document(filename, theme=\"default\")\n",
    "doc.required_scripts.add('mathjax')\n",
    "\n",
    "\n",
    "\"\"\" Add a title to the document \"\"\"\n",
    "doc.append(pg.Markdown(f\"\"\"# Benchmarks Quantization Assertion\n",
    "> Author: Manuel B Valentin\n",
    "\n",
    "> Creation date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "> Project: netsurf\n",
    "\n",
    "> Used packages: netsurf, tensorflow, numpy, matplotlib, pergamos\n",
    "        \n",
    "\"\"\"))\n",
    "\n",
    "# Add a tab container\n",
    "tabs = pg.TabbedContainer({'Motivation': [], \n",
    "                        'Pre-training analysis': [],\n",
    "                        'Training': [],\n",
    "                            'Post-Training': [],\n",
    "                        'BER Injection': [],\n",
    "                        'Conclusions': []})\n",
    "\n",
    "# Get individual tabs\n",
    "tabmotivation = tabs['Motivation']\n",
    "tabpretraining = tabs['Pre-training analysis']\n",
    "tabtraining = tabs['Training']\n",
    "tabposttraining = tabs['Post-Training']\n",
    "tabber = tabs['BER Injection']\n",
    "tabconclusions = tabs['Conclusions']\n",
    "\n",
    "# Add to documnt\n",
    "doc.append(tabs)\n",
    "\n",
    "# Add a markdown description of what we want to achieve with this report in the first tab\n",
    "md = r\"\"\"\n",
    "## 1. Loss Taylor expansion\n",
    "\n",
    "Given a loss function $\\mathcal{L}(w)$, where $w$ is the vector of all weights in the network, the Taylor expansion around some point $w_0$ (say, the trained weights) for a small perturbation $\\Delta w$ is:\n",
    "\n",
    "$$\\mathcal{L}(w_0 + \\Delta w) \\approx \\mathcal{L}(w_0) + \\nabla \\mathcal{L}(w_0)^T \\Delta w + \\frac{1}{2} \\Delta w^T H \\Delta w$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\nabla \\mathcal{L}(w_0)$ is the gradient vector of the loss at w_0\n",
    "* $H$ is the Hessian matrix, i.e. $H = \\nabla^2 \\mathcal{L}(w_0)$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. If the model is trained??\n",
    "\n",
    "If the model has been well trained, then:\n",
    "\n",
    "$\\nabla \\mathcal{L}(w_0) \\approx 0$\n",
    "\n",
    "Because you're sitting near a (local) minimum.\n",
    "\n",
    "This removes the linear term:\n",
    "$\\mathcal{L}(w_0 + \\Delta w) - \\mathcal{L}(w_0) \\approx \\frac{1}{2} \\Delta w^T H \\Delta w$\n",
    "\n",
    "So the change in loss caused by a perturbation $\\Delta w$ is approximately:\n",
    "$\\Delta \\mathcal{L} \\approx \\frac{1}{2} \\Delta w^T H \\Delta w$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Interpretation for bit flips\n",
    "\n",
    "A bit flip in the quantized weights causes a small but structured change in the weights:\n",
    "\n",
    "* Say, flipping the 3rd bit in weight $w_i$ causes it to change by $\\delta_i$, so:\n",
    "\n",
    "$\\Delta w = \\begin{bmatrix}\n",
    "0 \\\\ \\cdots \\\\ \\delta_i \\\\ \\cdots \\\\ 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Then the loss increase is (approximately):\n",
    "$\\Delta \\mathcal{L} \\approx \\frac{1}{2} \\delta_i^2 H_{ii}$\n",
    "\n",
    "If multiple bits are flipped across weights, you sum their pairwise interactions via H, including off-diagonal terms (if not ignored).\n",
    "\n",
    "--- \n",
    "\n",
    "## 4. Implications for ranking\n",
    "\n",
    "This approximation motivates ranking bit positions (or weights) by:\n",
    "\n",
    "* $\\delta^2 \\cdot H_{ii}$: bit-flip magnitude times curvature\n",
    "* This is the FKeras method: estimates $H_{ii}$ and ranks accordingly\n",
    "* You could generalize it to your method:\n",
    "$\\text{Impact} \\cdot H$, not just gradients\n",
    "\n",
    "---\n",
    "\n",
    "## 5. When does this approximation hold?\n",
    "\n",
    "?? Works well when:\n",
    "\n",
    "* Bit-flip magnitude is small (i.e., local region)\n",
    "* Model is near a minimum\n",
    "* Hessian is stable (not exploding)\n",
    "\n",
    "?? Fails when:\n",
    "\n",
    "* Model isn??t trained well (gradient is large)\n",
    "* Loss surface is highly non-quadratic\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "The formula:\n",
    "$\\Delta \\mathcal{L} \\approx \\frac{1}{2} \\Delta \\mathbf{w}^T H \\Delta \\mathbf{w}$\n",
    "\n",
    "tells us how bit-flips propagate into loss increases, and explains why the Hessian is so powerful for ranking robustness. \n",
    "It encodes:\n",
    "\n",
    "* How impactful a perturbation is (via $\\delta$)\n",
    "* How sensitive the loss is locally (via $H$)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create markdown\n",
    "md = pg.Markdown(md)\n",
    "\n",
    "# Add to the first tab\n",
    "tabmotivation.append(md)\n",
    "\n",
    "\"\"\" Add quantization container to doc report \"\"\"\n",
    "tabpretraining.extend(Q.html())\n",
    "\n",
    "\"\"\" Save doc to file (we save after adding each element) \"\"\"\n",
    "doc.save(filename)\n",
    "\n",
    "# Let's create a container for all benchmarks \n",
    "benchmark_ct = pg.CollapsibleContainer(\"ğŸ§º Benchmarks\", layout='vertical')\n",
    "\n",
    "# And another one for tabtraining\n",
    "benchmark_sessions_ct = pg.CollapsibleContainer(\"ğŸ§º Benchmarks\", layout='vertical')\n",
    "\n",
    "# And another one for tabposttraining\n",
    "benchmark_posttraining_ct = pg.CollapsibleContainer(\"ğŸ§º Benchmarks\", layout='vertical')\n",
    "\n",
    "# And yet another for \"BER Injection\"\n",
    "benchmark_ber_ct = pg.CollapsibleContainer(\"ğŸ§ª Experiments\", layout='vertical')\n",
    "\n",
    "\"\"\" Add to documnt \"\"\"\n",
    "tabpretraining.append(benchmark_ct)\n",
    "\n",
    "\"\"\" Add \"\"\"\n",
    "tabtraining.append(benchmark_sessions_ct)\n",
    "\n",
    "\"\"\" Add \"\"\"\n",
    "tabposttraining.append(benchmark_posttraining_ct)\n",
    "\n",
    "\"\"\" Add \"\"\"\n",
    "tabber.append(benchmark_ber_ct)\n",
    "\n",
    "# Define benchmarks to analyze\n",
    "#benchmarks = ['dummy', 'mnist_hls4ml', 'autompg', 'smartpixel_small', 'smartpixel_large',\n",
    "#              'cifar10', 'mnist_lenet5', 'ECONT_AE'\n",
    "# 'cifar100', 'svhn', 'fashion_mnist', 'imdb', 'reuters', 'boston_housing']\n",
    "# TODO: Fix visualization/contrast for cifar10\n",
    "# TODO: mnist_lenet5 seems to be working (good accuracy), but I'm not too happy about the alphas/betas. Some layers still have a big portion outside of the valid interval\n",
    "\n",
    "config_per_methods = netsurf.config.config_per_method\n",
    "protection_range = (0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4) #netsurf.config.DEFAULT_PROTECTION\n",
    "ber_range = (0.001, 0.005, 0.01, 0.05, 0.1, 0.15) #netsurf.config.DEFAULT_BER\n",
    "\n",
    "#methods = ['qpolar', 'qpolargrad', 'bitwise_msb', 'random', 'hirescam_norm', \n",
    "#           'hiresdelta', 'hessian', 'hessiandelta', 'weight_abs_value']\n",
    "methods = ['laplace', 'hessian', 'fisher', 'bitwise_msb', 'random', 'grad_norm', 'graddelta', 'weight_abs_value'] # 'qpolar', 'qpolargrad', \n",
    "methods = methods\n",
    "\n",
    "# Create benchmark object \n",
    "bmk = netsurf.get_benchmark(benchmark_name, Q,  benchmarks_dir = benchmarks_dir,\n",
    "                            datasets_dir = datasets_dir, pruning = pruning,\n",
    "                            load_weights = False)\n",
    "\n",
    "# Add benchmark html to container (this includes model + dataset htmls)\n",
    "# (run before training the model...)\n",
    "benchmark_ct.append(bmk.html())\n",
    "\n",
    "# Now let's prepare the data\n",
    "nsample_mod = 48 if 'ECON' in bmk.name else -1\n",
    "XYTrain = netsurf.utils.prepare_data(bmk, subset = 'train', nsample_mod = nsample_mod)\n",
    "\n",
    "# # Initialize the uncertainty profiler (pre-training)\n",
    "# pre_robustness_sgn_path = os.path.join(bmk.model_dir, 'uncertainty_signatures', f'{benchmark_name}.pretraining.netsurf.sgn')\n",
    "# try:\n",
    "#     pre_robustness_sgn = netsurf.UncertaintyProfiler.profile(bmk.model, XYTrain, bmk.model.loss, \n",
    "#                                                             batch_size = 2000, filepath = pre_robustness_sgn_path,\n",
    "#                                                             verbose = True)\n",
    "#     # Save \n",
    "#     pre_robustness_sgn.save_to_file(pre_robustness_sgn_path)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Add profile to tabpretraining\n",
    "# benchmark_ct.append(pre_robustness_sgn.html())\n",
    "\n",
    "# Now we can reload the weights (After the profiling)\n",
    "bmk.load_weights(verbose = True)\n",
    "\n",
    "# TRAINING - SESSION\n",
    "# Try to get a session (if not, train)\n",
    "sess = netsurf.get_training_session(bmk, show_plots = False, plot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb14804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " flatten (QQFlatten)         (None, 784)               1         \n",
      "                                                                 \n",
      " fc0 (QQDense)               (None, 10)                156991    \n",
      "                                                                 \n",
      " alpha0 (QQApplyAlpha)       (None, 10)                382       \n",
      "                                                                 \n",
      " act0 (QQActivation)         (None, 10)                1         \n",
      "                                                                 \n",
      " fc_out (QQDense)            (None, 10)                2191      \n",
      "                                                                 \n",
      " softmax (QQSoftmax)         (None, 10)                1         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 159,569\n",
      "Trainable params: 7,980\n",
      "Non-trainable params: 151,589\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bmk.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdd806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        â”‚ 14:57:14.98 â”‚ - Initialized RankingComparator with no rankers.\n",
      "        â”‚ 14:57:15.61 â”‚ - Loading ranking from /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/experiments/laplace/eNo9zLEOwjAMBNB_8ZyhZWCoEL9imdRtIzlOWzuAivh3goTYTqe79wK6WZHqjHeSyjD4XjkAWWQdk84wTCTWmht5XNDS0TZ913UBMvtSRhhAaBWKDAG07JmkbdBi2fl_1ppxqQ1IakXRKK_C9oUCbJXU00GeijZsu5zDKfTXplmdpvRsXcvVGEcWJyTDB6d58Z_-_gCd9USG/ranking.csv\n",
      "        â”‚ 14:57:15.61 â”‚ - Ranker laplace initialized and linked @ /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/experiments/laplace/eNo9zLEOwjAMBNB_8ZyhZWCoEL9imdRtIzlOWzuAivh3goTYTqe79wK6WZHqjHeSyjD4XjkAWWQdk84wTCTWmht5XNDS0TZ913UBMvtSRhhAaBWKDAG07JmkbdBi2fl_1ppxqQ1IakXRKK_C9oUCbJXU00GeijZsu5zDKfTXplmdpvRsXcvVGEcWJyTDB6d58Z_-_gCd9USG with loaded ranking from file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting base weight table for fnn...  6/6: [==============================>] [00:00:00]          \n",
      "Estimating diagonal Hessian              100/100: [==============================>] [00:00:58 < 00:00:00]\n",
      "Converged after 147 samples with relative change 9.33e-04\n",
      "Assigning diagonal Hessian               7980/7980: [==============================>] [00:00:00]          \n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "| Method: laplace                                                                                      |\n",
      "| Experiment: mnist_fnn_lite_laplace_q<6,2,1>_descending_num_hutchinson_samples_10_absolute_value_True |\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "| Ranking Config:                                                                                      |\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "|    method: laplace                                                                                   |\n",
      "|    ascending: False                                                                                  |\n",
      "|    num_hutchinson_samples: 10                                                                        |\n",
      "|    suffix:                                                                                           |\n",
      "|    quantization: q<6,2,1>                                                                            |\n",
      "|    normalize_score: False                                                                            |\n",
      "|    use_delta_as_weight: False                                                                        |\n",
      "|    batch_size: 1000                                                                                  |\n",
      "|    absolute_value: True                                                                              |\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° WARN â”€â”¤ 14:58:15.99 â”‚ - File /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/experiments/laplace/eNo9zLEOwjAMBNB_8ZyhZWCoEL9imdRtIzlOWzuAivh3goTYTqe79wK6WZHqjHeSyjD4XjkAWWQdk84wTCTWmht5XNDS0TZ913UBMvtSRhhAaBWKDAG07JmkbdBi2fl_1ppxqQ1IakXRKK_C9oUCbJXU00GeijZsu5zDKfTXplmdpvRsXcvVGEcWJyTDB6d58Z_-_gCd9USG/ranking.csv already exists. Use overwrite=True to overwrite it.\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â•° INFO â”€â”¤ 14:58:16.02 â”‚ - Loaded results from /Users/mbvalentin/scripts/netsurf/benchmarks/mnist_fnn_lite/q6_2_1/pruned_0.125_159569_qflatten_dense10u_qapplyalpha_quantized_relu_dense10u_qsoftmax/experiments/laplace/eNo9zLEOwjAMBNB_8ZyhZWCoEL9imdRtIzlOWzuAivh3goTYTqe79wK6WZHqjHeSyjD4XjkAWWQdk84wTCTWmht5XNDS0TZ913UBMvtSRhhAaBWKDAG07JmkbdBi2fl_1ppxqQ1IakXRKK_C9oUCbJXU00GeijZsu5zDKfTXplmdpvRsXcvVGEcWJyTDB6d58Z_-_gCd9USG/results.csv\n",
      "        â”‚ 14:58:16.02 â”‚ - No metrics found for experiment mnist_fnn_lite_laplace_q<6,2,1>_descending_num_hutchinson_samples_10_absolute_value_True\n",
      "        â”‚ 14:58:16.06 â”‚ - Experiment mnist_fnn_lite_laplace_q<6,2,1>_descending_num_hutchinson_samples_10_absolute_value_True started. Coverage: 0.00%\n",
      "        â”‚ 14:58:16.06 â”‚ - Bit flip variable fc0/kernel_N:0 reset to zero\n",
      "        â”‚ 14:58:16.07 â”‚ - Bit flip variable fc0/kernel_ranking:0 reset to zero\n",
      "        â”‚ 14:58:16.07 â”‚ - Bit flip variable fc0/bias_N:0 reset to zero\n",
      "        â”‚ 14:58:16.08 â”‚ - Bit flip variable fc0/bias_ranking:0 reset to zero\n",
      "        â”‚ 14:58:16.08 â”‚ - Bit flip variable alpha0/alpha_N:0 reset to zero\n",
      "        â”‚ 14:58:16.08 â”‚ - Bit flip variable alpha0/alpha_ranking:0 reset to zero\n",
      "        â”‚ 14:58:16.09 â”‚ - Bit flip variable alpha0/beta_N:0 reset to zero\n",
      "        â”‚ 14:58:16.09 â”‚ - Bit flip variable alpha0/beta_ranking:0 reset to zero\n",
      "        â”‚ 14:58:16.10 â”‚ - Bit flip variable fc_out/kernel_N:0 reset to zero\n",
      "        â”‚ 14:58:16.10 â”‚ - Bit flip variable fc_out/kernel_ranking:0 reset to zero\n",
      "        â”‚ 14:58:16.10 â”‚ - Bit flip variable fc_out/bias_N:0 reset to zero\n",
      "        â”‚ 14:58:16.11 â”‚ - Bit flip variable fc_out/bias_ranking:0 reset to zero\n",
      "        â”‚ 14:58:16.11 â”‚ - Bit flip counters reset to zero using the method method \"reset_bit_flip_counter\"\n",
      "        â”‚ 14:58:16.32 â”‚ - Applying ranking to fc0/kernel:0 with shape (784, 10) and rank (784, 10, 6)\n",
      "        â”‚ 14:58:16.51 â”‚ - Applying ranking to fc0/bias:0 with shape (10,) and rank (10, 6)\n",
      "        â”‚ 14:58:16.66 â”‚ - Applying ranking to alpha0/alpha:0 with shape (10,) and rank (10, 6)\n",
      "        â”‚ 14:58:16.78 â”‚ - Applying ranking to alpha0/beta:0 with shape (10,) and rank (10, 6)\n",
      "        â”‚ 14:58:16.93 â”‚ - Applying ranking to fc_out/kernel:0 with shape (10, 10) and rank (10, 10, 6)\n",
      "        â”‚ 14:58:17.05 â”‚ - Applying ranking to fc_out/bias:0 with shape (10,) and rank (10, 6)\n",
      "        â”‚ 14:58:17.05 â”‚ - Applied ranking to model fnn with 6 variables\n",
      "        â”‚ 14:58:17.05 â”‚ - Injector created in 0.99 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=================================================================================================================================================|\n",
      "| Iteration | Method | Protection(%) | Bit Error Rate(%) | Elapsed time | Remain. time | Cat. XEntropy | Accuracy (%) | True BER(%) | Coverage(%) |\n",
      "|===========|========|===============|===================|==============|==============|===============|==============|=============|=============|\n",
      "|   0/300   | laplac |     3.79%     |       9.25%       |   00:00:16   |   00:10:51   | 7.9769 Â± 2.52 | 24.04% Â± 10. |  0.01539%   |    2.5%     |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbvalentin/scripts/netsurf/tutorials/../netsurf/core/experiments.py:1094: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.results.data = pd.concat([self.results.data, pd.DataFrame(T)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   1/300   | laplac |    21.72%     |      13.84%       |   00:00:28   |   00:09:22   | 7.6752 Â± 3.01 | 26.98% Â± 14. |  0.04196%   |    5.0%     |\n",
      "|   2/300   | laplac |    18.85%     |       6.75%       |   00:00:47   |   00:10:35   | 5.3666 Â± 3.02 | 37.62% Â± 17. |  0.05175%   |    7.5%     |\n",
      "|   3/300   | laplac |    17.46%     |       9.85%       |   00:01:05   |   00:14:32   | 6.3524 Â± 2.40 | 31.39% Â± 13. |  0.06730%   |    7.5%     |\n",
      "|   4/300   | laplac |     2.05%     |       9.97%       |   00:01:17   |   00:17:12   | 8.6017 Â± 2.31 | 21.36% Â± 8.7 |  0.07335%   |    7.5%     |\n",
      "|   5/300   | laplac |    33.65%     |       0.97%       |   00:01:35   |   00:15:58   | 1.1591 Â± 1.17 | 75.51% Â± 12. |  0.10991%   |    10.0%    |\n",
      "|   6/300   | laplac |    22.65%     |      10.23%       |   00:01:50   |   00:18:29   | 6.2797 Â± 2.83 | 31.71% Â± 16. |  0.11133%   |    10.0%    |\n",
      "|   7/300   | laplac |    14.00%     |       8.26%       |   00:02:02   |   00:16:22   | 7.2508 Â± 2.65 | 27.19% Â± 12. |  0.11391%   |    12.5%    |\n",
      "|   8/300   | laplac |     8.62%     |       8.34%       |   00:02:23   |   00:15:56   | 7.3807 Â± 2.59 | 26.42% Â± 12. |  0.12111%   |    15.0%    |\n",
      "|   9/300   | laplac |    17.88%     |      11.25%       |   00:02:39   |   00:15:09   | 7.2448 Â± 2.73 | 27.09% Â± 14. |  0.15343%   |    17.5%    |\n",
      "|  10/300   | laplac |     6.54%     |       8.95%       |   00:02:51   |   00:16:17   | 7.8462 Â± 2.75 | 25.63% Â± 12. |  0.14975%   |    17.5%    |\n",
      "|  11/300   | laplac |    22.46%     |       9.92%       |   00:03:02   |   00:15:13   | 6.0595 Â± 2.61 | 33.30% Â± 15. |  0.19708%   |    20.0%    |\n",
      "|  12/300   | laplac |    31.72%     |       4.05%       |   00:03:14   |   00:14:24   | 3.2779 Â± 2.49 | 52.47% Â± 20. |  0.23055%   |    22.5%    |\n",
      "|  13/300   | laplac |    17.32%     |       2.57%       |   00:03:26   |   00:13:45   | 2.9083 Â± 2.58 | 56.26% Â± 21. |  0.19468%   |    25.0%    |\n",
      "|  14/300   | laplac |    39.78%     |       6.40%       |   00:03:38   |   00:13:14   | 3.1243 Â± 2.22 | 56.99% Â± 15. |  0.27794%   |    27.5%    |\n",
      "|  15/300   | laplac |    30.75%     |      12.57%       |   00:03:50   |   00:12:49   | 6.7727 Â± 2.75 | 29.12% Â± 14. |  0.26259%   |    30.0%    |\n",
      "|  16/300   | laplac |    19.28%     |       8.60%       |   00:04:02   |   00:13:28   | 5.9163 Â± 2.98 | 33.79% Â± 17. |  0.23961%   |    30.0%    |\n",
      "|  17/300   | laplac |     1.06%     |      13.92%       |   00:04:14   |   00:13:02   | 10.1366 Â± 2.1 | 17.25% Â± 6.7 |  0.21873%   |    32.5%    |\n",
      "|  18/300   | laplac |    36.88%     |       8.66%       |   00:04:25   |   00:13:38   | 3.5377 Â± 2.39 | 52.74% Â± 15. |  0.35726%   |    32.5%    |\n",
      "|  19/300   | laplac |    31.61%     |      11.33%       |   00:04:37   |   00:14:14   | 6.7130 Â± 2.83 | 29.88% Â± 16. |  0.34862%   |    32.5%    |\n",
      "|  20/300   | laplac |    39.59%     |       7.47%       |   00:04:49   |   00:14:50   | 2.9009 Â± 1.95 | 57.81% Â± 14. |  0.40712%   |    32.5%    |\n",
      "|  21/300   | laplac |     4.18%     |      14.92%       |   00:05:01   |   00:15:27   | 9.9090 Â± 2.06 | 18.33% Â± 7.6 |  0.28153%   |    32.5%    |\n",
      "|  22/300   | laplac |    30.28%     |       4.37%       |   00:05:13   |   00:16:03   | 3.3117 Â± 2.38 | 52.48% Â± 19. |  0.39422%   |    32.5%    |\n",
      "|  23/300   | laplac |    37.66%     |       3.99%       |   00:05:25   |   00:15:29   | 2.0756 Â± 1.69 | 65.52% Â± 14. |  0.44755%   |    35.0%    |\n",
      "|  24/300   | laplac |    10.74%     |       4.98%       |   00:05:36   |   00:14:58   | 5.2671 Â± 2.70 | 36.21% Â± 16. |  0.32086%   |    37.5%    |\n",
      "|  25/300   | laplac |    30.11%     |      14.31%       |   00:05:49   |   00:15:31   | 7.7105 Â± 2.70 | 24.96% Â± 13. |  0.43361%   |    37.5%    |\n",
      "|  26/300   | laplac |    22.42%     |       7.65%       |   00:06:00   |   00:16:02   | 5.6846 Â± 3.04 | 34.46% Â± 17. |  0.40339%   |    37.5%    |\n",
      "|  27/300   | laplac |    24.67%     |       1.96%       |   00:06:12   |   00:15:32   | 2.1606 Â± 2.09 | 63.49% Â± 19. |  0.41875%   |    40.0%    |\n",
      "|  28/300   | laplac |    12.80%     |       6.77%       |   00:06:39   |   00:16:37   | 5.2990 Â± 2.49 | 35.92% Â± 14. |  0.37301%   |    40.0%    |\n",
      "|  29/300   | laplac |     8.44%     |      12.12%       |   00:06:57   |   00:16:23   | 8.9760 Â± 2.54 | 21.76% Â± 9.1 |  0.37545%   |    42.5%    |\n",
      "|  30/300   | laplac |     1.88%     |      12.89%       |   00:07:17   |   00:17:09   | 9.4964 Â± 2.24 | 19.65% Â± 7.8 |  0.37181%   |    42.5%    |\n",
      "|  31/300   | laplac |    23.30%     |      12.79%       |   00:07:50   |   00:18:27   | 7.5747 Â± 2.63 | 25.82% Â± 13. |  0.49698%   |    42.5%    |\n",
      "|  32/300   | laplac |    38.35%     |       3.06%       |   00:08:32   |   00:20:04   | 1.6953 Â± 1.75 | 70.00% Â± 14. |  0.62336%   |    42.5%    |\n",
      "|  33/300   | laplac |    11.24%     |      10.64%       |   00:09:10   |   00:20:24   | 7.9363 Â± 2.52 | 25.11% Â± 10. |  0.45069%   |    45.0%    |\n",
      "|  34/300   | laplac |    28.46%     |      14.85%       |   00:09:32   |   00:20:05   | 7.3619 Â± 2.96 | 26.97% Â± 14. |  0.58385%   |    47.5%    |\n",
      "|  35/300   | laplac |     7.86%     |       6.15%       |   00:10:01   |   00:21:06   | 5.9565 Â± 2.74 | 32.35% Â± 13. |  0.46358%   |    47.5%    |\n",
      "|  36/300   | laplac |    32.31%     |      14.03%       |   00:10:14   |   00:21:34   | 7.0143 Â± 2.75 | 28.72% Â± 16. |  0.65448%   |    47.5%    |\n",
      "|  37/300   | laplac |    13.97%     |      12.37%       |   00:10:35   |   00:22:17   | 8.3359 Â± 2.40 | 23.88% Â± 10. |  0.53555%   |    47.5%    |\n",
      "|  38/300   | laplac |    33.83%     |      11.44%       |   00:10:51   |   00:22:52   | 4.3067 Â± 2.14 | 46.61% Â± 14. |  0.71534%   |    47.5%    |\n",
      "|  39/300   | laplac |    26.16%     |       5.44%       |   00:11:12   |   00:22:24   | 4.3429 Â± 2.94 | 44.32% Â± 20. |  0.65014%   |    50.0%    |\n",
      "|  40/300   | laplac |     3.36%     |       8.43%       |   00:11:35   |   00:23:10   | 7.7275 Â± 2.59 | 24.92% Â± 10. |  0.51075%   |    50.0%    |\n",
      "|  41/300   | laplac |    10.27%     |      12.29%       |   00:11:53   |   00:23:47   | 9.0346 Â± 2.52 | 21.17% Â± 10. |  0.57058%   |    50.0%    |\n",
      "|  42/300   | laplac |    28.65%     |      14.00%       |   00:12:14   |   00:24:28   | 7.7199 Â± 2.66 | 25.20% Â± 13. |  0.74091%   |    50.0%    |\n",
      "|  43/300   | laplac |    15.42%     |      10.87%       |   00:12:29   |   00:24:59   | 7.3749 Â± 2.60 | 27.02% Â± 11. |  0.64312%   |    50.0%    |\n",
      "|  44/300   | laplac |    23.47%     |       2.76%       |   00:12:44   |   00:25:29   | 2.8243 Â± 2.56 | 57.70% Â± 21. |  0.71533%   |    50.0%    |\n",
      "|  45/300   | laplac |    27.82%     |       8.22%       |   00:12:59   |   00:25:59   | 5.7949 Â± 2.77 | 34.56% Â± 17. |  0.77215%   |    50.0%    |\n",
      "|  46/300   | laplac |    35.18%     |       4.28%       |   00:13:12   |   00:26:25   | 2.4037 Â± 1.93 | 62.36% Â± 15. |  0.86697%   |    50.0%    |\n",
      "|  47/300   | laplac |    32.60%     |       7.63%       |   00:13:26   |   00:25:35   | 5.4640 Â± 3.04 | 36.83% Â± 19. |  0.84647%   |    52.5%    |\n",
      "|  48/300   | laplac |     5.48%     |       4.53%       |   00:13:40   |   00:24:51   | 5.4910 Â± 2.85 | 37.12% Â± 15. |  0.61115%   |    55.0%    |\n",
      "|  49/300   | laplac |    19.69%     |       3.64%       |   00:14:00   |   00:25:28   | 3.4558 Â± 2.86 | 51.60% Â± 22. |  0.72534%   |    55.0%    |\n",
      "|  50/300   | laplac |     4.39%     |       0.37%       |   00:14:16   |   00:24:49   | 1.0297 Â± 1.10 | 75.41% Â± 14. |  0.60991%   |    57.5%    |\n",
      "|  51/300   | laplac |     5.05%     |      11.57%       |   00:14:32   |   00:25:17   | 8.5582 Â± 2.37 | 22.30% Â± 9.5 |  0.63341%   |    57.5%    |\n",
      "|  52/300   | laplac |    16.22%     |       8.51%       |   00:14:56   |   00:25:58   | 6.1565 Â± 2.47 | 30.92% Â± 12. |  0.73205%   |    57.5%    |\n",
      "|  53/300   | laplac |     2.51%     |       6.90%       |   00:15:10   |   00:26:22   | 6.9743 Â± 2.78 | 27.91% Â± 11. |  0.64066%   |    57.5%    |\n",
      "|  54/300   | laplac |     4.53%     |       5.50%       |   00:15:21   |   00:26:43   | 6.5467 Â± 2.95 | 31.29% Â± 15. |  0.66341%   |    57.5%    |\n",
      "|  55/300   | laplac |    23.89%     |       6.04%       |   00:15:33   |   00:27:03   | 4.5083 Â± 2.35 | 43.55% Â± 16. |  0.84218%   |    57.5%    |\n",
      "|  56/300   | laplac |    28.18%     |       2.53%       |   00:15:45   |   00:26:15   | 2.4726 Â± 2.35 | 60.07% Â± 21. |  0.89672%   |    60.0%    |\n",
      "|  57/300   | laplac |    17.68%     |       3.02%       |   00:15:57   |   00:26:35   | 2.5279 Â± 2.09 | 59.76% Â± 17. |  0.78732%   |    60.0%    |\n",
      "|  58/300   | laplac |    17.15%     |       2.90%       |   00:16:09   |   00:26:56   | 2.8250 Â± 2.31 | 56.64% Â± 18. |  0.78713%   |    60.0%    |\n",
      "|  59/300   | laplac |    25.78%     |       1.31%       |   00:16:20   |   00:27:14   | 1.3969 Â± 1.44 | 72.22% Â± 14. |  0.88082%   |    60.0%    |\n",
      "|  60/300   | laplac |    38.63%     |       5.94%       |   00:16:32   |   00:27:34   | 3.1459 Â± 2.37 | 56.27% Â± 16. |  1.07521%   |    60.0%    |\n",
      "|  61/300   | laplac |    35.33%     |       0.71%       |   00:16:43   |   00:26:45   | 0.8582 Â± 0.67 | 78.90% Â± 8.0 |  1.02146%   |    62.5%    |\n",
      "|  62/300   | laplac |    23.70%     |      14.67%       |   00:16:53   |   00:27:00   | 8.5924 Â± 2.71 | 21.72% Â± 13. |  0.89022%   |    62.5%    |\n",
      "|  63/300   | laplac |    22.98%     |       1.67%       |   00:17:02   |   00:27:16   | 1.8123 Â± 1.93 | 67.14% Â± 20. |  0.88475%   |    62.5%    |\n",
      "|  64/300   | laplac |    30.41%     |      13.26%       |   00:17:11   |   00:27:31   | 7.8007 Â± 2.72 | 24.33% Â± 13. |  1.00126%   |    62.5%    |\n",
      "|  65/300   | laplac |    36.44%     |      12.46%       |   00:17:21   |   00:26:42   | 4.9207 Â± 2.30 | 42.62% Â± 14. |  1.11707%   |    65.0%    |\n",
      "|  66/300   | laplac |    28.03%     |       8.78%       |   00:17:31   |   00:26:57   | 6.3282 Â± 2.88 | 31.81% Â± 17. |  1.00115%   |    65.0%    |\n",
      "|  67/300   | laplac |     7.65%     |      13.05%       |   00:17:41   |   00:27:13   | 9.6670 Â± 2.21 | 19.38% Â± 8.7 |  0.80201%   |    65.0%    |\n",
      "|  68/300   | laplac |    15.99%     |      11.66%       |   00:17:51   |   00:27:29   | 8.2305 Â± 2.57 | 23.85% Â± 11. |  0.90103%   |    65.0%    |\n",
      "|  69/300   | laplac |    10.94%     |       4.42%       |   00:18:01   |   00:27:44   | 5.5020 Â± 2.57 | 35.42% Â± 15. |  0.85729%   |    65.0%    |\n",
      "|  70/300   | laplac |    34.88%     |       6.65%       |   00:18:11   |   00:27:59   | 2.8777 Â± 1.93 | 57.46% Â± 14. |  1.18355%   |    65.0%    |\n",
      "|  71/300   | laplac |    14.82%     |      12.05%       |   00:18:21   |   00:28:13   | 8.4765 Â± 2.45 | 23.15% Â± 9.3 |  0.92491%   |    65.0%    |\n",
      "|  72/300   | laplac |    25.29%     |       8.57%       |   00:18:30   |   00:28:28   | 5.5726 Â± 2.63 | 35.56% Â± 17. |  1.06883%   |    65.0%    |\n",
      "|  73/300   | laplac |    39.11%     |      12.21%       |   00:18:39   |   00:28:42   | 4.6211 Â± 2.39 | 44.83% Â± 15. |  1.33170%   |    65.0%    |\n",
      "|  74/300   | laplac |    16.36%     |       6.98%       |   00:18:49   |   00:28:56   | 6.2115 Â± 2.76 | 32.13% Â± 14. |  0.98110%   |    65.0%    |\n",
      "|  75/300   | laplac |     5.17%     |       9.99%       |   00:19:02   |   00:29:17   | 7.7981 Â± 2.52 | 25.42% Â± 10. |  0.88203%   |    65.0%    |\n",
      "|  76/300   | laplac |    30.81%     |      11.68%       |   00:19:14   |   00:29:36   | 7.3379 Â± 2.88 | 26.30% Â± 14. |  1.22832%   |    65.0%    |\n",
      "|  77/300   | laplac |    34.98%     |       3.36%       |   00:19:25   |   00:29:52   | 2.0222 Â± 1.88 | 66.81% Â± 15. |  1.31267%   |    65.0%    |\n",
      "|  78/300   | laplac |    16.92%     |       8.37%       |   00:19:42   |   00:30:18   | 6.0044 Â± 2.77 | 33.77% Â± 15. |  1.04126%   |    65.0%    |\n",
      "|  79/300   | laplac |     3.15%     |       7.79%       |   00:20:18   |   00:31:15   | 7.9320 Â± 2.64 | 24.62% Â± 11. |  0.90627%   |    65.0%    |\n",
      "|  80/300   | laplac |    39.41%     |      10.53%       |   00:21:05   |   00:32:26   | 4.4638 Â± 2.55 | 47.01% Â± 15. |  1.46601%   |    65.0%    |\n",
      "|  81/300   | laplac |     1.99%     |       2.13%       |   00:21:41   |   00:32:08   | 2.9428 Â± 2.54 | 53.72% Â± 19. |  0.90986%   |    67.5%    |\n",
      "|  82/300   | laplac |    34.64%     |       6.00%       |   00:22:04   |   00:32:42   | 2.6135 Â± 1.88 | 60.29% Â± 14. |  1.37439%   |    67.5%    |\n",
      "|  83/300   | laplac |    15.66%     |       5.04%       |   00:22:22   |   00:33:08   | 5.1062 Â± 2.66 | 36.75% Â± 16. |  1.07355%   |    67.5%    |\n",
      "|  84/300   | laplac |    18.28%     |      13.59%       |   00:22:46   |   00:33:44   | 7.7136 Â± 2.57 | 24.60% Â± 12. |  1.13059%   |    67.5%    |\n",
      "|  85/300   | laplac |    14.37%     |       9.62%       |   00:23:14   |   00:34:25   | 7.6086 Â± 2.77 | 26.70% Â± 12. |  1.08877%   |    67.5%    |\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a container for this bmk in tabtraining\n",
    "bmk_sess_ct = pg.CollapsibleContainer(benchmark_name, layout='vertical')\n",
    "\n",
    "# Add session to tabtraining\n",
    "bmk_sess_ct.append(sess.html())\n",
    "\n",
    "# And add to benchmarks in tabtraining\n",
    "benchmark_sessions_ct.append(bmk_sess_ct)\n",
    "\n",
    "# Add benchmark again to post-training to check how the weights changed\n",
    "#benchmark_posttraining_ct.append(bmk.html())\n",
    "\n",
    "# Initialize the uncertainty profiler (post-training)\n",
    "# try:\n",
    "#     post_robustness_sgn_path = os.path.join(bmk.model_dir, 'uncertainty_signatures', f'{benchmark_name}.posttraining.netsurf.sgn')\n",
    "#     post_robustness_sgn = netsurf.UncertaintyProfiler.profile(bmk.model, XYTrain, bmk.model.loss, \n",
    "#                                     batch_size = 2000, verbose = True, filepath = post_robustness_sgn_path)\n",
    "\n",
    "#     # Save \n",
    "#     post_robustness_sgn.save_to_file(post_robustness_sgn_path)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Add profile to tabposttraining\n",
    "# benchmark_posttraining_ct.append(post_robustness_sgn.html())\n",
    "\n",
    "# Let's compare the divergence between the profiles\n",
    "# div_profile = netsurf.ProfileDivergence.from_signatures(pre_robustness_sgn, post_robustness_sgn)\n",
    "\n",
    "#div_profile.plot_divergence_summary()\n",
    "#div_profile.plot_advanced_divergence_summary()\n",
    "\n",
    "\"\"\" Save doc to file (we save after adding each element) \"\"\"\n",
    "doc.save(filename)\n",
    "\n",
    "# Create a container for this benchmark in \"BER Injection\"\n",
    "bmk_ber_ct = pg.CollapsibleContainer(benchmark_name, layout='vertical')\n",
    "\n",
    "# Add to tabber\n",
    "benchmark_ber_ct.append(bmk_ber_ct)\n",
    "\n",
    "# Create another container to store the individual results of the experiments\n",
    "exp_individual_ct = pg.CollapsibleContainer(\"ğŸ Individual results\", layout='vertical')\n",
    "bmk_ber_ct.append(exp_individual_ct)\n",
    "\n",
    "# Create our ranker profiler (empty)\n",
    "rankers_bucket = netsurf.RankingComparator()\n",
    "\n",
    "# Loop thru methods\n",
    "exps = {}\n",
    "for method_alias in methods:\n",
    "    #########################################################################\n",
    "    # 0. Create the directory for the experiment (ranking, results, etc.)\n",
    "    #########################################################################\n",
    "    method = config_per_methods[method_alias]['method']\n",
    "    method_dir = os.path.join(bmk.experiments_dir, method)\n",
    "\n",
    "    #########################################################################\n",
    "    # 1. Create the ranker\n",
    "    #########################################################################\n",
    "    rkr = rankers_bucket.build_ranker(method, Q, config = config_per_methods[method], \n",
    "                                    path = method_dir,\n",
    "                                    is_baseline = (method == 'qpolargrad'))\n",
    "\n",
    "    # Get the exp dir from the ranker (with the hash)\n",
    "    exp_dir = rkr.path\n",
    "\n",
    "    #################################################################\n",
    "    # 1. Perform ranking according to method\n",
    "    #################################################################\n",
    "    # Rank weights \n",
    "    ranking = rkr.rank(bmk.model, *XYTrain, verbose = True)\n",
    "    # Save rank to csv file \n",
    "    ranking.save(overwrite = False)\n",
    "\n",
    "    #################################################################\n",
    "    # 2. Create experiment object\n",
    "    #################################################################\n",
    "    exp = netsurf.Experiment(bmk, ranking, num_reps = 100, \n",
    "                            ber_range = ber_range, \n",
    "                            protection_range = protection_range, \n",
    "                            path = exp_dir,\n",
    "                            verbose = True)\n",
    "\n",
    "    # Print experiment info \n",
    "    print(exp)\n",
    "\n",
    "    #################################################################\n",
    "    # 3. Run experiment with given ranking and for whatever \n",
    "    #       range of protection and rad \n",
    "    #################################################################\n",
    "    #batch_size = 1000,\n",
    "    exp.run_experiment(bmk, XYTrain,\n",
    "                    batch_size = None,\n",
    "                    ber_range = ber_range, \n",
    "                    protection_range = protection_range, \n",
    "                    rerun = False)\n",
    "    \n",
    "    # Add experiment to container\n",
    "    exp_individual_ct.append(exp.html())\n",
    "\n",
    "    # Save experiment object\n",
    "    exp.save()\n",
    "\n",
    "    # Add to dict\n",
    "    exps[method] = exp\n",
    "\n",
    "    \"\"\" Save doc to file (we save after adding each element) \"\"\"\n",
    "    doc.save(filename)\n",
    "\n",
    "\"\"\" Now perform the comparison of rankers \"\"\"\n",
    "#df = rankers_bucket.compare_rankers(granularity = 0.01, bootstrap = False)\n",
    "\n",
    "# Add comparison to container\n",
    "#bmk_ber_ct.append(rankers_bucket.html())\n",
    "\n",
    "# Save doc \n",
    "doc.save(filename)\n",
    "\n",
    "# # Save the rankers_bucket\n",
    "# rankers_comparison_filepath = os.path.join(bmk.experiments_dir, 'ranking_comparison.csv')\n",
    "# rankers_bucket.save_to_csv(rankers_comparison_filepath)\n",
    "\n",
    "# \"\"\" Create Experiment Comparator \"\"\"\n",
    "# exp_comp = netsurf.ExperimentComparator(list(exps.values()))\n",
    "#df = ExperimentComparator.compute_ranking_distribution(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6af04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsbmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
