{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca78130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding /Users/mbvalentin/scripts/netsurf to sys.path\n",
      "[INFO] - Added qkeras to sys.path from /Users/mbvalentin/scripts/netsurf/qkeras\n",
      "[INFO] - Added pergamos to sys.path from /Users/mbvalentin/scripts/netsurf/pergamos\n",
      "[INFO] - Loaded theme: default\n",
      "\u001b[48;2;141;47;102m\u001b[48;2;141;47;102m \u001b[48;2;117;39;85m \u001b[48;2;94;31;68m \u001b[48;2;70;23;51m \u001b[48;2;46;15;33m \u001b[48;2;23;7;17m \u001b[48;2;0;0;0m \u001b[48;2;23;7;17m \u001b[48;2;46;15;33m \u001b[48;2;70;23;51m \u001b[48;2;94;31;68m \u001b[48;2;117;39;85m \u001b[48;2;141;47;102m      \u001b[48;2;249;230;207m ▚ ▞▛ ▝   ▘   ▞  \u001b[0m\n",
      "\u001b[48;2;141;47;102m\u001b[48;2;141;47;102m \u001b[48;2;117;39;85m \u001b[48;2;94;31;68m \u001b[48;2;70;23;51m𐌍\u001b[48;2;46;15;33m \u001b[48;2;23;7;17m↠\u001b[48;2;0;0;0m⌾\u001b[48;2;23;7;17m↞\u001b[48;2;46;15;33m \u001b[48;2;70;23;51m𐌃\u001b[48;2;94;31;68m \u001b[48;2;117;39;85m𐌖\u001b[48;2;141;47;102m 𐌔    \u001b[48;2;250;198;122m ▝▗▞ ▖ ▙ ▞   ▜▛  \u001b[0m\n",
      "\u001b[48;2;141;47;102m\u001b[48;2;141;47;102m \u001b[48;2;117;39;85m \u001b[48;2;94;31;68m \u001b[48;2;70;23;51m \u001b[48;2;46;15;33m \u001b[48;2;23;7;17m \u001b[48;2;0;0;0m \u001b[48;2;23;7;17m \u001b[48;2;46;15;33m \u001b[48;2;70;23;51m \u001b[48;2;94;31;68m \u001b[48;2;117;39;85m \u001b[48;2;141;47;102m      \u001b[48;2;250;198;122m ▜  ▛ ▞ ▙  ▜     \u001b[0m\n",
      "\n",
      "Logging to file: /Users/mbvalentin/.nodus/nodus_20250415_233942.log\n",
      "Found config file: /Users/mbvalentin/.netsurf/config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-- Date: 15/Apr/2025\n",
      "╭───────┬─────────────╮\n",
      "╰ INFO ─┤ 23:39:42.26 │ - Nodus initialized\n",
      "        │ 23:39:42.26 │ - Nodus version: 0.1.0\n",
      "        │ 23:39:42.26 │ - Nodus imported\n",
      "        │ 23:39:42.26 │ - Jobs imported\n",
      "        │ 23:39:42.26 │ - JobManager imported\n",
      "        │ 23:39:42.26 │ - Nodus ready to use\n",
      "        │ 23:39:42.41 │ - Created jobs table in NodusDB instance 'netsurf_db'\n",
      "        │ 23:39:42.41 │ - Created job_dependencies table in NodusDB instance 'netsurf_db'\n",
      "        │ 23:39:42.41 │ - Added NodusDB instance 'netsurf_db' linked to database 'netsurf_db'\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "#hls4ml_catapult_path = '/home/manuelbv/hls4ml_catapult'\n",
    "#sys.path = [hls4ml_catapult_path] + sys.path\n",
    "\n",
    "# Import datetime to get today's date\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\" Let's add our custom netsurf code \"\"\"\n",
    "import netsurf\n",
    "\n",
    "\"\"\" Get netsurf path \"\"\"\n",
    "import os \n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(netsurf.__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca54892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 <QuantizationScheme(q<6,2,1>)> obj @ (0x16306d870):\n",
      "    Total number of bits (m): 6\n",
      "    Integer bits         (n): 2\n",
      "    Float bits           (f): 3\n",
      "    Signed\n",
      "    Range: (-4, 3.875)\n",
      "    Min step: 0.125\n",
      "    Format: Sxx.xxx\n",
      "\n",
      "7_HLS_q6_2_1_bmark_keyword_spotting_prune0_125.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "╭───────┼─────────────┤\n",
      "╰ BMK ──┤ 23:39:43.41 │ - Initializing benchmark object keyword_spotting\n",
      "        │ 23:39:43.41 │ - Loss sparse_categorical_crossentropy found in tf.keras.losses with definition <function sparse_categorical_crossentropy at 0x1456de7a0>.\n",
      "        │ 23:39:43.41 │ - Adding custom metric sparse_categorical_accuracy with definition SparseCategoricalAccuracy(name=sparse_categorical_accuracy,dtype=float32).\n",
      "        │ 23:39:45.53 │ - Benchmark object keyword_spotting initialized\n",
      "╭───────┼─────────────┤\n",
      "╰ PLOT ─┤ 23:39:54.18 │ - Variable qq_conv2d_1/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:54.18 │ - Variable qq_conv2d_4/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:54.19 │ - Variable qq_conv2d_3/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:54.19 │ - Variable qq_conv2d_2/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:54.20 │ - Variable qq_dense/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:54.20 │ - Variable qq_conv2d/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:54.20 │ - Variable qq_depthwise_conv2d/depthwise_kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:54.21 │ - Variable qq_depthwise_conv2d_2/depthwise_kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:54.21 │ - Variable qq_depthwise_conv2d_3/depthwise_kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:54.21 │ - Variable qq_depthwise_conv2d_1/depthwise_kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:54.21 │ - Variable qq_batch_normalization/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.22 │ - Variable qq_apply_alpha/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:54.22 │ - Variable qq_conv2d/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:54.22 │ - Variable qq_batch_normalization_3/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:54.23 │ - Variable qq_apply_alpha_3/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.23 │ - Variable qq_apply_alpha_3/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:54.23 │ - Variable qq_depthwise_conv2d_1/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:54.23 │ - Variable qq_batch_normalization_2/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.24 │ - Variable qq_batch_normalization_2/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:54.24 │ - Variable qq_apply_alpha_2/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.24 │ - Variable qq_batch_normalization/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:54.25 │ - Variable qq_apply_alpha_2/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:54.25 │ - Variable qq_conv2d_1/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:54.25 │ - Variable qq_apply_alpha/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.25 │ - Variable qq_batch_normalization_1/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.26 │ - Variable qq_batch_normalization_1/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:54.26 │ - Variable qq_apply_alpha_1/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.26 │ - Variable qq_apply_alpha_1/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:54.27 │ - Variable qq_depthwise_conv2d/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:54.27 │ - Variable qq_batch_normalization_3/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.27 │ - Variable qq_apply_alpha_4/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.28 │ - Variable qq_conv2d_2/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:54.28 │ - Variable qq_batch_normalization_6/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.28 │ - Variable qq_batch_normalization_8/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.29 │ - Variable qq_batch_normalization_8/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:54.29 │ - Variable qq_apply_alpha_8/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.29 │ - Variable qq_apply_alpha_8/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:54.29 │ - Variable qq_conv2d_4/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:54.30 │ - Variable qq_batch_normalization_7/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.30 │ - Variable qq_batch_normalization_7/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:54.30 │ - Variable qq_apply_alpha_7/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.31 │ - Variable qq_apply_alpha_7/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:54.31 │ - Variable qq_depthwise_conv2d_3/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:54.32 │ - Variable qq_batch_normalization_6/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:54.32 │ - Variable qq_apply_alpha_4/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:54.32 │ - Variable qq_apply_alpha_6/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.32 │ - Variable qq_apply_alpha_6/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:54.33 │ - Variable qq_conv2d_3/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:54.33 │ - Variable qq_batch_normalization_5/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.33 │ - Variable qq_batch_normalization_5/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:54.34 │ - Variable qq_apply_alpha_5/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.34 │ - Variable qq_apply_alpha_5/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:54.34 │ - Variable qq_depthwise_conv2d_2/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:54.34 │ - Variable qq_batch_normalization_4/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:54.35 │ - Variable qq_batch_normalization_4/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:54.35 │ - Variable qq_dense/bias:0 has sparsity 100.00%\n",
      "/Users/mbvalentin/scripts/netsurf/tutorials/../netsurf/utils/plot.py:1006: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n",
      "        │ 23:39:55.07 │ - Saved sparsity plot to /var/folders/6b/p_271rqj4psdknf2z079285c0000gn/T/netsurf_sparsity3uua0syf.png\n",
      "        │ 23:39:55.07 │ - Plot range is -2.862626314163208 to 2.763033866882324\n",
      "        │ 23:39:55.21 │ - Variable qq_conv2d/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:55.22 │ - Variable qq_conv2d/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:55.22 │ - Variable qq_apply_alpha/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:55.22 │ - Variable qq_apply_alpha/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.23 │ - Variable qq_batch_normalization/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:55.23 │ - Variable qq_batch_normalization/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.24 │ - Variable qq_depthwise_conv2d/depthwise_kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:55.24 │ - Variable qq_depthwise_conv2d/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:55.24 │ - Variable qq_apply_alpha_1/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:55.25 │ - Variable qq_apply_alpha_1/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.25 │ - Variable qq_batch_normalization_1/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:55.25 │ - Variable qq_batch_normalization_1/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.26 │ - Variable qq_conv2d_1/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:55.26 │ - Variable qq_conv2d_1/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:55.26 │ - Variable qq_apply_alpha_2/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:55.27 │ - Variable qq_apply_alpha_2/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.27 │ - Variable qq_batch_normalization_2/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:55.28 │ - Variable qq_batch_normalization_2/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.28 │ - Variable qq_depthwise_conv2d_1/depthwise_kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:55.28 │ - Variable qq_depthwise_conv2d_1/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:55.29 │ - Variable qq_apply_alpha_3/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:55.29 │ - Variable qq_apply_alpha_3/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.30 │ - Variable qq_batch_normalization_3/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:55.30 │ - Variable qq_batch_normalization_3/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.31 │ - Variable qq_conv2d_2/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:55.31 │ - Variable qq_conv2d_2/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:55.31 │ - Variable qq_apply_alpha_4/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:55.32 │ - Variable qq_apply_alpha_4/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.32 │ - Variable qq_batch_normalization_4/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:55.33 │ - Variable qq_batch_normalization_4/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.33 │ - Variable qq_depthwise_conv2d_2/depthwise_kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:55.33 │ - Variable qq_depthwise_conv2d_2/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:55.34 │ - Variable qq_apply_alpha_5/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:55.34 │ - Variable qq_apply_alpha_5/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.34 │ - Variable qq_batch_normalization_5/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:55.35 │ - Variable qq_batch_normalization_5/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.35 │ - Variable qq_conv2d_3/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:55.36 │ - Variable qq_conv2d_3/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:55.36 │ - Variable qq_apply_alpha_6/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:55.36 │ - Variable qq_apply_alpha_6/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.37 │ - Variable qq_batch_normalization_6/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:55.37 │ - Variable qq_batch_normalization_6/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.38 │ - Variable qq_depthwise_conv2d_3/depthwise_kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:55.38 │ - Variable qq_depthwise_conv2d_3/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:55.38 │ - Variable qq_apply_alpha_7/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:55.38 │ - Variable qq_apply_alpha_7/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.39 │ - Variable qq_batch_normalization_7/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:55.39 │ - Variable qq_batch_normalization_7/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.40 │ - Variable qq_conv2d_4/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:55.40 │ - Variable qq_conv2d_4/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:55.41 │ - Variable qq_apply_alpha_8/alpha:0 has sparsity 0.00%\n",
      "        │ 23:39:55.41 │ - Variable qq_apply_alpha_8/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.41 │ - Variable qq_batch_normalization_8/gamma:0 has sparsity 0.00%\n",
      "        │ 23:39:55.42 │ - Variable qq_batch_normalization_8/beta:0 has sparsity 100.00%\n",
      "        │ 23:39:55.42 │ - Variable qq_dense/kernel:0 has sparsity 0.00%\n",
      "        │ 23:39:55.43 │ - Variable qq_dense/bias:0 has sparsity 100.00%\n",
      "        │ 23:39:59.41 │ - Saved sparsity plot to /var/folders/6b/p_271rqj4psdknf2z079285c0000gn/T/netsurf_sparsity_separatedcbi5e4ng.png\n",
      "        │ 23:39:59.41 │ - Plot range is -2.862626314163208 to 2.763033866882324\n",
      "WARNING:absl:You use TensorFlow DType <dtype: 'int16'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int16.\n",
      "╭───────┼─────────────┤\n",
      "╰ DATA ─┤ 23:40:00.38 │ - Normalizing dataset (input) using quantizer range (-4, 3.875)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] - HTML not implemented for keyword_spotting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "╭───────┼─────────────┤\n",
      "╰ PLT ──┤ 23:40:01.84 │ - Plotting histogram for activation qq_conv2d\n",
      "        │ 23:40:01.91 │ - Plotting histogram for activation qq_apply_alpha\n",
      "        │ 23:40:01.98 │ - Plotting histogram for activation qq_batch_normalization\n",
      "        │ 23:40:02.06 │ - Plotting histogram for activation qq_activation\n",
      "        │ 23:40:02.11 │ - Plotting histogram for activation qq_dropout\n",
      "        │ 23:40:02.16 │ - Plotting histogram for activation qq_depthwise_conv2d\n",
      "        │ 23:40:02.23 │ - Plotting histogram for activation qq_apply_alpha_1\n",
      "        │ 23:40:02.31 │ - Plotting histogram for activation qq_batch_normalization_1\n",
      "        │ 23:40:02.38 │ - Plotting histogram for activation qq_activation_1\n",
      "        │ 23:40:02.43 │ - Plotting histogram for activation qq_conv2d_1\n",
      "        │ 23:40:02.50 │ - Plotting histogram for activation qq_apply_alpha_2\n",
      "        │ 23:40:02.57 │ - Plotting histogram for activation qq_batch_normalization_2\n",
      "        │ 23:40:02.65 │ - Plotting histogram for activation qq_activation_2\n",
      "        │ 23:40:02.69 │ - Plotting histogram for activation qq_depthwise_conv2d_1\n",
      "        │ 23:40:02.76 │ - Plotting histogram for activation qq_apply_alpha_3\n",
      "        │ 23:40:02.82 │ - Plotting histogram for activation qq_batch_normalization_3\n",
      "        │ 23:40:02.89 │ - Plotting histogram for activation qq_activation_3\n",
      "        │ 23:40:02.93 │ - Plotting histogram for activation qq_conv2d_2\n",
      "        │ 23:40:03.01 │ - Plotting histogram for activation qq_apply_alpha_4\n",
      "        │ 23:40:03.08 │ - Plotting histogram for activation qq_batch_normalization_4\n",
      "        │ 23:40:03.16 │ - Plotting histogram for activation qq_activation_4\n",
      "        │ 23:40:03.20 │ - Plotting histogram for activation qq_depthwise_conv2d_2\n",
      "        │ 23:40:03.27 │ - Plotting histogram for activation qq_apply_alpha_5\n",
      "        │ 23:40:03.34 │ - Plotting histogram for activation qq_batch_normalization_5\n",
      "        │ 23:40:03.41 │ - Plotting histogram for activation qq_activation_5\n",
      "        │ 23:40:03.45 │ - Plotting histogram for activation qq_conv2d_3\n",
      "        │ 23:40:03.53 │ - Plotting histogram for activation qq_apply_alpha_6\n",
      "        │ 23:40:03.61 │ - Plotting histogram for activation qq_batch_normalization_6\n",
      "        │ 23:40:03.69 │ - Plotting histogram for activation qq_activation_6\n",
      "        │ 23:40:03.73 │ - Plotting histogram for activation qq_depthwise_conv2d_3\n",
      "        │ 23:40:03.80 │ - Plotting histogram for activation qq_apply_alpha_7\n",
      "        │ 23:40:03.86 │ - Plotting histogram for activation qq_batch_normalization_7\n",
      "        │ 23:40:03.96 │ - Plotting histogram for activation qq_activation_7\n",
      "        │ 23:40:04.01 │ - Plotting histogram for activation qq_conv2d_4\n",
      "        │ 23:40:04.09 │ - Plotting histogram for activation qq_apply_alpha_8\n",
      "        │ 23:40:04.19 │ - Plotting histogram for activation qq_batch_normalization_8\n",
      "        │ 23:40:04.27 │ - Plotting histogram for activation qq_activation_8\n",
      "        │ 23:40:04.32 │ - Plotting histogram for activation qq_dropout_1\n",
      "        │ 23:40:04.36 │ - Plotting histogram for activation average_pooling2d\n",
      "        │ 23:40:04.38 │ - Plotting histogram for activation qq_flatten\n",
      "        │ 23:40:04.41 │ - Plotting histogram for activation qq_dense\n",
      "        │ 23:40:04.43 │ - Plotting histogram for activation qq_softmax\n",
      "╭───────┼─────────────┤\n",
      "╰ BMK ──┤ 23:40:11.09 │ - No path provided, looking for latest h5 file at default model path: /Users/mbvalentin/scripts/netsurf/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/models\n",
      "        │ 23:40:11.09 │ - Looking for file /Users/mbvalentin/scripts/netsurf/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/models/pruned_0.125_DSConv.keras.latest\n",
      "╭───────┼─────────────┤\n",
      "╰ WARN ─┤ 23:40:11.09 │ - Weights file /Users/mbvalentin/scripts/netsurf/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/models/pruned_0.125_DSConv.keras.latest not found\n",
      "        │ 23:40:11.09 │ - No session files found at /Users/mbvalentin/scripts/netsurf/benchmarks/keyword_spotting/q6_2_1/pruned_0.125_DSConv/sessions\n",
      "╭───────┼─────────────┤\n",
      "╰ MDL ──┤ 23:40:11.09 │ - Running session with batch_size = 100, epochs = 36, opt_params = {'learning_rate': 0.001}, pruning_params = {'final_sparsity': 0.125, 'step': 2, 'end_epoch': 10}\n",
      "        │ 23:40:11.09 │ - Compiling model with parameters learning_rate=0.001\n",
      "        │ 23:40:11.09 │ - Fitting model with 36 epochs and batch_size 100\n",
      "        │ 23:42:30.37 │ - Epoch 0 - {'loss': 8.133614540100098, 'sparse_categorical_accuracy': 0.5971629619598389, 'val_loss': 4.075942039489746, 'val_sparse_categorical_accuracy': 0.6214610934257507, 'avg_alpha': 0.82690847, 'avg_beta': 0.1654589, 'loss_alpha_reg': 0.008727408, 'fisher_trace': 0.30562805349944744, 'fisher_entropy': -6.688324084456679}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 226\u001b[0m\n\u001b[1;32m    222\u001b[0m bmk\u001b[38;5;241m.\u001b[39mload_weights(verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# TRAINING - SESSION\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Try to get a session (if not, train)\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m sess \u001b[38;5;241m=\u001b[39m \u001b[43mnetsurf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_training_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbmk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scripts/netsurf/tutorials/../netsurf/core/benchmarks.py:1080\u001b[0m, in \u001b[0;36mget_training_session\u001b[0;34m(bmk, train_model, pruning, show_plots, plot)\u001b[0m\n\u001b[1;32m   1077\u001b[0m bmk\u001b[38;5;241m.\u001b[39mcompile(opt_params \u001b[38;5;241m=\u001b[39m opt_params, batch_size \u001b[38;5;241m=\u001b[39m batch_size)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# Run fitting\u001b[39;00m\n\u001b[0;32m-> 1080\u001b[0m sess, logs \u001b[38;5;241m=\u001b[39m \u001b[43mbmk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#callbacks=[pruning_callbacks.UpdatePruningStep()]\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# Save session config and object\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m sess\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/scripts/netsurf/tutorials/../netsurf/core/benchmarks.py:875\u001b[0m, in \u001b[0;36mBenchmark.fit\u001b[0;34m(self, epochs, batch_size, callbacks, prune, pruning_params, save_weights_checkpoint, verbose, *args, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;66;03m# THIS HAS TO BE THE LAST TO BE ADDED\u001b[39;00m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;66;03m# Add custom printer callback\u001b[39;00m\n\u001b[1;32m    873\u001b[0m     callbacks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [netsurf\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mCustomPrinter()]\n\u001b[0;32m--> 875\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown dataset type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/wsbmr/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/wsbmr/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/wsbmr/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/wsbmr/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/wsbmr/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/wsbmr/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsbmr/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/wsbmr/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/wsbmr/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pergamos as pg\n",
    "\n",
    "benchmark_name = 'keyword_spotting'\n",
    "# Set variables\n",
    "qscheme = \"q<6,2,1>\"\n",
    "pruning = 0.125\n",
    "prerank = True\n",
    "\n",
    "\n",
    "\"\"\" First of all, let's define a quantization Scheme \"\"\"\n",
    "Q = netsurf.QuantizationScheme(qscheme)\n",
    "print(Q)\n",
    "\n",
    "# Set filename\n",
    "benchmarks_dir = os.path.join(parent_dir, 'benchmarks')\n",
    "datasets_dir = os.path.join(parent_dir, 'datasets')\n",
    "\n",
    "filename = f\"7_HLS_{Q._scheme_str.no_special_chars()}_bmark_{benchmark_name}_prune{str(pruning).replace('.','_')}.html\"\n",
    "print(filename)\n",
    "doc = pg.Document(filename, theme=\"default\")\n",
    "doc.required_scripts.add('mathjax')\n",
    "\n",
    "\n",
    "\"\"\" Add a title to the document \"\"\"\n",
    "doc.append(pg.Markdown(f\"\"\"# Benchmarks Quantization Assertion\n",
    "> Author: Manuel B Valentin\n",
    "\n",
    "> Creation date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "> Project: netsurf\n",
    "\n",
    "> Used packages: netsurf, tensorflow, numpy, matplotlib, pergamos\n",
    "        \n",
    "\"\"\"))\n",
    "\n",
    "# Add a tab container\n",
    "tabs = pg.TabbedContainer({'Motivation': [], \n",
    "                        'Pre-training analysis': [],\n",
    "                        'Training': [],\n",
    "                            'Post-Training': [],\n",
    "                        'BER Injection': [],\n",
    "                        'Conclusions': []})\n",
    "\n",
    "# Get individual tabs\n",
    "tabmotivation = tabs['Motivation']\n",
    "tabpretraining = tabs['Pre-training analysis']\n",
    "tabtraining = tabs['Training']\n",
    "tabposttraining = tabs['Post-Training']\n",
    "tabber = tabs['BER Injection']\n",
    "tabconclusions = tabs['Conclusions']\n",
    "\n",
    "# Add to documnt\n",
    "doc.append(tabs)\n",
    "\n",
    "# Add a markdown description of what we want to achieve with this report in the first tab\n",
    "md = r\"\"\"\n",
    "## 1. Loss Taylor expansion\n",
    "\n",
    "Given a loss function $\\mathcal{L}(w)$, where $w$ is the vector of all weights in the network, the Taylor expansion around some point $w_0$ (say, the trained weights) for a small perturbation $\\Delta w$ is:\n",
    "\n",
    "$$\\mathcal{L}(w_0 + \\Delta w) \\approx \\mathcal{L}(w_0) + \\nabla \\mathcal{L}(w_0)^T \\Delta w + \\frac{1}{2} \\Delta w^T H \\Delta w$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\nabla \\mathcal{L}(w_0)$ is the gradient vector of the loss at w_0\n",
    "* $H$ is the Hessian matrix, i.e. $H = \\nabla^2 \\mathcal{L}(w_0)$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. If the model is trained??\n",
    "\n",
    "If the model has been well trained, then:\n",
    "\n",
    "$\\nabla \\mathcal{L}(w_0) \\approx 0$\n",
    "\n",
    "Because you're sitting near a (local) minimum.\n",
    "\n",
    "This removes the linear term:\n",
    "$\\mathcal{L}(w_0 + \\Delta w) - \\mathcal{L}(w_0) \\approx \\frac{1}{2} \\Delta w^T H \\Delta w$\n",
    "\n",
    "So the change in loss caused by a perturbation $\\Delta w$ is approximately:\n",
    "$\\Delta \\mathcal{L} \\approx \\frac{1}{2} \\Delta w^T H \\Delta w$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Interpretation for bit flips\n",
    "\n",
    "A bit flip in the quantized weights causes a small but structured change in the weights:\n",
    "\n",
    "* Say, flipping the 3rd bit in weight $w_i$ causes it to change by $\\delta_i$, so:\n",
    "\n",
    "$\\Delta w = \\begin{bmatrix}\n",
    "0 \\\\ \\cdots \\\\ \\delta_i \\\\ \\cdots \\\\ 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Then the loss increase is (approximately):\n",
    "$\\Delta \\mathcal{L} \\approx \\frac{1}{2} \\delta_i^2 H_{ii}$\n",
    "\n",
    "If multiple bits are flipped across weights, you sum their pairwise interactions via H, including off-diagonal terms (if not ignored).\n",
    "\n",
    "--- \n",
    "\n",
    "## 4. Implications for ranking\n",
    "\n",
    "This approximation motivates ranking bit positions (or weights) by:\n",
    "\n",
    "* $\\delta^2 \\cdot H_{ii}$: bit-flip magnitude times curvature\n",
    "* This is the FKeras method: estimates $H_{ii}$ and ranks accordingly\n",
    "* You could generalize it to your method:\n",
    "$\\text{Impact} \\cdot H$, not just gradients\n",
    "\n",
    "---\n",
    "\n",
    "## 5. When does this approximation hold?\n",
    "\n",
    "?? Works well when:\n",
    "\n",
    "* Bit-flip magnitude is small (i.e., local region)\n",
    "* Model is near a minimum\n",
    "* Hessian is stable (not exploding)\n",
    "\n",
    "?? Fails when:\n",
    "\n",
    "* Model isn??t trained well (gradient is large)\n",
    "* Loss surface is highly non-quadratic\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "The formula:\n",
    "$\\Delta \\mathcal{L} \\approx \\frac{1}{2} \\Delta \\mathbf{w}^T H \\Delta \\mathbf{w}$\n",
    "\n",
    "tells us how bit-flips propagate into loss increases, and explains why the Hessian is so powerful for ranking robustness. \n",
    "It encodes:\n",
    "\n",
    "* How impactful a perturbation is (via $\\delta$)\n",
    "* How sensitive the loss is locally (via $H$)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create markdown\n",
    "md = pg.Markdown(md)\n",
    "\n",
    "# Add to the first tab\n",
    "tabmotivation.append(md)\n",
    "\n",
    "\"\"\" Add quantization container to doc report \"\"\"\n",
    "tabpretraining.extend(Q.html())\n",
    "\n",
    "\"\"\" Save doc to file (we save after adding each element) \"\"\"\n",
    "doc.save(filename)\n",
    "\n",
    "# Let's create a container for all benchmarks \n",
    "benchmark_ct = pg.CollapsibleContainer(\"🧺 Benchmarks\", layout='vertical')\n",
    "\n",
    "# And another one for tabtraining\n",
    "benchmark_sessions_ct = pg.CollapsibleContainer(\"🧺 Benchmarks\", layout='vertical')\n",
    "\n",
    "# And another one for tabposttraining\n",
    "benchmark_posttraining_ct = pg.CollapsibleContainer(\"🧺 Benchmarks\", layout='vertical')\n",
    "\n",
    "# And yet another for \"BER Injection\"\n",
    "benchmark_ber_ct = pg.CollapsibleContainer(\"🧪 Experiments\", layout='vertical')\n",
    "\n",
    "\"\"\" Add to documnt \"\"\"\n",
    "tabpretraining.append(benchmark_ct)\n",
    "\n",
    "\"\"\" Add \"\"\"\n",
    "tabtraining.append(benchmark_sessions_ct)\n",
    "\n",
    "\"\"\" Add \"\"\"\n",
    "tabposttraining.append(benchmark_posttraining_ct)\n",
    "\n",
    "\"\"\" Add \"\"\"\n",
    "tabber.append(benchmark_ber_ct)\n",
    "\n",
    "# Define benchmarks to analyze\n",
    "#benchmarks = ['dummy', 'mnist_hls4ml', 'autompg', 'smartpixel_small', 'smartpixel_large',\n",
    "#              'cifar10', 'mnist_lenet5', 'ECONT_AE'\n",
    "# 'cifar100', 'svhn', 'fashion_mnist', 'imdb', 'reuters', 'boston_housing']\n",
    "# TODO: Fix visualization/contrast for cifar10\n",
    "# TODO: mnist_lenet5 seems to be working (good accuracy), but I'm not too happy about the alphas/betas. Some layers still have a big portion outside of the valid interval\n",
    "\n",
    "config_per_methods = netsurf.config.config_per_method\n",
    "protection_range = (0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4) #netsurf.config.DEFAULT_PROTECTION\n",
    "ber_range = (0.001, 0.005, 0.01, 0.05, 0.1, 0.15) #netsurf.config.DEFAULT_BER\n",
    "\n",
    "#methods = ['qpolar', 'qpolargrad', 'bitwise_msb', 'random', 'hirescam_norm', \n",
    "#           'hiresdelta', 'hessian', 'hessiandelta', 'weight_abs_value']\n",
    "methods = ['laplace', 'hessian', 'fisher', 'bitwise_msb', 'random', 'grad_norm', 'graddelta', 'weight_abs_value'] # 'qpolar', 'qpolargrad', \n",
    "methods = methods\n",
    "\n",
    "# Create benchmark object \n",
    "bmk = netsurf.get_benchmark(benchmark_name, Q,  benchmarks_dir = benchmarks_dir,\n",
    "                            datasets_dir = datasets_dir, pruning = pruning,\n",
    "                            load_weights = False)\n",
    "\n",
    "# Add benchmark html to container (this includes model + dataset htmls)\n",
    "# (run before training the model...)\n",
    "benchmark_ct.append(bmk.html())\n",
    "\n",
    "# Now let's prepare the data\n",
    "nsample_mod = 48 if 'ECON' in bmk.name else -1\n",
    "XYTrain = netsurf.utils.prepare_data(bmk, subset = 'train', nsample_mod = nsample_mod)\n",
    "\n",
    "# # Initialize the uncertainty profiler (pre-training)\n",
    "# pre_robustness_sgn_path = os.path.join(bmk.model_dir, 'uncertainty_signatures', f'{benchmark_name}.pretraining.netsurf.sgn')\n",
    "# try:\n",
    "#     pre_robustness_sgn = netsurf.UncertaintyProfiler.profile(bmk.model, XYTrain, bmk.model.loss, \n",
    "#                                                             batch_size = 2000, filepath = pre_robustness_sgn_path,\n",
    "#                                                             verbose = True)\n",
    "#     # Save \n",
    "#     pre_robustness_sgn.save_to_file(pre_robustness_sgn_path)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Add profile to tabpretraining\n",
    "# benchmark_ct.append(pre_robustness_sgn.html())\n",
    "\n",
    "# Now we can reload the weights (After the profiling)\n",
    "bmk.load_weights(verbose = True)\n",
    "\n",
    "# TRAINING - SESSION\n",
    "# Try to get a session (if not, train)\n",
    "sess = netsurf.get_training_session(bmk, show_plots = False, plot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb14804",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmk.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a container for this bmk in tabtraining\n",
    "bmk_sess_ct = pg.CollapsibleContainer(benchmark_name, layout='vertical')\n",
    "\n",
    "# Add session to tabtraining\n",
    "bmk_sess_ct.append(sess.html())\n",
    "\n",
    "# And add to benchmarks in tabtraining\n",
    "benchmark_sessions_ct.append(bmk_sess_ct)\n",
    "\n",
    "# Add benchmark again to post-training to check how the weights changed\n",
    "#benchmark_posttraining_ct.append(bmk.html())\n",
    "\n",
    "# Initialize the uncertainty profiler (post-training)\n",
    "# try:\n",
    "#     post_robustness_sgn_path = os.path.join(bmk.model_dir, 'uncertainty_signatures', f'{benchmark_name}.posttraining.netsurf.sgn')\n",
    "#     post_robustness_sgn = netsurf.UncertaintyProfiler.profile(bmk.model, XYTrain, bmk.model.loss, \n",
    "#                                     batch_size = 2000, verbose = True, filepath = post_robustness_sgn_path)\n",
    "\n",
    "#     # Save \n",
    "#     post_robustness_sgn.save_to_file(post_robustness_sgn_path)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Add profile to tabposttraining\n",
    "# benchmark_posttraining_ct.append(post_robustness_sgn.html())\n",
    "\n",
    "# Let's compare the divergence between the profiles\n",
    "# div_profile = netsurf.ProfileDivergence.from_signatures(pre_robustness_sgn, post_robustness_sgn)\n",
    "\n",
    "#div_profile.plot_divergence_summary()\n",
    "#div_profile.plot_advanced_divergence_summary()\n",
    "\n",
    "\"\"\" Save doc to file (we save after adding each element) \"\"\"\n",
    "doc.save(filename)\n",
    "\n",
    "# Create a container for this benchmark in \"BER Injection\"\n",
    "bmk_ber_ct = pg.CollapsibleContainer(benchmark_name, layout='vertical')\n",
    "\n",
    "# Add to tabber\n",
    "benchmark_ber_ct.append(bmk_ber_ct)\n",
    "\n",
    "# Create another container to store the individual results of the experiments\n",
    "exp_individual_ct = pg.CollapsibleContainer(\"🎁 Individual results\", layout='vertical')\n",
    "bmk_ber_ct.append(exp_individual_ct)\n",
    "\n",
    "# Create our ranker profiler (empty)\n",
    "rankers_bucket = netsurf.RankingComparator()\n",
    "\n",
    "# Loop thru methods\n",
    "exps = {}\n",
    "for method_alias in methods:\n",
    "    #########################################################################\n",
    "    # 0. Create the directory for the experiment (ranking, results, etc.)\n",
    "    #########################################################################\n",
    "    method = config_per_methods[method_alias]['method']\n",
    "    method_dir = os.path.join(bmk.experiments_dir, method)\n",
    "\n",
    "    #########################################################################\n",
    "    # 1. Create the ranker\n",
    "    #########################################################################\n",
    "    rkr = rankers_bucket.build_ranker(method, Q, config = config_per_methods[method], \n",
    "                                    path = method_dir,\n",
    "                                    is_baseline = (method == 'qpolargrad'))\n",
    "\n",
    "    # Get the exp dir from the ranker (with the hash)\n",
    "    exp_dir = rkr.path\n",
    "\n",
    "    #################################################################\n",
    "    # 1. Perform ranking according to method\n",
    "    #################################################################\n",
    "    # Rank weights \n",
    "    ranking = rkr.rank(bmk.model, *XYTrain, verbose = True)\n",
    "    # Save rank to csv file \n",
    "    ranking.save(overwrite = False)\n",
    "\n",
    "    #################################################################\n",
    "    # 2. Create experiment object\n",
    "    #################################################################\n",
    "    exp = netsurf.Experiment(bmk, ranking, num_reps = 100, \n",
    "                            ber_range = ber_range, \n",
    "                            protection_range = protection_range, \n",
    "                            path = exp_dir,\n",
    "                            verbose = True)\n",
    "\n",
    "    # Print experiment info \n",
    "    print(exp)\n",
    "\n",
    "    #################################################################\n",
    "    # 3. Run experiment with given ranking and for whatever \n",
    "    #       range of protection and rad \n",
    "    #################################################################\n",
    "    #batch_size = 1000,\n",
    "    exp.run_experiment(bmk, XYTrain,\n",
    "                    batch_size = None,\n",
    "                    ber_range = ber_range, \n",
    "                    protection_range = protection_range, \n",
    "                    rerun = False)\n",
    "    \n",
    "    # Add experiment to container\n",
    "    #exp_individual_ct.append(exp.html())\n",
    "\n",
    "    # Save experiment object\n",
    "    #exp.save()\n",
    "\n",
    "    # Add to dict\n",
    "    exps[method] = exp\n",
    "\n",
    "    \"\"\" Save doc to file (we save after adding each element) \"\"\"\n",
    "    doc.save(filename)\n",
    "\n",
    "\"\"\" Now perform the comparison of rankers \"\"\"\n",
    "#df = rankers_bucket.compare_rankers(granularity = 0.01, bootstrap = False)\n",
    "\n",
    "# Add comparison to container\n",
    "#bmk_ber_ct.append(rankers_bucket.html())\n",
    "\n",
    "# Save doc \n",
    "doc.save(filename)\n",
    "\n",
    "# # Save the rankers_bucket\n",
    "# rankers_comparison_filepath = os.path.join(bmk.experiments_dir, 'ranking_comparison.csv')\n",
    "# rankers_bucket.save_to_csv(rankers_comparison_filepath)\n",
    "\n",
    "# \"\"\" Create Experiment Comparator \"\"\"\n",
    "# exp_comp = netsurf.ExperimentComparator(list(exps.values()))\n",
    "#df = ExperimentComparator.compute_ranking_distribution(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6af04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
