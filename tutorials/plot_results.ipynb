{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\"\"\" Define some parameters \"\"\"\n",
    "key_map = {'hiresdelta_None': 'HiResDelta', 'hiresdelta': 'HiResDelta', \n",
    "            'hirescam_None': 'HiResCam', 'hirescam': 'HiResCam', \n",
    "            'hessian_original_msb_ranking_method_msb': 'Hessian (Original - MSB)', 'hessian_original_msb': 'Hessian (Original - MSB)', \n",
    "            'hessian_ranking_same_ranking_method_same' : 'Hessian (Original - Same)', 'hessian_ranking_same': 'Hessian (Original - Same)',\n",
    "            'hessian_original_same_ranking_method_same': 'Hessian (Original - Same)','hessian_original_same': 'Hessian (Original - Same)', \n",
    "            'hessian_ranking_hierarchical_ranking_method_hierarchical': 'Hessian (Ours)', 'hessian_hierarchical': 'Hessian (Ours)', 'hessian_ranking_hierarchical': 'Hessian (Ours)', \n",
    "            'hessian_ranking_lsb_ranking_method_msb' : 'Hessian (Original - LSB)', 'hessian_ranking_lsb': 'Hessian (Original - LSB)', 'hessian_ranking_method_lsb': 'Hessian (Original - LSB)',\n",
    "            'hessian_ranking_msb_ranking_method_msb' : 'Hessian (Original - MSB)', 'hessian_ranking_msb': 'Hessian (Original - MSB)', 'hessian_ranking_method_msb': 'Hessian (Original - MSB)',\n",
    "            'hessian_': 'Hessian',\n",
    "            'hessiandelta': 'Hessian Delta', 'hessiandelta_None': 'Hessian Delta',\n",
    "            'random': 'Random',\n",
    "            'recursive_uneven': 'Recursive Uneven',\n",
    "            'diffbitperweight': 'Diff Bit',\n",
    "            'layerwise_first_to_last': 'Layerwise (First)',\n",
    "            'layerwise_last_to_first': 'Layerwise (Last)',\n",
    "            'weight_abs_val': 'Weight Abs. Value',\n",
    "            'bitwise_msb_to_lsb': 'MSB to LSB',\n",
    "            'bitwise_lsb_to_msb': 'LSB to MSB',\n",
    "            '_None': '',\n",
    "            '_ranking_msb_ranking_method_msb': ' (Original - MSB)', \n",
    "            '_ranking_msb_ranking_method_lsb': ' (Original - LSB)',\n",
    "            '_ranking_method_msb': ' (Original - MSB)', '_ranking_msb': ' (Original - MSB)', \n",
    "            '_ranking_same_ranking_method_same' : ' (Original - Same)', '_ranking_method_same': ' (Original - Same)', '_ranking_same': ' (Original - Same)', \n",
    "            '_ranking_hierarchical_ranking_method_hierarchical' : ' (Ours)', '_ranking_method_hierarchical': ' (Ours)', '_ranking_hierarchical': ' (Ours)', \n",
    "            '_times_weights_true': ' x W', '_times_weights_false': '',\n",
    "            '_times_weights': ' x W',\n",
    "            '_ascending_true': '', '_ascending_false': '',\n",
    "            '_normalize_score_true': ' (norm)', '_normalize_score_false': '',\n",
    "            '_batch_size_10000': '', '_batch_size_1000': '', '_batch_size_100': '', \n",
    "            '_batch_size_96': '', '_batch_size_32': '', '_batch_size_256': '',\n",
    "            '_6bits_0int': '',\n",
    "            '_normalized': '',\n",
    "            '_bit_value_0': '', '_bit_value_1': '', '_bit_value_2': '', '_bit_value_3': '', '_bit_value_4': '', '_bit_value_5': '',\n",
    "            }\n",
    "\n",
    "metrics_map = {'mnist_hls4ml': 'noise_accuracy', \n",
    "               'mnist_lenet5': 'noise_accuracy', \n",
    "               'cifar10_hls4ml': 'noise_accuracy', \n",
    "               'mnist_squeezenet': 'noise_accuracy', \n",
    "               'tinyml_anomaly_detection': 'avg_mse', \n",
    "               'tinyml_person_detection': 'noise_accuracy',\n",
    "               'autompg': 'avg_mse', \n",
    "               'ECONT_AE': 'avg_mse',\n",
    "               'keyword_spotting': 'noise_accuracy'}\n",
    "\n",
    "level_map = {'key_level_1': 'model', 'key_level_2': 'bits_config', 'key_level_3': 'benchmark'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmarks\n",
      "[INFO] - Found metric noise_accuracy for benchmark mnist_hls4ml\n",
      "  mnist_hls4ml\n",
      "    6bits_0int\n",
      "      pruned_0.0_10954_hls4ml_cnn\n",
      "[INFO] - Creating plotter for /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments\n",
      "[INFO] - Adding file \".../layerwise_last/run_20241215_115414/result_bit_flip_injection.20241215_115437.csv\".\n",
      "[INFO] - Adding file \".../weight_abs_val/run_20241215_121421/result_bit_flip_injection.20241215_121515.csv\".\n",
      "[INFO] - Adding file \".../random/run_20241215_010958/result_bit_flip_injection.20241215_011000.csv\".\n",
      "[INFO] - Adding file \".../layerwise_first/run_20241215_135548/result_bit_flip_injection.20241215_135610.csv\".\n",
      "[INFO] - Adding file \".../bitwise_msb/run_20241215_103739/result_bit_flip_injection.20241215_103741.csv\".\n",
      "[INFO] - Adding file \".../hirescam_norm/run_20241215_121911/result_bit_flip_injection.20241215_122830.csv\".\n",
      "[INFO] - Adding file \".../hiresdelta/run_20241215_170120/result_bit_flip_injection.20241215_172343.csv\".\n",
      "[INFO] - Processing layerwise_last_to_first_ascending_false_6bits_0int_normalized from /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/layerwise_last/run_20241215_115414/result_bit_flip_injection.20241215_115437.csv\n",
      "[INFO] - Processing weight_abs_val_None_ascending_false_batch_size_100_6bits_0int_normalized from /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/weight_abs_val/run_20241215_121421/result_bit_flip_injection.20241215_121515.csv\n",
      "[INFO] - Processing random_None_ascending_false_6bits_0int_normalized from /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/random/run_20241215_010958/result_bit_flip_injection.20241215_011000.csv\n",
      "[INFO] - Processing layerwise_first_to_last_ascending_true_6bits_0int_normalized from /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/layerwise_first/run_20241215_135548/result_bit_flip_injection.20241215_135610.csv\n",
      "[INFO] - Processing bitwise_msb_to_lsb_ascending_true_6bits_0int_normalized from /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/bitwise_msb/run_20241215_103739/result_bit_flip_injection.20241215_103741.csv\n",
      "[INFO] - Processing hirescam_None_times_weights_false_ascending_false_normalize_score_true_batch_size_100_6bits_0int_normalized from /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/hirescam_norm/run_20241215_121911/result_bit_flip_injection.20241215_122830.csv\n",
      "[INFO] - Processing hiresdelta_None_times_weights_false_ascending_false_normalize_score_true_batch_size_100_6bits_0int_normalized from /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/hiresdelta/run_20241215_170120/result_bit_flip_injection.20241215_172343.csv\n",
      "[INFO] - Found global metrics for layerwise_last_to_first_ascending_false_6bits_0int_normalized in /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/layerwise_last/run_20241215_115414/global_metrics_bit_flip_injection.20241215_122240.json\n",
      "[INFO] - Found global metrics for weight_abs_val_None_ascending_false_batch_size_100_6bits_0int_normalized in /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/weight_abs_val/run_20241215_121421/global_metrics_bit_flip_injection.20241215_125239.json\n",
      "[INFO] - Found global metrics for random_None_ascending_false_6bits_0int_normalized in /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/random/run_20241215_010958/global_metrics_bit_flip_injection.20241215_013519.json\n",
      "[INFO] - Found global metrics for layerwise_first_to_last_ascending_true_6bits_0int_normalized in /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/layerwise_first/run_20241215_135548/global_metrics_bit_flip_injection.20241215_142208.json\n",
      "[INFO] - Found global metrics for bitwise_msb_to_lsb_ascending_true_6bits_0int_normalized in /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/bitwise_msb/run_20241215_103739/global_metrics_bit_flip_injection.20241215_110301.json\n",
      "[INFO] - Found global metrics for hirescam_None_times_weights_false_ascending_false_normalize_score_true_batch_size_100_6bits_0int_normalized in /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/hirescam_norm/run_20241215_121911/global_metrics_bit_flip_injection.20241215_130325.json\n",
      "[INFO] - Found global metrics for hiresdelta_None_times_weights_false_ascending_false_normalize_score_true_batch_size_100_6bits_0int_normalized in /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments/hiresdelta/run_20241215_170120/global_metrics_bit_flip_injection.20241215_174659.json\n",
      "      /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments\n",
      "      /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments\n",
      "    /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments\n",
      "    /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn\n",
      "  /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments\n",
      "  /mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int\n",
      "/mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_0.0_10954_hls4ml_cnn/experiments\n",
      "/mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks/mnist_hls4ml\n",
      "[INFO] - Expanding DFs for benchmarks\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/home/manuelbv/WSBMR/workspace/dev')\n",
    "sys.path.append('/home/manuelbv/fkeras') #https://github.com/KastnerRG/fkeras\n",
    "\n",
    "\"\"\" Import plotting libraries \"\"\"\n",
    "from wsbmr import plots\n",
    "import os\n",
    "\n",
    "# Init base benchmarks dir \n",
    "#benchmarks_dir = '/home/manuelbv/WSBMR/workspace/benchmarks'\n",
    "#benchmarks_dir = '/asic/projects/NU/WSBMR/common/benchmarks'\n",
    "#benchmarks_dir = '/asic/projects/NU/WSBMR/manuelbv/common/benchmarks_new'\n",
    "#benchmarks_dir = '/mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks'\n",
    "\n",
    "benchmarks_dir = '/mnt/raid0/asic/projects/NU/WSBMR/manuelbv/workspace/benchmarks'\n",
    "\n",
    "maps_kwargs = {'key_map': key_map, 'metrics_map': metrics_map, 'level_map': level_map}\n",
    "\n",
    "\"\"\" Create container (with recursive subcontainers) \"\"\"\n",
    "e = plots.ExpContainer(benchmarks_dir, 'benchmarks', level = 3, log = True, verbose = True, **maps_kwargs)\n",
    "\n",
    "# Save the global DF to file \n",
    "e.curves.to_csv(os.path.join(benchmarks_dir,'benchmarks_curves_df.csv'))\n",
    "e.global_df.to_csv(os.path.join(benchmarks_dir,'benchmarks_global_df.csv'))\n",
    "e.global_metrics.to_csv(os.path.join(benchmarks_dir,'benchmarks_global_metrics_df.csv'))\n",
    "\n",
    "# this is how to access specific plotters/metrics\n",
    "#e.subs['mnist_hls4ml'].subs['6bits_0int'].subs['non_pruned_10954_conv16f3k1s_bn_relu_pool2p2s_conv16f3k1s_bn_relu_pool2p2s_conv24f3k1s_bn_relu_pool2p2s_flatten_dense42u_bn_relu_dense64u_bn_relu_dense10u_softmax'].plotter.metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Print available benchmarks\n",
    "print('Available benchmarks:')\n",
    "print(\"   - \" + \"\\n   - \".join(list(e.subs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      pruned_0.0_10954_hls4ml_cnn"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.subs['mnist_hls4ml'].subs['6bits_0int'].subs['pruned_0.0_10954_hls4ml_cnn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define plot params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define some vars \"\"\"\n",
    "verbose = True\n",
    "cmap = 'viridis'\n",
    "single_out = 'Random'\n",
    "show = True\n",
    "save_to_file = os.getenv('USER') == 'manuelbv'\n",
    "\n",
    "# Create kwargs to be passed\n",
    "kwargs = {'show': show, 'cmap': cmap, 'verbose': verbose, 'save_to_file': save_to_file, 'single_out': single_out}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ranking times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot ranking times \"\"\"\n",
    "# '/home/manuelbv/WSBMR/common/ranking_times.png'\n",
    "methods = ['MSB to LSB', 'Random', 'Weight Abs. Value', 'Layerwise (First)', \n",
    "            'HiResCam',#'HiResCam (norm)', 'HiResCam x W', 'HiResCam x W (norm)',\n",
    "           'Hessian (Original - MSB)', 'Hessian Delta (Original - MSB)', 'HiResDelta (norm)']\n",
    "e.plot_ranking_times(filename = None, methods = methods, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot pruning correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.global_df.loc[(e.global_df['benchmark'] == 'mnist_hls4ml') & (e.global_df['method'] == 'HiResCam x W (norm)'),'method'] = 'HiResCam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot pruning correlation \"\"\"\n",
    "methods = ['Random', 'MSB to LSB', 'Layerwise (First)', 'Weight Abs. Value','HiResCam', 'HiResDelta (norm)', 'Hessian (Original - MSB)']\n",
    "figsize = (7, 3*2)\n",
    "e.plot_pruning_correlation(methods = methods, ylog = True, figsize = figsize, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2d curves, 3d volumes, boxplots, etc per benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "benchmarks = list(e.subs.keys())\n",
    "#benchmarks = ['keyword_spotting', 'cifar10_hls4ml'][1:]\n",
    "#benchmarks = ['tinyml_anomaly_detection']\n",
    "# Define the benchmarks we want to plot\n",
    "benchmarks = ['mnist_hls4ml']\n",
    "methods = ['HiResCam x W (norm)']\n",
    "figsize = (7, 9)\n",
    "for b in benchmarks:\n",
    "    \"\"\" Get subcontainer for benchmark \"\"\"\n",
    "    container = e.subs[b]\n",
    "\n",
    "    # ylog for mse\n",
    "    #ylog = 'avg_mse' in metrics_map[b]\n",
    "    ylog = 'tinyml_anomaly_detection' in b\n",
    "\n",
    "    # 3d volumes for all experiments and models\n",
    "    container.plot_3d_volumes(cat_name = b, methods = methods, title = '', figsize = figsize, **kwargs)\n",
    "    break\n",
    "\n",
    "    #2d curves for all experiments and models\n",
    "    container.plot_2d_curves(cat_name = b, methods = methods, ylog = ylog, title = '', figsize = figsize, **kwargs)\n",
    "\n",
    "    # Box plot for all experiments and models\n",
    "    container.plot_boxplot(cat_name = b, add_connecting_line = True, **kwargs)\n",
    "\n",
    "    \n",
    "    # Plot bars for all experiments and models\n",
    "    container.plot_bars(cat_name = b, remove_baseline = False, ylog = ylog, **kwargs)\n",
    "    if not ylog:\n",
    "        container.plot_bars(cat_name = b, suffix = '_minus_zeroTMR', remove_baseline = True, ylog = ylog, **kwargs)\n",
    "        container.plot_bars(cat_name = b, suffix = '_minus_random', baseline ='Random', remove_baseline = True, ylog = ylog, **kwargs)\n",
    "    \n",
    "    # Save all plotter objects to file \n",
    "    container.save_to_file(cat_name = b, suffix = '_experiments_results', extension = '.pkl')\n",
    "\n",
    "    # Save all plotters dfs to csvs \n",
    "    container.save_dfs_to_csvs(cat_name = b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container.plot_2d_curves(cat_name = b, methods = methods, ylog = ylog, title = '', figsize = (7,7), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot how many times each method ranked #1, #2 and #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Get the methods in static order \n",
    "all_methods = list(e.curves['method'].unique())\n",
    "\n",
    "# Init pd of differences \n",
    "df_podium = []\n",
    "\n",
    "# Loop thru benchmarks\n",
    "for b in e.curves['benchmark'].unique():\n",
    "    # get subdf\n",
    "    subdf = e.curves[e.curves['benchmark'] == b]\n",
    "    # Loop thru models\n",
    "    for m in subdf['model'].unique():\n",
    "        # get subdf\n",
    "        subdf2 = subdf[subdf['model'] == m]\n",
    "        # Loop thru tmrs\n",
    "        for tmr in subdf2['tmr'].unique():\n",
    "            # get subdf\n",
    "            subdf3 = subdf2[subdf2['tmr'] == tmr]\n",
    "            for rad in subdf3['radiation'].unique():\n",
    "                subdf4 = subdf3[subdf3['radiation'] == rad]\n",
    "                \n",
    "                metric = metrics_map[b]\n",
    "                meds = subdf4.groupby('method')[metric].mean().sort_values(ascending = 'avg_mse' in metric)\n",
    "\n",
    "                t = [np.where(meds.index == x)[0] for x in all_methods if x in meds.index] \n",
    "                # Drop empty\n",
    "                t = [x[0] if len(x) > 0 else np.nan for x in t]\n",
    "\n",
    "                # Case label \n",
    "                case_label = f'{b}_{m}_{tmr}_{rad}'\n",
    "\n",
    "                t = [case_label, b] + t\n",
    "\n",
    "                df_podium.append(t)\n",
    "\n",
    "# Convert df_podium to pd with all_methods as columns\n",
    "df_podium = pd.DataFrame(df_podium, columns = ['case', 'benchmark'] + all_methods)\n",
    "\n",
    "# Drop rows with nans\n",
    "#df_podium.dropna(inplace = True)\n",
    "\n",
    "# Infer numbers \n",
    "df_podium = df_podium.infer_objects()\n",
    "\n",
    "# Sum over benchmarks\n",
    "#df_podium = df_podium.groupby('benchmark')\n",
    "\n",
    "\n",
    "# Count how many times each method ranks at the first position\n",
    "#(df_podium[all_methods] == 0.0)\n",
    "\n",
    "\n",
    "# Add total column \n",
    "#df_podium['total'] = df_podium[all_methods].sum(axis = 1)\n",
    "\n",
    "# Relative (only for methods columns)\n",
    "#df_podium[all_methods] = 100*df_podium[all_methods].div(df_podium['total'], axis = 0)\n",
    "\n",
    "# # Show stats\n",
    "# a = df_podium.describe()\n",
    "# # Add SUM row \n",
    "# a.loc['sum'] = df_podium[all_methods].sum()\n",
    "# a\n",
    "df_podium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranked = []\n",
    "\n",
    "for b in df_podium['benchmark'].unique():\n",
    "    subdf = df_podium[df_podium['benchmark'] == b]\n",
    "    subdf = subdf.drop(columns = ['benchmark', 'case'])\n",
    "    \n",
    "    if True:\n",
    "        for i in range(3):\n",
    "            t = (subdf == i).sum(axis = 0)\n",
    "            t['rank'] = i\n",
    "            t['benchmark'] = b\n",
    "            df_ranked.append(t)\n",
    "    else:\n",
    "        t = (subdf == 0).sum(axis = 0) + (subdf == 1).sum(axis = 0) + (subdf == 2).sum(axis = 0)\n",
    "        t['rank'] = \"0,1,2\"\n",
    "        t['benchmark'] = b\n",
    "        df_ranked.append(t)\n",
    "    \n",
    "df_ranked = pd.DataFrame(df_ranked)\n",
    "df_ranked = df_ranked[['benchmark', 'rank'] + all_methods]\n",
    "df_ranked['total'] = df_ranked[all_methods].sum(axis = 1)\n",
    "\n",
    "# Sum per benchmark \n",
    "#df_ranked = df_ranked.groupby(['benchmark','rank']).sum()\n",
    "\n",
    "\n",
    "df_ranked['ours'] = df_ranked[['hirescam', 'hiresdelta','hessiandelta']].sum(axis = 1)\n",
    "\n",
    "# Relative (only for methods columns)\n",
    "df_ranked[all_methods + ['ours']] = 100*df_ranked[all_methods + ['ours']].div(df_ranked['total'], axis = 0)\n",
    "\n",
    "df_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "maps = {'hirescam': 'HiResCam', 'hiresdelta': 'HiResDelta', 'hessiandelta': 'Hessian Delta', 'hessian': 'Hessian',\n",
    "        'random': 'Random', 'layerwise': 'Layerwise', 'bitwise': 'MSB', }\n",
    "\n",
    "\"\"\" Do the barplot manually cause it's just too complex to try to do it with damn pandas \"\"\"\n",
    "benchmarks = df_ranked['benchmark'].unique()\n",
    "#benchmarks = ['ECONT_AE', 'mnist_hls4ml', 'tinyml_anomaly_detection']\n",
    "# keep only benchmarks in df_ranked\n",
    "benchmarks = [b for b in benchmarks if b in df_ranked['benchmark'].unique()]\n",
    "\n",
    "nrows = len(benchmarks)\n",
    "\n",
    "fig, axs = plt.subplots(nrows, 1, figsize = (7, 5*nrows))\n",
    "\n",
    "if not isinstance(axs, np.ndarray):\n",
    "    axs = [axs]\n",
    "\n",
    "space = 0.3\n",
    "n = len(all_methods)\n",
    "width = (1 - space) / 3\n",
    "\n",
    "for ib,b in enumerate(benchmarks):\n",
    "\n",
    "    subdf = df_ranked[df_ranked['benchmark'] == b]\n",
    "\n",
    "    # Rearrange the columns according to the values of the first rank\n",
    "    subdf = subdf[all_methods].sort_values(subdf.index[0], axis = 1, ascending = False)\n",
    "\n",
    "    cc = subdf.columns\n",
    "\n",
    "    # Let's get the bars positions. Each method will have 3 bars, one for each rank, and we will separate them by \"space\". \n",
    "    # Also, each rank will be one next to each other, so all 3 bars are clumped together. \n",
    "    for i in range(3):\n",
    "        bar_x = []\n",
    "        bar_y = []\n",
    "        for x in range(n):\n",
    "            dx = 3*width/2\n",
    "            xx = x - dx + width/2\n",
    "            bar_x.append(xx + width*i)\n",
    "            bar_y.append(subdf[cc[x]].values[i])\n",
    "        \n",
    "        axs[ib].bar(bar_x, bar_y, width = width, edgecolor = 'k', linewidth = 0.4, alpha = 0.7, label = f'{i+1}{[\"st\", \"nd\", \"rd\"][i]} place')\n",
    "    \n",
    "    # Let's plot a rectangle around the methods ['hirescam', 'hiresdelta', 'hessiandelta']\n",
    "    for m in ['hirescam', 'hiresdelta', 'hessiandelta']:\n",
    "        idx = np.where(cc == m)[0]\n",
    "        if len(idx) > 0:\n",
    "            idx = idx[0]\n",
    "            axs[ib].add_patch(plt.Rectangle((idx - 0.5, -1), 1, np.max(subdf[m].values) + 3, facecolor = 'none', edgecolor = 'g', linewidth = 1.5))\n",
    "    \n",
    "    # Do the same with red rectangles for hessian\n",
    "    for m in ['hessian']:\n",
    "        idx = np.where(cc == m)[0]\n",
    "        if len(idx) > 0:\n",
    "            idx = idx[0]\n",
    "            axs[ib].add_patch(plt.Rectangle((idx - 0.5, -1), 1, np.max(subdf[m].values) + 3, facecolor = 'none', edgecolor = 'r', linewidth = 1.5))\n",
    "\n",
    "    axs[ib].set_xticks(np.arange(n), cc, rotation = 90)\n",
    "    axs[ib].legend()\n",
    "    axs[ib].set_title(b)\n",
    "    axs[ib].set_ylabel('%')\n",
    "    axs[ib].grid('major', linestyle='--')\n",
    "    axs[ib].set_ylim(0, 63)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf[all_methods].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    # Get only rank == i\n",
    "    subdf = df_ranked[df_ranked['rank'] == i]\n",
    "\n",
    "    fig, axs = plt.subplots(len(subdf), 1, figsize = (10, 5*len(subdf)))\n",
    "\n",
    "    col = {'ours': 'green', 'hessian': 'red'}\n",
    "\n",
    "    # Gather 'hirescam', 'hiresdelta' and 'hessiandelta' into 'ours' column\n",
    "    #df_podium['ours'] = df_podium[['hirescam', 'hiresdelta', 'hessiandelta']].max(axis = 1)\n",
    "\n",
    "    #meth2 = [mm for mm in all_methods if mm not in ['hirescam', 'hiresdelta', 'hessiandelta']] + ['ours']\n",
    "\n",
    "    for i, (idx, row) in enumerate(subdf.iterrows()):\n",
    "        # Colors for specific methods\n",
    "        row[all_methods].sort_values(ascending = False).plot(kind = 'bar', ax = axs[i], title = 'Podium for ' + row['benchmark'])\n",
    "        axs[i].set_ylabel('%')\n",
    "        axs[i].set_xlabel('Method')\n",
    "        #axs[i].set_ylim(0, 100)\n",
    "        axs[i].grid()\n",
    "        axs[i].set_axisbelow(True)\n",
    "\n",
    "    plt.title(f'Podium for rank {i}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Get the methods in static order \n",
    "all_methods = list(e.global_df['method'].unique())\n",
    "\n",
    "# Init pd of differences \n",
    "df_podium = []\n",
    "\n",
    "# Loop thru benchmarks\n",
    "for b in e.global_df['benchmark'].unique():\n",
    "    # get subdf\n",
    "    subdf = e.global_df[e.global_df['benchmark'] == b]\n",
    "    # Loop thru models\n",
    "    for m in subdf['model'].unique():\n",
    "        # get subdf\n",
    "        subdf2 = subdf[subdf['model'] == m]\n",
    "        # Loop thru tmrs\n",
    "        for tmr in subdf2['tmr'].unique():\n",
    "            # get subdf\n",
    "            subdf3 = subdf2[subdf2['tmr'] == tmr]\n",
    "\n",
    "            # Sort by auc \n",
    "            subdf3 = subdf3.sort_values(by = 'auc', ascending = False)\n",
    "            # Reset index\n",
    "            subdf3 = subdf3.reset_index(drop = True)\n",
    "\n",
    "            t = [subdf3[subdf3['method'] == x].index.values for x in all_methods] \n",
    "            # Drop empty\n",
    "            t = [x[0] if len(x) > 0 else np.nan for x in t]\n",
    "\n",
    "            # Case label \n",
    "            case_label = f'{b}_{m}_{tmr}'\n",
    "\n",
    "            t = [case_label, b] + t\n",
    "\n",
    "            df_podium.append(t)\n",
    "\n",
    "# Convert df_podium to pd with all_methods as columns\n",
    "df_podium = pd.DataFrame(df_podium, columns = ['case', 'benchmark'] + all_methods)\n",
    "df_podium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podium_rank_counts = []\n",
    "\n",
    "# For each benchmark, count how many times each method has been ranked as 1st, 2nd, 3rd, etc\n",
    "for m in df_podium['benchmark'].unique():\n",
    "    print(f'Ranking for benchmark {m}')\n",
    "    subdf = df_podium[df_podium['benchmark'] == m]\n",
    "    subdf = subdf.drop(columns = ['case', 'benchmark'])\n",
    "    subdf = subdf.apply(pd.Series.value_counts) \n",
    "    subdf = subdf.fillna(0)\n",
    "    subdf = subdf.T\n",
    "    #subdf = subdf.sort_values(by = 0, ascending = False)\n",
    "    #print(subdf)\n",
    "\n",
    "    for ri in range(subdf.shape[1]):\n",
    "        podium_rank_counts.append([m, ri] + subdf[ri].values.tolist())\n",
    "\n",
    "# Convert to pd\n",
    "podium_rank_counts = pd.DataFrame(podium_rank_counts, columns = ['benchmark', 'rank'] + all_methods)\n",
    "print(podium_rank_counts)\n",
    "\n",
    "# Save to file\n",
    "podium_rank_counts.to_csv(os.path.join(benchmarks_dir,'benchmarks_podium_rank_counts.csv'))\n",
    "print(f'Saved podium rank counts to {os.path.join(benchmarks_dir,\"benchmarks_podium_rank_counts.csv\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf.sort_values(by=0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf[0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Loop thru benchmarks\n",
    "df = e.curves\n",
    "\n",
    "old_titles = df['simplified_title'].unique()\n",
    "\n",
    "# Gather some titles together, by category\n",
    "new_titles = ['HiResDelta' if t.startswith('HiResDelta ') else t for t in old_titles]\n",
    "new_titles = ['HiResCam' if t.startswith('HiResCam ') else t for t in new_titles]\n",
    "new_titles = ['HessianDelta' if t.startswith('Hessian Delta ') else t for t in new_titles]\n",
    "new_titles = ['Hessian' if t.startswith('Hessian ') else t for t in new_titles]\n",
    "new_titles = ['Layerwise' if t.startswith('Layerwise ') else t for t in new_titles]\n",
    "\n",
    "# And gather them a bit more for our purpose now \n",
    "new_titles = ['ours' if t.startswith('HessianDelta') or t.startswith('HiResCam') or t.startswith('HiResDelta') else t for t in new_titles]\n",
    "new_titles = ['theirs' if t.startswith('Hessian') else t for t in new_titles]\n",
    "new_titles = ['others' if t.startswith('MSB to LSB') else t for t in new_titles]\n",
    "\n",
    "# make a copy df\n",
    "df2 = df.copy() \n",
    "\n",
    "for old_t, new_t in zip(old_titles, new_titles):\n",
    "    # add to df \n",
    "    df2.loc[df2['simplified_title'] == old_t, 'simplified_title'] = new_t\n",
    "\n",
    "# Init pd of differences \n",
    "df_diff = pd.DataFrame()\n",
    "\n",
    "# Loop thru benchmarks\n",
    "for b in df2['benchmark'].unique():\n",
    "    # get subdf\n",
    "    subdf = df2[df2['benchmark'] == b]\n",
    "    # Loop thru models\n",
    "    for m in subdf['model'].unique():\n",
    "        # get subdf\n",
    "        subdf2 = subdf[subdf['model'] == m]\n",
    "        # Loop thru tmrs\n",
    "        for tmr in subdf2['tmr'].unique():\n",
    "            # get subdf\n",
    "            subdf3 = subdf2[subdf2['tmr'] == tmr]\n",
    "            # Loop thru radiation\n",
    "            for rad in subdf3['radiation'].unique():\n",
    "                # get subdf\n",
    "                subdf4 = subdf3[subdf3['radiation'] == rad]\n",
    "\n",
    "                met = metrics_map[b]\n",
    "\n",
    "                # Separate by ours, theirs, others\n",
    "                cols = ['benchmark','model','simplified_title', 'title', 'method_idx', 'tmr','radiation','true_radiation', 'noise_accuracy', 'avg_mse']\n",
    "                df_ours = subdf4[subdf4['simplified_title'] == 'ours'][cols]\n",
    "                df_theirs = subdf4[subdf4['simplified_title'] == 'theirs'][cols]\n",
    "                df_others = subdf4[subdf4['simplified_title'] == 'others'][cols]\n",
    "\n",
    "                global_min_value = subdf4[met].min()\n",
    "                ours_min_value = df_ours[met].min()\n",
    "\n",
    "                if df_ours.empty or df_theirs.empty or df_others.empty:\n",
    "                    continue\n",
    "                \n",
    "                # Build this row \n",
    "                for d,dn in zip((df_theirs, df_others),('theirs', 'others')):\n",
    "                    diff = (df_ours[met].values[:,None] - d[met].values[None,:])\n",
    "                    if met == 'noise_accuracy':\n",
    "                        idx = np.unravel_index(diff.argmax(), diff.shape)\n",
    "                    else:\n",
    "                        idx = np.unravel_index(diff.argmin(), diff.shape)\n",
    "                    min_val = diff[idx]\n",
    "                    if met == 'noise_accuracy':\n",
    "                        rel_val = 100*min_val\n",
    "                    else:\n",
    "                        rel_val = 100*min_val / df_ours.iloc[idx[0]][met]\n",
    "                    row = {'benchmark': b, 'model': m, 'tmr': tmr, 'radiation': rad, 'true_radiation': df_ours.iloc[idx[0]]['true_radiation'], 'ours': df_ours.iloc[idx[0]][met], 'other_theirs': d.iloc[idx[1]][met]}\n",
    "                    row.update({'compare': dn, 'diff': min_val, 'rel_diff_%': rel_val, 'title': df_ours.iloc[idx[0]]['title'], 'method_idx': df_ours.iloc[idx[0]]['method_idx']})\n",
    "                    row.update({f'global_min_{met}': global_min_value, 'ours_min_value': ours_min_value})\n",
    "\n",
    "                    # Now get the condition\n",
    "                    dftmp = pd.DataFrame([row])\n",
    "\n",
    "                    # Add to df_diff\n",
    "                    df_diff = pd.concat([df_diff, dftmp])\n",
    "       \n",
    "# Only keep those in which abs_val rel_diff is greater than 0\n",
    "df_diff = df_diff[df_diff['rel_diff_%'] > 0]\n",
    "\n",
    "# Sort by rel_diff_%\n",
    "df_diff = df_diff.sort_values(by='rel_diff_%', ascending=False)\n",
    "\n",
    "# Discard the ones with tmr == 0\n",
    "df_diff = df_diff[df_diff['tmr'] != 0.0]\n",
    "# And the ones with radiation == 0\n",
    "df_diff = df_diff[df_diff['radiation'] != 0.0]\n",
    "\n",
    "# Only keep those in which our method is the best one\n",
    "df_diff = df_diff[df_diff[f'global_min_{met}'] == df_diff['ours_min_value']]\n",
    "\n",
    "\n",
    "df_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "\n",
    "def plot_barplot_df_per_row(r, title, ylabel, colors = None, cmap = 'viridis', figsize = (7, 7), filename = None):\n",
    "    \"\"\" Define params \"\"\"\n",
    "    fontsize = 8\n",
    "    \n",
    "    # barwidth depends on the (max) number of unique methods, this is so \n",
    "    # in our plot, each model is centered at integer positions [0, 1, 2 ...]\n",
    "    #max_num_methods = df.groupby(['benchmark', 'model'])['method'].count().max()\n",
    "    barwidth = 0.25\n",
    "\n",
    "    hatch_styles = ['\\\\\\\\', '++', '..', 'xx', '||', '--', 'oo', '**', '//', 'OO']\n",
    "\n",
    "    vmin = np.min(list(r.values()))\n",
    "    vmax = np.max(list(r.values()))\n",
    "    # Create a color palette\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "    # Normalize the values\n",
    "    norm = plt.Normalize(vmin, vmax)\n",
    "    # Create lambda function to map any value to color later \n",
    "    cmapper = lambda x: cmap(norm(x))\n",
    "\n",
    "    # Store old value of 'hatc.linewidth'\n",
    "    old_linewidth = plt.rcParams['hatch.linewidth']\n",
    "    # Set the linewidth of the hatch lines\n",
    "    plt.rcParams['hatch.linewidth'] = 0.3\n",
    "    plt.rcParams[\"lines.solid_capstyle\"] = \"butt\"\n",
    "    old_grid_color = plt.rcParams['grid.color']\n",
    "    plt.rcParams['grid.color'] = (0.5, 0.5, 0.5, 0.3)\n",
    "    \n",
    "    # Init figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize = figsize)\n",
    "\n",
    "    # Initialize cumulative x position\n",
    "    cumx = 0 - barwidth*len(r)/2\n",
    "\n",
    "    legend_handles = []\n",
    "\n",
    "    # Loop thru columns of r \n",
    "    for me, (method, value) in enumerate(r.items()):\n",
    "        \n",
    "        if method not in colors or colors is None:\n",
    "            # Get color for bar\n",
    "            c = cmapper(value)\n",
    "            # Add transparecy\n",
    "            c = c[:-1] + (0.3,)\n",
    "        else:\n",
    "            c = colors[method]\n",
    "\n",
    "        # Calculate positions of our rectangle \n",
    "        x0, x1, y0, y1 = cumx, cumx + barwidth, 0, value\n",
    "\n",
    "        # Add bar to plot\n",
    "        bar = ax.fill_between([x0, x1], y0, y1, color=c, edgecolor = 'k', linewidth = 0.5)\n",
    "\n",
    "        # Apply hatch \n",
    "        hs = hatch_styles[me % len(hatch_styles)]\n",
    "        bar.set_hatch(hs)\n",
    "\n",
    "        # Increase cumx \n",
    "        cumx += barwidth\n",
    "\n",
    "        # add legend handle\n",
    "        p = Patch(facecolor = c, edgecolor = 'k', label = method, hatch = hs)\n",
    "        legend_handles.append(p)\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # ytitle\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    # Set grid to dashed and also turn minor grid on\n",
    "    ax.grid(which='major', linestyle='--')\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(which='minor', linestyle=':')\n",
    "\n",
    "    # Now finally, add the global legend\n",
    "    # Add legend\n",
    "    ax.legend(handles=legend_handles, loc=(0.01,0.55), handleheight=2, handlelength=3, fontsize = fontsize)\n",
    "\n",
    "    if filename is not None:\n",
    "        fig.savefig(filename, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Now pick this case from e.curves\n",
    "# Init pd \n",
    "df_cases = pd.DataFrame()\n",
    "\n",
    "bars_to_plot = ['hessiandelta', 'hirescam', 'hiresdelta', \n",
    "                'hessian', \n",
    "                'layerwise', 'bitwise', 'recursive_uneven', 'diffbitperweight', 'weight_abs_val', 'random']\n",
    "\n",
    "ours_color = 'tab:blue'\n",
    "theirs_color = 'tab:orange'\n",
    "others_color = 'tab:green'\n",
    "\n",
    "colors = {'hessiandelta': ours_color, 'hirescam': ours_color, 'hiresdelta': ours_color, \n",
    "            'hessian': theirs_color,\n",
    "            'layerwise': others_color, 'bitwise': others_color, 'recursive_uneven': others_color, 'diffbitperweight': others_color, 'weight_abs_val': others_color, 'random': others_color}\n",
    "\n",
    "output_dir = '/asic/projects/NU/WSBMR/manuelbv/common/_cases'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "_files = []\n",
    "\n",
    "# loop thru rows \n",
    "for ri, (_,r) in enumerate(df_diff.iterrows()):\n",
    "    #r = df_diff.iloc[0]\n",
    "\n",
    "    met = metrics_map[r['benchmark']]\n",
    "\n",
    "    subdf = df2[(df2['benchmark'] == r['benchmark']) & (df2['model'] == r['model']) & (df2['tmr'] == r['tmr']) & (df2['radiation'] == r['radiation']) & (df2['title'] == r['title']) & (df2['method_idx'] == r['method_idx'])]\n",
    "\n",
    "    # bars_in_df \n",
    "    a = df2[(df2['benchmark'] == r['benchmark']) & (df2['model'] == r['model']) & (df2['tmr'] == r['tmr']) & (df2['radiation'] == r['radiation'])].groupby('method')[met].mean()\n",
    "    a = a[[b for b in bars_to_plot if b in a.index]].sort_values().to_dict()\n",
    "    \n",
    "    # Build title\n",
    "    rad = 10**(np.round(np.log(r['radiation'])))\n",
    "    tmr = 10**(np.round(np.log(r['tmr'])))\n",
    "    t = f'{r[\"benchmark\"]}\\n{r[\"model\"][:15]}...\\nTMR: {100*tmr:3.2f}% - Rad: {100*rad:3.2f}%'\n",
    "\n",
    "    # Plot \n",
    "    filename = os.path.join(output_dir, f'case_{ri}.png')\n",
    "    _files.append(filename)\n",
    "    plot_barplot_df_per_row(a, t, 'Avg. MSE' if met == 'avg_mse' else 'Accuracy (%)', colors = colors, cmap = 'viridis', figsize = (7, 7), filename = filename)\n",
    "    \n",
    "    # Add \"r\" to a dict\n",
    "    a = {**a, **r.to_dict()}\n",
    "\n",
    "    # convert to pd and append to df_cases \n",
    "    df_cases = pd.concat([df_cases, pd.DataFrame([a])])\n",
    "\n",
    "# Zip files folder \n",
    "import zipfile\n",
    "with zipfile.ZipFile(os.path.join(output_dir, 'cases.zip'), 'w') as z:\n",
    "    for f in _files:\n",
    "        z.write(f, os.path.basename(f))\n",
    "\n",
    "df_cases = df_cases.reset_index()\n",
    "df_cases.drop(columns=['index'], inplace=True)\n",
    "df_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "key_map = {'hiresdelta_None': 'HiResDelta', 'hiresdelta': 'HiResDelta', \n",
    "            'hirescam_None': 'HiResCam', 'hirescam': 'HiResCam', \n",
    "            'hessian_original_msb_ranking_method_msb': 'Hessian (Original - MSB)', 'hessian_original_msb': 'Hessian (Original - MSB)', \n",
    "            'hessian_ranking_same_ranking_method_same' : 'Hessian (Original - Same)', 'hessian_ranking_same': 'Hessian (Original - Same)',\n",
    "            'hessian_original_same_ranking_method_same': 'Hessian (Original - Same)','hessian_original_same': 'Hessian (Original - Same)', \n",
    "            'hessian_ranking_hierarchical_ranking_method_hierarchical': 'Hessian (Ours)', 'hessian_hierarchical': 'Hessian (Ours)', 'hessian_ranking_hierarchical': 'Hessian (Ours)', \n",
    "            'hessian_ranking_lsb_ranking_method_msb' : 'Hessian (Original - LSB)', 'hessian_ranking_lsb': 'Hessian (Original - LSB)', 'hessian_ranking_method_lsb': 'Hessian (Original - LSB)',\n",
    "            'hessian_ranking_msb_ranking_method_msb' : 'Hessian (Original - MSB)', 'hessian_ranking_msb': 'Hessian (Original - MSB)', 'hessian_ranking_method_msb': 'Hessian (Original - MSB)',\n",
    "            'hessian_': 'Hessian',\n",
    "            'hessiandelta': 'Hessian Delta', 'hessiandelta_None': 'Hessian Delta',\n",
    "            'random': 'Random',\n",
    "            'recursive_uneven': 'Recursive Uneven',\n",
    "            'diffbitperweight': 'Diff Bit',\n",
    "            'layerwise_first_to_last': 'Layerwise (First)',\n",
    "            'layerwise_last_to_first': 'Layerwise (Last)',\n",
    "            'weight_abs_val': 'Weight Abs. Value',\n",
    "            'bitwise_msb_to_lsb': 'MSB to LSB',\n",
    "            'bitwise_lsb_to_msb': 'LSB to MSB',\n",
    "            '_None': '',\n",
    "            '_ranking_msb_ranking_method_msb': ' (Original - MSB)', \n",
    "            '_ranking_msb_ranking_method_lsb': ' (Original - LSB)',\n",
    "            '_ranking_method_msb': ' (Original - MSB)', '_ranking_msb': ' (Original - MSB)', \n",
    "            '_ranking_same_ranking_method_same' : ' (Original - Same)', '_ranking_method_same': ' (Original - Same)', '_ranking_same': ' (Original - Same)', \n",
    "            '_ranking_hierarchical_ranking_method_hierarchical' : ' (Ours)', '_ranking_method_hierarchical': ' (Ours)', '_ranking_hierarchical': ' (Ours)', \n",
    "            '_times_weights_true': ' x W', '_times_weights_false': '',\n",
    "            '_times_weights': ' x W',\n",
    "            '_ascending_true': '', '_ascending_false': '',\n",
    "            '_normalize_score_true': ' (norm)', '_normalize_score_false': '',\n",
    "            '_batch_size_10000': '', '_batch_size_1000': '', '_batch_size_100': '', \n",
    "            '_batch_size_96': '', '_batch_size_32': '',\n",
    "            '_6bits_0int': '',\n",
    "            '_normalized': '',\n",
    "            }\n",
    "\n",
    "# Get num of params from model name \n",
    "df['model_num_params'] = df['model'].apply(lambda x: int(x.replace('non_pruned_','').replace('pruned_','').split('_')[0]))\n",
    "\n",
    "# change aliases to human readable names\n",
    "old_titles = df['method'].unique()\n",
    "\n",
    "nt = []\n",
    "for t in old_titles:\n",
    "    for k, v in key_map.items():\n",
    "        # if k in t:\n",
    "        #     if c:\n",
    "        #         print('\\t', t, ' -> ', t.replace(k,v))\n",
    "        t = t.replace(k,v)\n",
    "    nt.append(t)\n",
    "   # new_titles = [t.replace(k,v) for t in new_titles]\n",
    "new_titles = nt\n",
    "#new_titles = plots.simplify_strings(new_titles, verbose = False)\n",
    "\n",
    "#common_part = plots.get_common_suffix(new_titles)\n",
    "#if common_part != \"\":\n",
    "#    new_titles = [t.replace(common_part,'') for t in new_titles]\n",
    "\n",
    "# Gather some titles together, by category\n",
    "new_titles = ['HiResDelta' if t.startswith('HiResDelta ') else t for t in new_titles]\n",
    "new_titles = ['HiResCam' if t.startswith('HiResCam ') else t for t in new_titles]\n",
    "new_titles = ['HessianDelta' if t.startswith('Hessian Delta ') else t for t in new_titles]\n",
    "new_titles = ['Hessian' if t.startswith('Hessian ') else t for t in new_titles]\n",
    "new_titles = ['Layerwise' if t.startswith('Layerwise ') else t for t in new_titles]\n",
    "\n",
    "# make a copy df\n",
    "df2 = df.copy() \n",
    "\n",
    "for old_t, new_t in zip(old_titles, new_titles):\n",
    "    # add to df \n",
    "    df2.loc[df2['method'] == old_t, 'method'] = new_t\n",
    "    #print(old_t, ' -> ', new_t)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "\n",
    "mms = df2['method'].unique()\n",
    "mms = ['MSB to LSB', 'Random', 'Layerwise', 'Diff Bit', 'Recursive Uneven', 'Weight Abs. Value', 'HiResCam', 'HiResDelta', 'Hessian', 'HessianDelta']\n",
    "\n",
    "mms = ['Random', 'MSB to LSB', 'HiResCam', 'HiResDelta', 'Hessian', 'HessianDelta']\n",
    "# Loop thru methods \n",
    "for method in mms:\n",
    "    print(method)\n",
    "    # Get subdf \n",
    "    subdf = df2[df2['method'] == method]\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    # Loop thru unique model_params to average those with the same params\n",
    "    for mp in subdf['model_num_params'].unique():\n",
    "        subsubdf = subdf[subdf['model_num_params'] == mp]\n",
    "        if len(subsubdf) > 1:\n",
    "            #print(subsubdf)\n",
    "            #subdf = subdf.append(subsubdf.mean(), ignore_index = True)\n",
    "            #print(subdf)\n",
    "            #subdf = subdf.drop(subsubdf.index)\n",
    "            #print(subdf)\n",
    "            x.append(mp)\n",
    "            y.append(subsubdf['ranking_time'].max())\n",
    "        else:\n",
    "            x.append(mp)\n",
    "            y.append(subsubdf['ranking_time'].values[0])\n",
    "\n",
    "    # Get params and ranking times \n",
    "    #model_num_params = subdf['model_num_params'].values\n",
    "    #ranking_time = subdf['ranking_time'].values\n",
    "    model_num_params = np.array(x)\n",
    "    ranking_time = np.array(y)\n",
    "\n",
    "    # sort \n",
    "    idx = np.argsort(model_num_params)\n",
    "    model_num_params = model_num_params[idx]\n",
    "    ranking_time = ranking_time[idx]\n",
    "    ax.plot(model_num_params, ranking_time, '--o', label = method)\n",
    "\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Find all exps \n",
    "# dir = '/home/manuelbv/WSBMR/workspace/benchmarks/mnist_hls4ml/6bits_0int/non_pruned_10954_conv16f3k1s_bn_relu_pool2p2s_conv16f3k1s_bn_relu_pool2p2s_conv24f3k1s_bn_relu_pool2p2s_flatten_dense42u_bn_relu_dense64u_bn_relu_dense10u_softmax/experiments'\n",
    "# #dir = '/home/manuelbv/WSBMR/workspace/benchmarks/mnist_lenet5/6bits_0int/non_pruned_545546_conv6f5k1s_tanh_pool2p1s_conv16f5k1s_tanh_pool2p2s_conv120f5k1s_tanh_flatten_dense84u_tanh_dense10u_softmax/experiments'\n",
    "# #dir = '/home/manuelbv/WSBMR/workspace/benchmarks/cifar10_hls4ml/6bits_0int/non_pruned_14266_conv16f3k1s_bn_relu_pool2p2s_conv16f3k1s_bn_relu_pool2p2s_conv24f3k1s_bn_relu_pool2p2s_flatten_dense42u_bn_relu_dense64u_bn_relu_dense10u_softmax/experiments'\n",
    "# #dir = '/home/manuelbv/WSBMR/workspace/benchmarks/mnist_squeezenet/6bits_0int/non_pruned_squeezenet/experiments'\n",
    "# #dir = '/home/manuelbv/WSBMR/workspace/benchmarks/tinyml_anomaly_detection/6bits_0int/non_pruned_187752_flatten_dense128u_bn_relu_dense128u_bn_relu_dense128u_bn_relu_dense128u_bn_relu_dense8u_bn_relu_dense128u_bn_relu_dense128u_bn_relu_dense128u_bn_relu_dense128u_bn_relu_dense320u_reshape/experiments'\n",
    "# #dir = '/home/manuelbv/WSBMR/workspace/benchmarks/mnist_hls4ml/6bits_0int/pruned_10954_conv16f3k1s_bn_relu_pool2p2s_conv16f3k1s_bn_relu_pool2p2s_conv24f3k1s_bn_relu_pool2p2s_flatten_dense42u_bn_relu_dense64u_bn_relu_dense10u_softmax/experiments'\n",
    "# #dir = '/home/manuelbv/WSBMR/workspace/benchmarks/dummy/6bits_0int/pruned_61_dense20u_dense1u/experiments'\n",
    "# #dir = '/home/manuelbv/WSBMR/workspace/benchmarks/autompg/6bits_0int/non_pruned_221_dense20u_dense1u/experiments'\n",
    "\n",
    "# #dir = '/home/manuelbv/WSBMR/workspace/benchmarks/autompg/6bits_0int/non_pruned_4865_dense64u_relu_dense64u_relu_dense1u/experiments'\n",
    "# #dir = '/home/manuelbv/WSBMR/workspace/benchmarks/autompg/6bits_0int/pruned_4865_dense64u_relu_dense64u_relu_dense1u/experiments'\n",
    "# dir = '/home/manuelbv/WSBMR/workspace/benchmarks/autompg/6bits_0int/pruned_4865_dense64u_dense64u_dense1u/experiments'\n",
    "# dir = '/home/manuelbv/WSBMR/workspace/benchmarks/autompg/6bits_0int/non_pruned_4865_dense64u_dense64u_dense1u/experiments'\n",
    "\n",
    "# metrics = ['noise_accuracy', 'avg_mse', 'wasserstein_distance', 'kl_divergence', 'emd']\n",
    "\n",
    "# for metric in metrics[1:2]:\n",
    "#     print(f'[INFO] - Plotting {metric} metric')\n",
    "\n",
    "#     # Only plot noise accuracy\n",
    "#     #show = metric == 'noise_accuracy'\n",
    "#     show = True\n",
    "\n",
    "#     \"\"\" Initialize plotter object \"\"\"\n",
    "#     plotter = plots.ExperimentsPlotter(dir, log = log, metric = metric, key_map = key_map, verbose = True)\n",
    "\n",
    "#     # Plot 3d volumes \n",
    "#     plotter.plot_3d_volumes(filename = f'{dir}/{metric}_3d_volumes.png', cmap = True, show = show)\n",
    "\n",
    "#     # 3d GIF\n",
    "#     #plotter.make_3d_gif(filename = f'{dir}/{metric}_3d_volumes.gif', fps = 1, verbose = True, show = False)\n",
    "\n",
    "#     # Plot 2d lines \n",
    "#     plotter.plot_2d_curves(filename = f'{dir}/{metric}_2d_curves.png', cmap = True, show = show)\n",
    "\n",
    "#     # 2d GIF\n",
    "#     #plotter.make_2d_gif(filename = f'{dir}/{metric}_2d_curves.gif', fps = 1, verbose = True, show = False)\n",
    "\n",
    "#     \"\"\" Plot bars \"\"\"\n",
    "#     plotter.plot_bars(filename = f'{dir}/{metric}_bars.png', remove_baseline = False, single_out = single_out, show = show)\n",
    "#     plotter.plot_bars(filename = f'{dir}/{metric}_bars_minus_zeroTMR.png', remove_baseline = True, single_out = single_out, show = show)\n",
    "#     plotter.plot_bars(filename = f'{dir}/{metric}_bars_minus_random.png', baseline ='Random', remove_baseline = True, single_out = single_out, show = show)\n",
    "\n",
    "#     \"\"\" Plot errorbox \"\"\"\n",
    "#     plotter.plot_boxplot(filename = f'{dir}/{metric}_boxplot.png', show = show, cmap = cmap)\n",
    "\n",
    "#     \"\"\" Save plotter object to file \"\"\"\n",
    "#     plotter.save_to_file(filename = f'{dir}/{metric}_experiments_results.pkl')\n",
    "\n",
    "#     \"\"\" Save plotter dfs to csvs \"\"\"\n",
    "#     plotter.save_dfs_to_csvs(verbose = True)\n",
    "\n",
    "#     print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qkeras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
